[["index.html", "A First Course In Statistics A Concise Introduction with Examples in R Preface", " A First Course In Statistics A Concise Introduction with Examples in R Trang Bui 2025-12-18 Preface Statistics is the study of data. This ranges from collecting, analyzing, to making conclusions based on the data. Understanding statistics is crucial if you work with any type of research that uses data. This book presents a short and gentle introduction to statistics. You will learn some (i) basic concepts about data and their summaries, (ii) statistical tools to analyze them, and (iii) how to implement them in R. The book will be helpful for those of you who just get started, as well as those who want to have a better and more systematic understanding. In writing the book, as well as in writing the lecture notes, I try to present the ideas as clear as possible with examples and thought exercises included throughout the chapters. The book is intended to help you build and strengthen your statistical knowledge from square one by presenting basic solutions to several fundamental statistical problems. Although the book can be quite simple, I hope it will still inspire you to go further and explore more about the broad field of statistics. Acknowledgement: The book closely follows my lecture notes for AFM 113 at University of Waterloo. I would like to thank Ms. Dina Dawoud for helping me build structure of the lecture notes, and hence this book. "],["chap-step.html", "1 Four Research Steps 1.1 Why Statistics? 1.2 Research Question 1.3 Data Collection 1.4 Data Analysis", " 1 Four Research Steps 1.1 Why Statistics? Curiosity, as a part of human nature, motivates us to ask questions and find answers. Imagine our ancestors in their early days. Every night, around the fire, they might have looked at the sky and observed the shapes of the moon changing from day to day, and asked themselves if there is any principle to such change. They might have started by counting the days between two full moons and found a roughly 30-day difference. Will this be true all the time, or is it true only for this instance? To answer this, they might have had to continue to count the days from month to month, year to year, to finally understand the moon cycle. This answer-finding process can be applied to almost any question in our lives. Starting with a question or a general area of interest, we collect data (information) around the topic, look at the data to find pattern, and form an answer to the question. We can stop there if we are satisfied with the answer, or we can modify the question, continue to collect data, revise the answer, and so on. These are the main steps of a general research process, which can be summarized in Figure 1.1. Figure 1.1: General research process Statistics is the study of data1, hence it involves all steps of the general research process. In particular, statistics will help us with forming our research questions in a way that it can be answered using data, designing the data collecting process, understanding and interpreting the data collected, and making an informed (data-driven) conclusion. Therefore, we can think of statistics as a supporting field for almost all data-related or quantitative sciences. Besides general procedures and techniques, specialized statistical methods can be developed to cater to specific research areas with specific types of data and requirements. Some specializations of statistics are biostatistics, econometrics, psychometrics, etc. In fact, many important results in statistics were established by non-statisticians. This emphasizes the interdisciplinary nature of statistics and its important role in scientific research. Knowing statistics will help you understand and critically evaluate published research and conduct research yourselves. Therefore LET’S LEARN STATISTICS! In this chapter, we will briefly describe each step in Figure 1.1 while learning some important definitions and concepts in statistics. 1.2 Research Question Let’s start with the first step of a research: forming a research question. What can be a research question? And how to look at this research question in a statistical way? 1.2.1 Variables A variable is a characteristic or quantity which can take different values. A variable can be qualitative: characteristics that can be described or categorized into categories. Some examples are gender: this can take the values of female, male, or others; education: this can take the values of high school, undergraduate, graduate, etc. quantitative: characteristics that can be described or quantified by numerical values. Some examples are height: this can take numerical values such as 165cm, 180cm, 155cm, etc. age: this can take numerical values such as 19, 5, 67, etc. Variables are the topics of research goals: We may want to understand one variable. e.g., What are the ages of people when they enter college? How much Canadians earn this year? etc. We may want to understand the relationship among multiple variables. e.g., the relationship of gender and income, of age and height, of location and house price, etc. We may want to predict the value of one variable given the values of some other variables e.g., predict the final exam score given the midterm exam score; predict income given gender, years of education, job types, etc. If a research is involved with more than one variable, there is usually only one variable of main interest. This is the variable whose trends you want to understand or the one which you want to make predictions about. This variable is called the response variable (also known as the dependent variable). Other variables that help explain or predict the response variable are called the explanatory variables (also known as the independent variables). Notes: I think it is now a good time to take an example. I will use this example throughout the book to explain statistical concepts because I myself find it an easier way to understand, especially, a chapter packed with new concepts like this one. For those of you who are new to statistics, the amount of new terminology in this lesson may seem intimidating. I strongly recommend you to think of another example of your own, and then apply or explain the new definitions in the context of your example. Having your own examples and trying to explain them yourself will be extremely useful to help you clearly understand new ideas and definitions. Example 1.1 Suppose I am interested in how much students pay for rent. Then rent is my response variable. If furthermore I also want to know if gender or level of study (undergraduate/graduate) affects how much students pay for rent. Then gender and level of study will be my explanatory variables. Exercise 1.1 If I am interested in people’s income, what is my response variable, and what can be my explanatory variables? 1.2.2 Inference and Prediction In general, there are two general goals of statistics: inference: understanding and inferring a principle of the current situation, and prediction: making an informed guess about an unseen situation or the future. Different statistical techniques are developed for each of the two goals. Classical statistics usually involves inference, while the recently popular machine learning researches focuses on prediction. Example 1.2 Continue with the rent example. Now, if I am interested in knowing if female students pay more for rent compared to male students, my goal will be inference. On the other hand, if, for example, I meet a new friend and I know that he is an undergraduate student, based on the data I collected about other students, I want to guess how much he is paying for rent. Now my goal would be prediction. 1.2.3 Population and Sample Units (or individuals, or cases) are the objects on which the values of the variables are measured or recorded. In the student rent example, “students” will be our study units. Example 1.3 Now that I know the topics of my research (i.e., the variables), I will need to decide the scope of my research. Who do I want to make conclusions about? Do I want to understand the monthly rent expense of UWaterloo students, or do I want to know the monthly rent expense of all Canadian students? Population is the entire group of units that we want to make conclusions about. Example 1.4 Suppose I want to understand the monthly rent expense of UWaterloo students. If I want to understand this exactly, I will need to ask every single student studying at University of Waterloo how much they are paying for rent. But will that ever be possible? There are tens of thousands of students studying at UWaterloo. So instead, I will try to get a sense of the monthly rent expense of UWaterloo students by walking around the campus and asking them how much they are paying for rent. However, I cannot meet every student, and even if I’m committed to see every UWaterloo student, it will take a tremendous amount of time to do so. In the end, I will only be able to collect the rent data from some students. This is called the sample for my study. A sample is the specific group of units on which we collect our data. Example 1.5 Because I will walk around the campus to ask students about their rent, I will only be able to collect information from ones who actually walk on campus, and I will hardly meet anyone who studies remotely. Furthermore, a person who walks on campus needs not to be a student. In this case, the group that I am able to survey is different from the group that I want to research. Because of the way we collect our data, there will be two types of population: Target population: the group of units that you want to make conclusions about. Accessible population (or study population): the group of units that you are actually able to get access to. When these two types of population coincide, we just call both of them population for short. In summary, we want to use data of the sample to make conclusions about the target population. However, due to the way we collect our data, we are only able to collect data from the accessible population, and hence our conclusions will be most valid on the accessible population. Figure 1.2: Population and sample Example 1.6 Describe a situation when the accessible population and the target population are the same, and when they are not the same. Exercise 1.2 Cannabis has recently been legalized in Canada. We want to understand the use of cannabis by adult Canadians by surveying 1000 Canadians by phone. What can be the variables that we want to collect information about? What is the target population? What is the accessible population? Is it the same with the target population? Is it a subset of the target population? Describe the sample obtained. 1.2.4 Parameter and Statistic Example 1.7 Now we collect the data about students’ rent, there will be tens to hundreds of observations in our sample. We can look at each observation on its own, for example, how much Phillip, or Sophia, or Jeremy pays for their rent this month. In this way, we will know more about Phillip, and Sophia, and Jeremy. But what can we say about the rent paid by UWaterloo students in general? How can we understand the tens or hundreds of individual observations in a collective way? The answer is, we need to summarize the data. A parameter is a quantity that can be calculated from the population. A statistic is a quantity that can be calculated from the sample. For each parameter, there will be a corresponding statistic. Example 1.8 Continue with our rent example, we may be interested in how much UWaterloo students are paying for rent on average. The parameter here is the population average. The corresponding statistic is the average rent that we calculate from the data in our sample. Exercise 1.3 Suppose now I am interested in the vaccination rate in Canada. What is my parameter of interest and what is its corresponding statistic? We can calculate the value of a statistic from the collected sample, thus we know the value of the statistic. However, we cannot calculate or know the value of the parameter because we cannot collect all information in the population. Hence, the value of a population parameter is often unknown2. So, the goal of statistics is to use a sample statistic3 to make conclusions about a population parameter. 1.3 Data Collection 1.3.1 Variability and Bias 1.3.1.1 Fixed Population and Random Sample Example 1.9 In the rent example, our population is the whole group of students studying at University of Waterloo. In a semester, this set will likely be unchanged. However, in the next semester, there may be students who graduate and new students who just begin. In this case, the population may be different from one semester to another. However, in basic statistics, we usually assume that our population is fixed and the population parameter of interest is thus a fixed quantity4. While the population is assumed fixed, the sample is random. Example 1.10 To understand the average rent expense of UWaterloo students, I decide to walk around the campus and ask 100 students about their rent expenses. In this case, my sample will contain 100 observations (i.e., my sample size is 100). However, these 100 people are not fixed. If I were to go on another day, there will be a high chance that the people I will meet will be different from the people I am going to meet today. That is, today, I will meet these 100 people just by chance. I do not know who I am going to meet before I actually meet them. In this case, I have a random sample. If I repeat the data collection process on several days, I will receive a different sample and thus a different statistic every time I conduct the data collection. 1.3.1.2 Variability The difference among the different observed samples of the same sampling method is called the variability of the sampling method. A sampling method with low variability is a sampling method with high precision. Example 1.11 Suppose I randomly choose one region in Waterloo and Kitchener and then ask students in that region there about their rent. In this case, my sampling scheme will have a high variability or a low precision. This is because if I repeat this same sample collecting method for many times, every time I will interview students living in a different region. Since rent will depend on specific regions and their distances to school, the rent data I collect will tend to be substantially different from one sample to another. 1.3.1.3 Bias The difference between the population data and the average of different samples (coming from the same sampling method) is called the bias of the sampling method. Small bias means high accuracy. Example 1.12 Suppose I only interview female students. Even if I repeat this same sample collection method many times, I only get the data that tells me about the rent price for female students. This data will likely be different from the rent price of all students on campus. Hence I say the bias is high and accuracy is low. Figure 1.3 summarizes concepts about bias, variance, accuracy and precision. Figure 1.3: Accuracy and precision. 1.3.2 What Makes A Good Sample? Because we want to use the sample to understand the population, we need the sample to produce an accurate and precise estimate of the population parameter. To increase accuracy and precision, i.e., to decrease variability and bias, we want our sample to be large because the more units we observe, the closer we will be to the population. Our sample will then be more accurate. It will also be more precise because when the sample is large, the possible samples will be not very different from each other. representative of the population. This way, the sample will resemble the population and our estimator will be more accurate. With a good sample, we can generalize our results to the population with better confidence. 1.3.2.1 Unmeasured Variables There can be infinitely many possible variables that contribute to the response variable that we are interested in. And we can never be able to collect information about all those variables. The variables that can affect the variable of interest, but are not recorded in the data are called unmeasured variables. Example 1.13 Together with gender and level study, personal lodging preference can be a factor that affects the rent price a student is paying. However, it is hard to measure this variable, hence it is one of the unmeasured variables. 1.3.2.2 Representative Sample A representative sample needs to be representative of all variables, even unmeasured ones. If a sample is not representative of a variable, there will be a systematic difference between the sample and the population, this will introduce bias on our estimator. In this case, we call this a biased sample. (Refer to Example 1.12) If the accessible population is not representative of the target population, we have a study bias. If the sample is not representative of the accessible population, we have a sample bias. Exercise 1.4 Suppose I am interested in Canadians’ overall happiness. I am going to call them by phone to ask them questions about happiness. In what way will my sample be biased, and which bias is it? 1.3.3 Sampling Schemes There are many sampling methods (a.k.a sampling schemes or protocols) that can be used to select a sample. These can be divided into two categories: non-probabilistic and probabilistic sampling schemes. 1.3.3.1 Non-Probabilistic Sampling Schemes Non-probabilistic sampling schemes are ones where the samples are not chosen randomly. These include Convenience sample: units that are convenient for us to collect data from. Self-selection: units who volunteer to be a part of the sample. Judgment sampling: units are selected based on our judgment. Quota sampling: first divide the population into mutually exclusive subsets, then in each subset, select some units based on judgment. Since with non-probabilistic sampling, the samples are not chosen randomly, they become more deterministic. This means that the variability of the statistics calculated from these samples will be low, i.e., the different possible samples will not be very different from each other. However, because of the non-randomness, there are potentially unmeasured variables where our judgment cannot cover, thus the sample may not be representative of the population. In this sense, the bias will be high. Example 1.14 Continue with our rent example. A convenience sampling scheme will be to ask our friends about their monthly rent. A self-selection scheme would be to hold a billboard on campus to ask for volunteers. A judgment sampling scheme would be to select people based on our intuition or judgment. The samples obtained by these sampling schemes will likely be biased. This is because we are only asking people that we like/people who volunteer. These people may have the same preference about housing and because of that, our sample will not be representative of the population and is thus biased. 1.3.3.2 Probabilistic Sampling Schemes Probabilistic sampling schemes select samples in a random way. The randomness will eliminate the effect of any unmeasured variables, making the sample more representative and the bias will be reduced. Common schemes include Simple random sampling (SRS): List all the units in the population, then randomly choose a fixed number of units in the list. In this scheme, each unit has the same chance of being selected. However, it is hard to conduct this sampling scheme because we are usually not able to get a full list of all the units in our population. Stratified random sampling: Divide the population into mutually exclusive subgroups called strata. Then we apply an SRS on each stratum. Stratified sampling helps us increase the representativeness level of our sample. At the same time, it will decrease the variability because we are randomly selecting units from smaller sub-populations. Cluster sampling: The units in the population are grouped into clusters. These clusters are usually pre-defined, such as provinces, cities, classes, etc. We then do an SRS on the clusters to select several specific clusters. After that, we include all the units in the chosen clusters in our sample. Because the clusters are usually pre-defined, cluster sampling is less costly. Depending on the characteristics of the population and the clusters, cluster sampling can increase or decrease the variability. Figure 1.4: Stratified and cluster sampling Example 1.15 If I were to use stratified sampling, I will divide UWaterloo students according to each faculty. In each faculty, I will randomly choose students to be included in my sample. However, it can be hard to get a full list of the students in each of the faculty. If I were to use cluster sampling, I will randomly select a number of courses from UWaterloo’s course list. I will then go to ask all the students in those chosen courses about their monthly rent. Exercise 1.5 In which situations will cluster sampling will decrease or increase variability? We can also combine different sampling schemes by applying each scheme at a different stage. This is called multiple-stage sampling. Example 1.16 In our rent example, a multi-stage sampling scheme I can use is that I will divide the population into faculties, and in each faculty, I sample among the courses given by that faculty. Then I will interview all students taking those selected courses. Exercise 1.6 If my population of interest is Canadian citizens, describe a sampling scheme that you will recommend to me and explain why it will result in a representative sample. 1.3.4 Observational and Experimental Studies Having chosen the sample, we need to think about how we are going to collect the data from the sample. At this stage, we can choose to conduct an observational study or an experimental study. Observational study: is when we collect pre-existing information. In an observational study, we can only observe the existing situation, and thus we can only conclude about the association among the variables5. Experimental study: is when we manipulate (or control) some explanatory variables and then collect the information about the response variables. In an experiment, we can observe the change in the response variables when the explanatory variable is intentionally changed. With data collected from the experiment, we may conclude about the causal relationship between the explanatory and the response variables. Example 1.17 Coming back to the rent example. If I choose to ask students about the rent they are currently paying, I am conducting an observational study. I can also collect information about, say, the price of their winter coat. I may find that the higher the price of their winter coat, the higher the monthly rent they pay. I can say that rent expense and winter coat price are associated with each other. But I cannot conclude that high winter coat price causes high rent expense. If instead I choose to conduct an experimental study in this way: I give some students in my sample cheaper winter coats, and some other students more expensive winter coats. After that, I let them choose a place to rent among a list of places with different prices. Now, if I find that the students who are given expensive coats choose more expensive places to rent, I may be able to conclude that high winter coat price causes high rent expense because students’ rent choice was made given the price of the winter coat. Experiments can be very useful compared to observational studies because it can support causal conclusions. Indeed, if we can find the cause of something, we can change/manipulate the cause to get the desired result. However, in many situations, experiments are not possible at all due to time, money, or ethical reasons. For example, if I want to know if smoking causes lung cancer, I cannot force any person to smoke! In those cases, observational study will be our only choice. Example 1.18 In Example 1.17, suppose I choose to conduct the experiment I described. In what situation will my conclusion about the causal relationship between winter coat price and rent expense go wrong? 1.3.4.1 Confounding Variables In an observational study, because we only observe the current, existing situation, we will only be able to make conclusions about the association among the variables. The best effort that we can make in an observational study is to try to obtain a sample that is as representative as possible so that our conclusion about the association is accurate and generalizable to the population. On the contrary, even in an experiment, a causal conclusion (A causes B) can still be difficult to make due to confounders. A confounder (a.k.a confounding or lurking variable) is a variable that influences both the response variable and the explanatory variable. Because of the confounder, we cannot separate the effect of the explanatory variable from the effect of the confounder on the response variable. In other words, we cannot conclude whether the change in the response variable is due to the explanatory variable or due to the confounder. Example 1.19 Winter coat price and rent expense may both be influenced by budget. People with higher budgets may tend to buy more expensive winter coats and rent more expensive places. So even with the experiment described in the Example 1.17, we cannot conclude whether the higher rent we observe is due to a more expensive winter coat or because of a higher budget. This can be illustrated in Figure 1.5. Figure 1.5: Relationships of winter coat price, budget and rent expense. 1.3.4.2 Neutralizing Confounders To neutralize the effect of a confounder, we can control the confounder at specific levels, and/or randomly assign the experimental conditions to sample units. In practice, to neutralize the effects of all possible confounders, scientists try to randomly assign the experimental conditions to sample units. These are called random controlled trials (RCTs). With RCTs, causal conclusions can be made with better confidence. Example 1.20 To eliminate the effect of budget, I can give all students in my samples the same budget. Then I proceed to give some of them expensive coats and some of them cheaper coats and let them choose among different places of different prices. If the group who receive expensive coats still choose places with higher rent, I am more confident to say that winter coat price causes rent expense. However, be aware that this causal relationship is only happening on the level of budget that I give them. We are still not sure if I give them more or less budget, the relationship still exists or not. I can also try to randomly divide my samples into two groups, one group will be given expensive coats and one group will be given cheaper coats. Because of the randomization, each group will hopefully have the same variety of budgets, and the association between winter coats and budget is removed. Now we can make a causal conclusion. Exercise 1.7 Researchers are interested in investigating whether smoking causes lung cancer. Identify the response variable of interest. Identify the explanatory variable of interest. Describe a possible confounding variable. 1.4 Data Analysis Having collected the data we need, we can proceed to analyze the data. In doing so, we can provide both descriptive and inferential statistics. Descriptive statistics helps us describe, show or summarize our data set in a meaningful way. Some examples of descriptive statistical techniques are visual plots or summary statistics (for example, average of rent in our sample). With descriptive statistics, we can get a better sense of the data. We can also notice some pattern in our data. However, these impressions are only valid for our sample. Inferential statistics helps us make generalizations or conclusions about the population. In other words, it helps us move from the sample to the population. Because we do not have data about the whole population, the conclusions will be made with uncertainty. Inferential statistics attempts to give us a sense of the level of uncertainty associated with our conclusion. In the next chapter, we will cover some topics in descriptive statistics. The rest of the course will be dedicated to inferential statistics. The term statistics usually does not include the study of data input, storage, retrieval, or data management in general. These topics are covered in data engineering.↩︎ Sometimes you can know some of the population parameter. Think about the Canadian Census.↩︎ This is not always the corresponding statistic. Sometimes we will modify them a bit to better estimate the population parameter.↩︎ This is a frequentist point of view. Bayesian statistics assumes that the population parameters are random. Nevertheless, in this course, we will only focus on frequentist statistics, which is the most popular in practice.↩︎ Association does not imply causation. You can see some examples of spurious associations (two variables that are related to each other but they do not cause one another) here.↩︎ "],["chap-des.html", "2 Descriptive Statistics 2.1 Types of Variables 2.2 Summaries of Qualitative Variables 2.3 Summaries of Quantitative Variables", " 2 Descriptive Statistics In the previous chapter, I have mentioned that there are (i) descriptive statistics that describes the sample data we collected, and (ii) inferential statistics that helps us make conclusions about the population using the sample. In this chapter, we will learn about basic descriptive statistics and methods to summarize the sample data. There are two main ways to summarize the data: visual presentation (plots) and numeric summary. The type of plots and numeric summaries differ based on the type of the data variables that we are considering. 2.1 Types of Variables We have discussed briefly in the last lesson that there are two types of variables: quantitative or qualitative. The table below gives a more detailed comparison between these two types of variables. Quantitative (numeric) variable Qualitative (categorical) variable - Takes on numeric values - Takes on levels/categories - Quantifies some aspect of an individual or object - Qualifies some aspect of an individual or object - Can be further classified as discrete or continuous - Can be further classified as ordinal  (have a natural ordering) or non-ordinal - Can be binary (two levels) or categorical (more than two levels) - Examples include: Time, Height, Weight, Age, Temperature, Price, etc. - Examples include: Color, Race, Gender, Province, Faculty, Blood type, etc. 2.2 Summaries of Qualitative Variables 2.2.1 Frequency Qualitative variables can be summarized by frequency: the number of units/observations (the count) in each category relative frequency: the proportion of observations in each category \\[\\frac{\\text{frequency}}{\\text{total number of observations}}\\] precentage relative frequency: the relative frequency expressed as a percentage \\[\\frac{\\text{frequency}}{\\text{total number of observations}}\\times 100\\%\\] 2.2.2 Plots The frequencies can be plotted using a bar plot: can be used for comparison among categories. The \\(x\\)-axis typically represents the different categories and the \\(y\\)-axis can then capture the frequency, relative frequency or % relative frequency. In a bar plot, the categories can be sorted in any order. However, it is usually prefered to order the categories by some (natural) ordering for clarity. There is usually a space in between the bars to separate the categories. pie plot: can be used to show the composition of the variable (i.e., share of total of each category). Typically the (percentage) relative frequency of each category is plotted within a circle so that the area for each category adds up to the area of the whole circle (100%) Notes: It is best to illustrate these plots using an example. We will continue the students’ rent example in Chapter 1. And from this chapter, we will work with the data set fakeRent.csv. We will assume that this data set records the sample I collected to study UWaterloo students’ rent6. Learn more about the data set in Appendix C. From this chapter, we will also include R codes and examples to illustrate concepts and help you analyze real data sets in your future. Learn about R in Appendix A. Example 2.1 Suppose I am interested in the faculty of students in the sample I collected in the fakeRent.csv data set. I can open the file in Excel and calculate the frequency and relative frequency as follows Faculty Frequency % Relative frequency Arts \\(160\\) \\(\\frac{160}{1345}\\times 100 = 11.9\\) Engineering \\(246\\) \\(18.3\\) Environment \\(125\\) \\(9.3\\) Health \\(199\\) \\(14.8\\) Math \\(376\\) \\(27.9\\) Science \\(239\\) \\(17.8\\) Total 1345 100 Or I can calculate the frequency and % relative frequency in R with the following code # read the data set and save it as &quot;rent&quot; in the environment rent &lt;- read.csv(&quot;fakeRent.csv&quot;) # calculate the frequency table and save it as &quot;fac.freq&quot; in the R environment fac.freq &lt;- table(rent$faculty) # print the frequency table fac.freq ## ## Arts Engineering Environment Health Math Science ## 160 246 125 199 376 239 # calculate the relative frequency table and save it as &quot;fac.relfreq&quot; in the R environment fac.relfreq &lt;- table(rent$faculty)/nrow(rent) #nrow(rent) gives the number of students observed in the &quot;rent&quot; data set # print the relative frequency table fac.relfreq ## ## Arts Engineering Environment Health Math Science ## 0.1189591 0.1828996 0.0929368 0.1479554 0.2795539 0.1776952 Using these frequency tables, we can plot a bar plot in R. # plot the bar plot of the calculated frequency table bp &lt;- barplot(fac.relfreq, # frequency table space = 0.5, # space in between the bars cex.names = 0.6, # font size of bar names in the x-axis ylim = c(0, 0.32), # limits for y-axis ylab = &quot;% Relative frequency&quot;, # label of the y-axis main = &quot;Bar plot for the faculty of students&quot;) # title of the plot # annotate numbers at the top of the barplot text(bp, # the barplot fac.relfreq+0.02, # positions of the numbers labels=paste0(round(fac.relfreq*100, 1),&quot;%&quot;), # the numbers, as percentage cex=0.8) # size of the annotated numbers Figure 2.1: Bar plot of students’ faculties in the sample data We can also create a pie plot. # create the pie plot pie(fac.relfreq, # the frequency table labels = paste0(round(fac.relfreq*100, 1),&quot;%&quot;), # the number annotated in the pie plot clockwise = TRUE, # whether the pie will plot the data clock-wise col = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;violet&quot;), # colors of the pie slices main = &quot;Pie plot of faculty&quot;) # title of the plot # drawing a legend in the plot legend(&quot;bottomleft&quot;, # position of the legend relative to the plot legend = names(fac.relfreq), # names of the pie slices (i.e., the column names of the frequency table) fill = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;violet&quot;), # colors of the pie slices cex = 0.7) # size of the legend&#39;s words Figure 2.2: Pie plot of students’ faculties in the sample data To present data about two qualitative variables, we can plot a side-by-side or a stacked bar plot. Example 2.2 Now, suppose that I am interested in both students’ faculty and gender. I can calculate a two-way frequency table. # calculate a two-way frequency table by gender and faculty and save it as an object called &quot;fac.gender&quot; in R environment fac.gender &lt;- table(rent$gender, rent$faculty) # print the calculated frequency table fac.gender ## ## Arts Engineering Environment Health Math Science ## Female 80 52 58 104 133 73 ## Male 63 157 46 62 177 120 ## Others 17 37 21 33 66 46 The side-by-side bar plot of students’ faculty and gender can be plotted using the following R code: # create a side-by-side bar plot bp2 &lt;- barplot(fac.gender, cex.names = 0.6, ylab = &quot;frequency&quot;, main = &quot;Number of students by faculty and gender&quot;, beside = TRUE, # whether the bar plot is side-by-side col = c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;), # colors of the different genders ylim = c(0, 190)) # add text annotation to the bar plot text(bp2, fac.gender+8, fac.gender, # now the numbers are the counts instead of percentages, because fac.gender is a frequency table, not a % relative frequency table as in Figure 2.1 cex=0.8) # add a legend to the bar plot legend(&quot;topleft&quot;, legend = rownames(fac.gender), fill = c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;), cex = 1) Figure 2.3: Counts of students by faculty and gender From the plot, we can see that there are some faculties where the number of males is more than that of females and on the other hand, there are faculties where the opposite applies. Remember that this result is only reflecting the sample that I collected, it does not necessarily reflect the real distribution of students in the population. That is the case only when our sample is large and representative of the population. To plot a stacked bar plot, change the beside argument from TRUE to FALSE. However, it is not straightforward to annotate the numbers into the stacked bar plot. Look at the code below. # create a side-by-side bar plot bp2 &lt;- barplot(fac.gender, cex.names = 0.6, ylab = &quot;frequency&quot;, main = &quot;Number of students by faculty and gender&quot;, beside = FALSE, # now the bar plot is stacked instead of side-by-side col = c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;), ylim = c(0, 500)) # calculate tables to input as annotated texts in the plot fac.gender.txt &lt;- t(fac.gender) # function to calculate the the y-coordinate at the middle of each stacked bar # you do not need to understand this function in details, just run the code stacked_position &lt;- function(mat) { ret &lt;- mat for (i in 1:ncol(mat)) { if (i == 1) { ret[,i] &lt;- mat[,i]/2 } else { ret[,i] &lt;- ret[,i-1] + mat[,i-1]/2 + mat[,i]/2 } } ret } # calculate the position of the annotated texts in the plot using the stacked_position function defined above fac.gender.txt.position &lt;- stacked_position(fac.gender.txt) # add text annotation to the bar plot text(bp2, fac.gender.txt.position, fac.gender.txt, cex=0.8) # add a legend to the bar plot legend(&quot;topleft&quot;, legend = rownames(fac.gender), fill = c(&quot;red&quot;, &quot;green&quot;, &quot;yellow&quot;), cex = 1) Figure 2.4: Counts of students by faculty and gender Exercise 2.1 When will the data I collect likely represent the real distribution of students in UWaterloo? Recreate the stacked bar plot but with % relative frequency instead of frequency table. 2.3 Summaries of Quantitative Variables 2.3.1 Stem Plots For a small sample size (small number of sampled observations), one quantitative variable can be presented using a stem (or stem-and-leaf) plot. Example 2.3 Consider the first 24 students in the fakeRent.csv data set. Their rents are \\[\\begin{array}{cccccccc} 890 &amp; 1120 &amp; 590 &amp; 830 &amp; 910 &amp; 940 &amp; 550 &amp; 850 \\\\ 930 &amp; 1040 &amp; 860 &amp; 720 &amp; 1020 &amp; 540 &amp; 680 &amp; 700 \\\\ 760 &amp; 730 &amp; 730 &amp; 910 &amp; 960 &amp; 350 &amp; 940 &amp; 710 \\end{array}\\] The stem plot for this 24 observations can be plotted in R using the following code: # save the first 24 rent data into a variable called &quot;dat&quot; in the R environment dat &lt;- rent$rent[1:24] # create the stem plot stem(dat, # the data to be plotted scale = 2) # scale = 1 gives even-numbered stems, scale = 2 gives all available stems ## ## The decimal point is 2 digit(s) to the right of the | ## ## 3 | 5 ## 4 | ## 5 | 459 ## 6 | 8 ## 7 | 012336 ## 8 | 3569 ## 9 | 113446 ## 10 | 24 ## 11 | 2 There are three parts in a stem plot: legend: the legend tells us about the number of digits that the decimal point is to the right of |. In other words, this tells us how the number is split. In the previous example, the legend says that the decimal point is two digits to the right of |. For example, consider the first number in the plot To know the first number, we start by placing the decimal point at the | then move it to the right or left as specified in the legend. In the above picture, we move the decimal point two digits to the right and get 350.0 stem: is the number to be used to categorize the observations. In Example 2.3, the stem is the hundred. leaves: each leaf represents an observation in our data set. In a stem plot the leaves are ordered in ascending order. From the plot in Example 2.3, we can see that there are some “leaves” that are repeated twice. This means that there are two observations having the same value. In the above plot, these are 730, 910, 940. The stem plot is pros: easy to create and individual observations (data points) are maintained. cons: not practical when data set is large. 2.3.2 Quantiles An \\(x\\)-quantile of a distribution is the value at which \\(x\\) of the data lies below this value. A \\(x\\)-percentile of a distribution is the value at which \\(x\\) percent of the data lies below this value. For example, a \\(0.5\\)-quantile (\\(50\\%\\)-percentile) is the value at which 50% of the data lies below this value, i.e., half of the data lies below this value and the other half of the data lies above this value. The \\(0.5\\)-quantile is an important descriptive quantity and is called the median of a distribution. To find the median, we follow the following steps. Suppose we have \\(n\\) observations, then Sort the data from the smallest to largest If \\(n\\) is odd, then the median is the value at the \\(\\frac{n+1}{2}\\)th position If \\(n\\) is even, then the median is the average of the \\(\\frac{n}{2}\\)th and \\((\\frac{n}{2}+1)\\)th value. Example 2.4 Consider again the first 24 values of rent in Example 2.3. The stem plot helped me sort the values. Now since I have \\(n=24\\) observation, according to step 3, the median will be the average between the \\(\\frac{24}{2}=12\\)th and the \\(12+1 = 13\\)th position. Those are the numbers highlighted in the above stem plot: \\(830\\) and \\(850\\). The median is \\[\\frac{830+850}{2} = 840.\\] If I omit the last observation of the data in Example 2.4, i.e., \\(710\\), what will be the median of my data now? Generally, to find \\(x\\)-quantile, we follow the following steps Sort the data from smallest to largest The position of interest is \\(n \\times x\\). If this number is not an integer, we round this up to the next whole number. The value in that position is our \\(x\\)-quantile. If this number is an integer, the \\(x\\)-quantile is the average of the values in position \\(n \\times x\\) and \\(n \\times x + 1\\). Example 2.5 Consider the sorted data (\\(n=12\\)) \\[35, 54, 55, 59, 68, 70, 71, 72, 73, 73, 76, 80\\] Suppose we want to find the \\(0.3\\)-quantile. The position of our interest will then be \\(12 \\times 0.3 = 3.6\\). Because this is not an integer, we need to round this up and take the value at the \\(4\\)rd position. So the \\(0.3\\)-quantile is \\(59\\). We can also calculate this in R. # save the data into a variable called &quot;dat&quot; dat &lt;- c(35, 54, 55, 59, 68, 70, 71, 72, 73, 73, 76, 80) # calculate the 0.3-quantile quantile(dat, # the data whose quantile is to be calculated probs = 0.3, # the probability at which the quantile is calculated type = 2) # the type of quantile-finding algorithm to use ## 30% ## 59 Now suppose we want to find the \\(0.75\\)-quantile. The position of interest is \\(12 \\times 0.75 = 9\\), which is an integer. We then take the average of the values in the \\(9\\)th and \\(10\\)th position, i.e., \\((73+73)/2 = 73\\). The R code is then quantile(dat, probs = 0.75, type = 2) ## 75% ## 73 2.3.3 Five-number Summary Besides the median, there are two other important quantiles that are usually used in statistics: the \\(25\\) and \\(75\\) percentiles (or the \\(0.25\\)- and \\(0.75\\)-quantiles). We call the \\(25\\) percentile the first quartile, and denote it by Q1, the median, i.e., the \\(50\\) percentile, the second quartile and denote it by Q2, the \\(75\\) percentile the third quartile, and denote it by Q3. Example 2.6 Continue with Example 2.4. Q1 is the \\(0.25\\)-quantile, so the position we are looking for is \\(24\\times 0.25 = 6\\). Because this is an integer, we take the average of the values at the \\(6\\)th and the \\(6+1 = 7\\)th position, which gives us \\[Q1 = \\frac{700+710}{2} = 705.\\] Q3 is the \\(0.75\\)-quantile, so the position we are looking for is \\(24\\times 0.75 = 18\\). Because this is an integer, we take the average of the values of the \\(18\\)th and the \\(19\\)th position, which is 930 and 940. Then Q3 is \\[Q3 = \\frac{930+940}{2} = 935.\\] Q1, Q2 (the median) and Q3, together with the minimum and the maximum value, are called the five-number summary of the variable. The five-number summary roughly tells us about the distribution of the variable. Example 2.7 In the above Example 2.6, the five-number summary of the \\(24\\) observations of student rent is Minimum: \\(350\\) Q1: \\(705\\) Median (Q2): \\(840\\) Q3: \\(935\\) Maximum: \\(1120\\) 2.3.4 Box Plots Box plots helps us represent the distribution of a quantitative variables using the five-number summary. Figure 2.5: Elements of a box plot To draw a box plot, we follow the following steps calculate the five-number summary of the data, draw a box from Q1 to Q3, draw a line at Q2 calculate the interquartile range (IQR) \\[IQR = Q3 - Q1\\] calculate the upper (UL) and lower limits (LL) \\[\\begin{align*} LL &amp; = Q1 - 1.5 \\times IQR \\\\ UL &amp; = Q3 + 1.5 \\times IQR \\end{align*}\\] draw the upper whisker up until the maximum value which is smaller than the UL, draw the lower whisker up until the minimum value which is larger than the LL, sometimes dotted lines are drawn at the UL and the LL, denote any point outside the whiskers by dot(s). These points are called the outliers, i.e., the extreme values when compared to the other values in the data. Example 2.8 Consider again the data in Example 2.3. From Example 2.7, we know the five-number summary Minimum: \\(350\\) Q1: \\(705\\) Median (Q2): \\(840\\) Q3: \\(935\\) Maximum: \\(1120\\) The interquartile is \\[IQR = 935-705 = 230\\] LL and UL are \\[LL = Q1 - IQR \\times 1.5 = 705-230 \\times 1.5 = 360\\] \\[UL = Q3 + IQR \\times 1.5 = 935+230 \\times 1.5 = 1280\\] The maximum value smaller than UL is \\(1120\\). The minimum value larger than LL is \\(540\\). We can draw the box plot as follows # save the first 24 students&#39; rent into an object called &quot;dat&quot; dat &lt;- rent$rent[1:24] # draw the box plot boxplot(dat, # data to be plotted main = &quot;Box plot of students&#39; rent&quot;, # title of the plot xlab = &quot;rent&quot;, # label of the x-axis horizontal = TRUE) # whether the box plot is horizontal or vertical Figure 2.6: Box plot of rents for the first 24 students in the data set We can also draw side-by-side box-plots to compare the distribution of a quantitative variable across different categories of a qualitative variable. Example 2.9 Suppose we want to compare the rent between different genders. boxplot(rent~gender, # box plot of rent column by gender column in the rent data set data = rent, # the data set to be plotted from xlab = &quot;rent&quot;, horizontal = FALSE, main = &quot;Box plot of students&#39; rent&quot;) Figure 2.7: Box plot of students’ rents by gender We can see from the plot that male students seem to pay less rent than other genders. Again, this is just an observation in our sample, we should not carelessly generalize this conclusion to the whole population. 2.3.5 Histogram Another great way to view the distribution of quantitative data is to plot a histogram. The histogram is the analogue of bar plots for quantitative data, in which the categories are intervals of values and the height of the bars are the frequency or the (percentage) relative frequency of values that lie within the intervals. In a histogram, these intervals (categories) are called the bins. Conventionally, we design the bins so that they cover the whole range of the variable (i.e., from the minimum to the maximum) and the the bins (intervals) are of the same length. Usually, we divide the range into 5 to 20 bins. Note that there is no gap in between the bins of a histogram (in contrast to the bar plot for qualitative data). This shows the continuity of scale of quantitative variables. Example 2.10 To draw a histogram of rent in the fakeRent.csv data, we use the code hist(rent$rent, # data to be plotted breaks = 10, # number of breaks to divide the range into bins of equal length main = &quot;Histogram of students&#39; of rent&quot;, xlab = &quot;rent&quot;) Figure 2.8: Histogram of students’ rent In first plot (Figure 2.8), there are 9 bins. We can zoom more closely into the data by drawing another histogram using more bins (Figure 2.9). Figure 2.9: Histogram of students’ rent Histogram’s advantages and disadvantages: pros: visually summarize data in a nice way. With histograms, we can get an idea about the center: the “middle point” of the distribution spread of the distribution shape of the distribution outliers: extreme observations that fall outside of the common range of other observations. cons: unlike the stem plots, we cannot know the exact value of an observation in histograms. 2.3.6 Measure of Centrality A measure of centrality is a numeric summary which gives us one value that lies in the center of the whole distribution. There are three common centrality measures mode: the value that is most popular in the data, the value that most individuals/units have may be a candidate the represent the whole distribution. you will see that even though the rationale of the mode is sound, it is the least popular because there can be more than one modes there is no universal formula to calculate the mode for all distribution, it is not always representative and informative. median: the point that divides the data into two halves. (We discussed this in the Section 2.3.2 ) the median lies in the middle of the distribution and it gives us a sense of the “middle point”. even though this one is more popular than the mode, it is still less popular than the mean because there is a universal algorithm to find the median but the algebra is not nice (for anyone familiar with calculus, the formula of the median is usually not smooth7). mean: the average (or “arithmetic mean”) of the data, suppose the data I collect the data \\(x_1, x_2, ..., x_n\\) (\\(n\\) observations), then the mean is \\[\\frac{x_1+x_2 + ... + x_n}{n}\\] notation: \\(\\bar{x}\\) the mean is the closest to all other values on average and it is the middle of the values this is the most popular measure of centrality because there is a universal formula to find the mean the algebra is nice the mean has good properties that we will look at in Chapter 6. Example 2.11 Come back to the data in Example 2.3 \\[\\begin{array}{cccccccc} 890 &amp; 1120 &amp; 590 &amp; 830 &amp; 910 &amp; 940 &amp; 550 &amp; 850 \\\\ 930 &amp; 1040 &amp; 860 &amp; 720 &amp; 1020 &amp; 540 &amp; 680 &amp; 700 \\\\ 760 &amp; 730 &amp; 730 &amp; 910 &amp; 960 &amp; 350 &amp; 940 &amp; 710 \\end{array}\\] The mode is the value that appears most of the time. As we discussed in Example 2.3, there are 3 values that repeats two times: \\(730\\), \\(910\\), \\(940\\). These are the modes of the distribution. As it shows in this example, the mode is usually not informative about the centrality of the distribution. The median is \\(840\\) as we discussed in Example 2.4. The mean is \\[\\frac{890 + 1120 + 590 + ... + 940 + 710}{24} = 802.5\\] Example 2.12 We can calculate the mean, median, and modes using R # mean of the rent column in the rent data set mean(rent$rent) ## [1] 767.0409 # median of the rent median(rent$rent) ## [1] 760 # or quantile(rent$rent, probs = 0.5, type = 2) ## 50% ## 760 # a function to calculate the modes # you do not need to understand the function in details, just run the code stat_mode &lt;- function(vec) { vals &lt;- unique(vec) counts &lt;- sapply(1:length(vals), function(i) {sum(vec == vals[i])}) vals[which(counts == max(counts))] } # mode of the rent stat_mode(rent$rent) ## [1] 720 2.3.7 Measure of Variability The variability or dispersion of a variable gives us a sense of the spread (i.e., how different the values in our data are) of the distribution of that variable. Some measures of variability is range: distance between the largest and the smallest value in the data set Formula: Range = Maximum value - Minimum value this gives us the range of all the values in our data interquartile range (IQR): the distance covered by the middle 50% of the data Formula: IQR = Q3 - Q1 this gives us the range of the central values in our data standard deviation: this gives us the average difference of the values from the mean notation: \\(s\\) suppose we have the data \\(x_1, x_2, ..., x_n\\) one may think that we can simply average the differences \\[\\frac{(x_1 - \\bar{x}) + (x_2-\\bar{x}) + ... + (x_n-\\bar{x})}{n} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})}{n},\\] however, if you pay closer attention, this formula will always be zero no matter what the values \\(x_1, x_2, ..., x_n\\) are. This means this formula will not give us any information about the spread of the distribution. another viable option is to take the absolute value of the differences \\[\\frac{|x_1 - \\bar{x}| + |x_2-\\bar{x}| + ... + |x_n-\\bar{x}|}{n} = \\frac{\\sum_{i=1}^n |x_i-\\bar{x}|}{n}.\\] This is a good option, but the algebra is not nice (i.e., again not smooth) finally, we can choose to square the differences, calculate the average then take the square root. In fact, this is the formula of the standard deviation \\[\\begin{align*} s &amp; = \\sqrt{\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}} \\\\ &amp; = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2-\\bar{x})^2 + ... + (x_n-\\bar{x})^2}{n-1}} \\\\ &amp; = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}{n-1}}. \\end{align*}\\] the variance is just the square of the standard deviation \\[s^2 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] you may wonder why we divide by \\(n-1\\) instead of \\(n\\). We will explain this in later chapters. the variance and the standard deviation are always positive. the standard deviation, the mean, mode and median all have the same unit as the raw data. For example, the standard deviation of the rent data will be in dollars ($). the variance has the unit squared. In the rent data, the unit of the variance is \\(\\$^2\\). Example 2.13 Again consider the data in Example 2.3 \\[\\begin{array}{cccccccc} 890 &amp; 1120 &amp; 590 &amp; 830 &amp; 910 &amp; 940 &amp; 550 &amp; 850 \\\\ 930 &amp; 1040 &amp; 860 &amp; 720 &amp; 1020 &amp; 540 &amp; 680 &amp; 700 \\\\ 760 &amp; 730 &amp; 730 &amp; 910 &amp; 960 &amp; 350 &amp; 940 &amp; 710 \\end{array}\\] From Example 2.11, the mean is \\(\\$ 802.5\\). The variance is \\[\\begin{align*} s^2 &amp; = \\frac{\\sum{i=1}^24 x_i^2 - 24\\bar{x}^2}{24-1} \\\\ &amp; = \\frac{[890^2 + 1120^2 + ... + 940^2 + 710^2] - [25\\times 802.5^2]}{25-1} \\\\ &amp; = 32880.43 \\quad (\\$^2) \\end{align*}\\] The standard deviation is \\[s = \\sqrt{32880.43} = 181.3296 \\hspace{2mm} (\\$)\\] We can also calculate the variance and standard deviation in R. Example 2.14 The variance and standard deviation of students’ rent in the fakeRent.csv data set is # variance var(rent$rent) ## [1] 19045.7 # standard deviation sd(rent$rent) ## [1] 138.0062 # or standard deviation = square root of variance sqrt(var(rent$rent)) ## [1] 138.0062 Exercise 2.2 Prove that for all data \\(x_1, x_2, ..., x_n\\) the following hold \\[\\frac{\\sum_{i=1}^n (x_i-\\bar{x})}{n} = 0\\] \\[s^2 = \\frac{\\sum_{i=1}^n x_i^2 - n\\bar{x}^2}{n-1}\\] where, by the formula, \\[\\begin{align*} \\bar{x} &amp; = \\frac{\\sum_{i=1}^n x_i}{n}, \\\\ s &amp; = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}. \\end{align*}\\] 2.3.8 Shape of the Distribution The stem plots, box plots, histograms help us understand the shape of the distribution. We can describe the shape of the distribution using the number of modes and the skewness of the distributions. 2.3.8.1 Number of Modes A distribution can be unimodal: the data is piled around a single peak bimodal: the data is piled around two peaks multimodal: the data is piled around multiple peaks Figure 2.10: The number of modes. 2.3.8.2 Skewness There are three types of skewness symmetric: a distribution is symmetric if the right and the left of the distribution are approximately mirror images of each other the values are distributed equally around the center we have: Mean = Median = Mode Figure 2.11: Symmetric distribution left-skewed a distribution is skewed to the left if the left side extends further out than the right side, i.e., the distribution has a longer left tail imagine someone pulls the left tail of a symmetric distribution to the right half of the observations spread on the left side and the other half of the observations concentrate on the right of the distribution, so the median is on the right of the value range. we have: Mean &lt; Median Figure 2.12: Left-skewed distribution right-skewed a distribution is skewed to the right if the right side extends further out than the left side, i.e., the distribution has a longer right tail imagine someone pulls the right tail of a symmetric distribution to the right half of the observations spread on the right side and the other half of the observations concentrate on the left of the distribution, so the median is on the left of the value range. we have: Mean &gt; Median Figure 2.13: Right-skewed distribution Example 2.15 Look at the box plot in Figure 2.6, the Q2 line is slightly to the right of the middle of the box, we can say that the distribution is slightly skewed to the left. Look at the histograms in Figure 2.8 and 2.9, since the two tails look similar to each other, we say that the distribution is symmetric. 2.3.9 Robustness to outliers We are introduced to outliers in Section 2.3.4 where we discuss Box plot. Recall that outliers are the extreme values that lie outside the common range of most other values in our data. When we have outliers in our data, we should ask the question: is the outlier produced by some error in measurement or data record? is there another reason for such an outlier and is it interesting and worth our concern? Many of the times, outliers are produced by errors. Because there are only a few of them, outliers will not help us generalize or make conclusions for the majority of the cases. Therefore, we usually omit the outliers from our sample. Desirably, we want our numeric summaries (or measures, or statistics) to be robust to outliers, i.e., they will not be affected by the existence or removal of outliers. In this lesson so far robust to outliers: mode, median, interquartile range not robust to outliers: mean, range, standard deviation, variance. Notes: In this chapter we have studied two types of descriptive statistics: (i) the plots and (ii) the numeric summaries (measures). Note that these are only applicable to the data we collect, i.e., the sample. In fact, when we talk about numeric summaries of sample data, we usually add the word “sample” in front of the name of the summaries to stress that the quantities were calculated from the sample. For example, sample mean, sample median, sample standard deviation, sample variance, etc. Since the summaries are calculated based on the collected sample, it may not be appropriate to directly generalize some results/patterns from these descriptive statistics to the population. That is, it is usually naive to say “I see this in my sample so this is what happens in the population”. However, when our sample is, as we discussed in Chapter 1, large and representative of the population, these conclusions can be made with good confidence. Again, we will understand this better when we study about inferential statistics. Note that this data is artificially generated for education purpose, not the actual data collected on actual students. More information about the data set can be found in the Appendix.↩︎ mathematically, smooth means continuously differentiable↩︎ "],["chap-prob.html", "3 The Laws of Probability 3.1 What is Probability? 3.2 Addition Rule 3.3 Multiplication Rule 3.4 Law of Total Probability", " 3 The Laws of Probability In the last chapter, we have learned some basic descriptive summaries of sample data. From this chapter onward, we will learn how to generalize and make conclusions about the population from the information of the sample, i.e., we will learn about statistical inference. For such a purpose, we should first learn about probability and its laws, which will help us model the randomness of the sample. 3.1 What is Probability? 3.1.1 Randomness In Chapter 1, we learned something about randomness: we should randomly select the individuals in our population in order to obtain a representative sample of the population we should randomly assign the experimental conditions to sample units so that the effect of confounding variables are neutralized abd we can make causal conclusions from the experiment. In Chapter 1’s context, randomness meant equal chance of happening, which implies that the selection of individuals or the assignment of experiment conditions should be non-deterministic. I guess one can argue whether true randomness exists or whether everything happens by cause and effect. However, let us agree on this definition: Randomness is the state of being unable to be predicted with certainty. 3.1.2 Probabilistic Experiment Following the definition of randomness, we have the definition of a probabilistic experiment8: a repeatable process in which the complexity of the underlying system leads to an outcome that cannot be known ahead of time. Example 3.1 Tossing a coin can be thought of as a probabilistic experiment. First, we also can toss the coin again and again, i.e., we can repeat the coin toss. Second, even though we can argue that the result of tossing a coin may depend on the technique used to toss the coin the force used to toss the coin the air condition of where the coin is tossed the individual who tosses the coin the size of the coin etc., however, since there are considerably many factors that can affect the outcome of a coin toss, it is hard for us to tell exactly whether the outcome is head or tail. Hence, we say that the outcome is random and a coin toss is a probabilistic experiment. Other examples can be: rolling a die, drawing a card from a playing cards, rotating a spinner, etc. Some terminologies related to probabilistic experiments: run/trial: a repetition of the probabilistic experiment. sample space: the collection (set) of all possible outcomes. The outcomes in the sample space must be defined in a way such that they are mutually exclusive (no overlap) and collectively exhaustive (represent all possible scenarios) Notation: \\(\\Omega\\) (pronounce “Omega”). event: a subset of possible outcomes (i.e., some, but not necessarily all possible outcomes). These can be simple events (made up of one possible outcome) or a compound events (made up of two or more outcomes) Notation: capital letters, for example \\(A, B, C\\), etc. Example 3.2 Two coins are tossed and their faces are recorded. Let \\(H\\) denote “head” and \\(T\\) denote “tail”. the sample space: \\(\\Omega = \\{HH, HT, TH, TT\\}\\). Here \\(\\{\\}\\) mathematically denote a set (collection). The sample space is not \\(\\{HH, TT\\}\\) because it is not exhaustive, i.e., it does not contain all possible outcomes. the event that we observe two heads: \\(A = \\{HH\\}\\). This is a simple event. the event that we observe at least one tail: \\(B = \\{HT, TH, TT\\}\\). This is a compound event since it is made up of multiple outcomes in \\(\\Omega\\). Note here that \\(A\\) and \\(B\\) are just our convenient choice of notation, you can also choose other capital letters to represent any other event. Example 3.3 A fair six-sided die is rolled and the value obtained is recorded. the sample space: \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) the event that we observe an even number: \\(A = \\{2, 4, 6\\}\\) the event that the outcome is at most 3: \\(B = \\{1, 2, 3\\}\\). Exercise 3.1 Two students are randomly selected and asked if their rooms have an independent washroom (Y) or not (N). The experiment records the response from both students. Define the sample space \\(\\Omega\\) List the outcomes in event \\(A\\): both students have an independent washroom. Is this a simple or compound event? List the outcomes in event \\(B\\): both students made the same choice. Is this a simple or compound event? List the outcome in event \\(C\\): at most one student have an independent washroom. Is this a simple or compound event? 3.1.3 Three Views of Probability Probability is generally defined as a numeric measure of chance. However, there have been three different major views on how to understand probability. subjective probability: probability is a measure of a person’s belief that some given event will occur. this definition fits the intuitive sense of probability challenge: it can be inconsistent because beliefs can vary from one person to another. classical probability: probability of an event is the proportion of possible outcomes contained in the event compared to the sample space. mathematically, the probability of an event \\(A\\) is denoted as \\(\\mathbb{P}(A)\\) and is defined as \\[\\mathbb{P}(A) = \\frac{\\text{number of outcomes in } A}{\\text{number of outcomes in }\\Omega}\\] this definition is simple and can be applied to many situations assuming that the outcomes are equally likely challenge: it cannot be applied to situations where the outcomes are not equally likely, or there are infinitely many possible outcomes. empirical probability (or frequentist probability, or relative frequency probability): the proportion of times that the event occurs when the probabilistic experiment is repeated a very large (infinite) number of times. mathematically, suppose the probabilistic experiment is repeated \\(n\\) times (\\(n\\) is very large), then the probability of event \\(A\\) is \\[\\mathbb{P}(A) = \\lim_{n\\to\\infty}\\frac{\\text{number of times that outcomes in $A$ are obtained}}{n},\\] i.e., \\(\\mathbb{P}(A)\\) is the relative frequency that the outcomes in \\(A\\) are obtained in \\(n\\) repetitions of the probabilistic experiment. cover more cases than classical probability intuitively agree with classical probability when the outcomes are equally likely challenge: how many times is considered “very large” or large enough to approximate the limit in infinite repetitions? Example 3.4 Consider tossing a fair six-sided die. The sample space is \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\). Because the die is fair, the outcomes are equally likely. Then, according to classical probability: the probability to observe each one of the possible outcomes is \\(\\frac{1}{6}\\) the probability to observe an even number is \\[\\frac{|\\{2,4,6\\}|}{|\\{1,2,3,4,5,6\\}|} = \\frac{3}{6} = \\frac{1}{2}\\] Notation: \\(|A|\\) denotes the number of elements of a set \\(A\\). According to empirical probability, suppose we roll the die for a large number of times and receive, for example \\[4 3 6 5 2 1 6 6 2 3 6 1 1 5 1 4 3 3 6 6 4 1 2 2 5 3 6 4 6 3 2 3 1 2 1 3 4 5 6 2 3 4 3 2 4 6 1 1 4 1...\\] then each number appears roughly \\(\\frac{1}{6}\\) of the times and even numbers appears roughly half (\\(\\frac{1}{2}\\)) of the times. Hence, the probability to observe an even number is \\(\\frac{1}{2}\\). 3.1.4 Mathematical Definition of Probability Although the three views of probability differ, all three agree on the mathematical definition of probability: Probability is a function \\(\\mathbb{P}\\) that maps events to non-negative numbers, which satisfies \\(0 \\le \\mathbb{P}(A) \\le 1\\) for all events \\(A\\) this means a probability must be non-negative and must not exceed 1. \\(\\mathbb{P}(\\Omega) = 1\\) this means the probability of all possible outcomes must be exactly 1. \\(\\mathbb{P}(\\text{union of mutually exclusive events}) = \\text{sum of } \\mathbb{P}( \\text{individual events})\\) if the sample space is finite, i.e., there are finite number of possible outcomes. This implies that the probability of an event is the sum of probability of each individual outcome of the event. We will discuss the case of infinite sample space in Chapter 4. we will learn more about this property in the subsequent sections. Notes: Up to this point we discussed at length about what is probability. The aim is to help you get a good sense of what probability is. In statistics, many of the times we use the empirical view of probability, some of the times we use the subjective probability. However, for now, it is most important that you know the classical and mathematical definitions of probability. Exercise 3.2 An experiment consists of rolling a special fair 8-sided die and recording the number that faces up. All of the numbers are equally likely to occur. Find the probability of each following event. \\(A\\): obtain an even number \\(B\\): obtain a number divisible by 4 \\(C\\): obtain a number less than 6 \\(D\\): obtain a number that is divisible by 2 and 3 3.2 Addition Rule Probability is a function of events, and events are basically sets of possible outcomes. Therefore, to learn about probability calculation, we first need to go over some basics about mathematical sets and Venn diagram. 3.2.1 Venn Diagram It is useful to visualize the relationship existing among the events (sets). Venn diagram is a way to illustrate such relationships visually. A Venn diagram is made up of a rectangle that represents the sample space \\(\\Omega\\) circles within the rectangle which represents the events of interest in \\(\\Omega\\) Example 3.5 Consider tossing 2 coins and the following events \\(A\\): two heads are obtained \\(B\\): one head is obtained \\(C\\): no heads are obtained The Venn diagram that represents these three events is Example 3.6 Consider rolling a six-sided die. We found in Example 3.3 that \\(\\Omega = \\{1,2,3,4,5,6\\}\\). Consider the following events \\[E = \\{1,2,3\\}, \\hspace{3mm} F = \\{2,3,4\\}, \\hspace{3mm} G = \\{1, 5\\}\\] The Venn diagram that represents these three events is Exercise 3.3 Consider a probabilistic experiment with sample space \\(\\Omega = \\{0,1,2,3,4,5,6,7,8,9\\}\\) and the events \\[ A = \\{0,1,2,7,8,9\\}, \\hspace{5mm} B = \\{0,1,2,4,8\\}, \\hspace{5mm} C = \\{0,1,2,9\\}, \\hspace{5mm} D = \\{1,4,9\\} \\] Represent these events using a Venn diagram. 3.2.2 Set Operations Intersection: The intersection of \\(A\\) and \\(B\\) is a set that contains all outcomes in both \\(A\\) and \\(B\\) Notation: \\(A \\cap B\\) (pronounce “A intersection B” or “A and B”) Visually: Union: the union of \\(A\\) and \\(B\\) is a set that contains all outcomes that either in \\(A\\) or in \\(B\\), or in both \\(A\\) and \\(B\\) Notation: \\(A \\cup B\\) (pronounce “A union B” or “A or B”) Visually: Complement: The complement of a set \\(A\\) is a set that contains all outcomes that are not in \\(A\\). Notation: \\(A^c\\) (pronounce “A complement” or “not A”) Visually: \\(A \\cap A^c = \\emptyset\\) (where \\(\\emptyset\\) means “empty set”, i.e., nothing) \\(A \\cup A^c = \\Omega\\) De Morgan’s laws: Look at the below Venn diagram, we have what is called the De Morgan’s laws for sets \\((A\\cup B)^c = A^c \\cap B^c\\) \\((A\\cap B)^c = A^c \\cup B^c\\) Example 3.7 Let us look at the first equality of the De Morgan’s law: \\((A\\cup B)^c = A^c \\cap B^c\\) \\(A\\cup B\\) is the pink, purple, and blue regions \\(\\Rightarrow\\) \\((A\\cup B)^c\\) is the yellow region \\(A^c\\) is the yellow and blue region, \\(B^c\\) is the yellow and pink region \\(\\Rightarrow\\) \\(A^c \\cap B^c\\) is also the yellow region. Hence, \\((A\\cup B)^c = A^c \\cap B^c\\). Exercise 3.4 Use the same way as Example 3.7 to prove the second equality of the De Morgan’s law. 3.2.3 Addition Rule for Mutually Exclusive Events Mutually exclusiveness: \\(A\\) and \\(B\\) are called mutually exclusive if and only if there is no possible outcome lying in both \\(A\\) and \\(B\\). Notation: \\(A \\cap B = \\emptyset\\) Visually: Addition rule: From the third requirement of the mathematical definition of probability in Section 3.1.4, we have: \\[\\text{If } A \\cap B = \\emptyset, \\text{then }\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B)\\] Example 3.8 Consider \\(\\Omega = \\{1,2,3,4,5,6\\}\\) and events \\(A = \\{1,2\\}\\) and \\(B = \\{5\\}\\). Suppose that the outcomes are equally likely, then \\[\\mathbb{P}(A) = \\frac{2}{6} = \\frac{1}{3}, \\hspace{5mm} \\text{and} \\hspace{5mm} \\mathbb{P}(B) = \\frac{1}{6}\\] Because \\(A\\) and \\(B\\) are mutually exclusive, \\[\\mathbb{P}(A\\text{ or }B) = \\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}.\\] Example 3.9 Since \\(A \\cap A^c = \\emptyset\\) and \\(A\\cup A^c = \\Omega\\), so \\[\\mathbb{P}(A) + \\mathbb{P}(A^c) = \\mathbb{P}(A\\cup A^c) = \\mathbb{P}(\\Omega) = 1\\] because based on the mathematical definition of a probability, \\(\\mathbb{P}(\\Omega) = 1\\). From here, we have \\[\\mathbb{P}(A^c) = 1 - \\mathbb{P}(A)\\] Exercise 3.5 Prove that \\(\\mathbb{P}(\\emptyset) = 0\\). Hint: Use the fact that \\(\\Omega^c = \\emptyset\\). 3.2.4 Addition Rule for Non-mutually-exclusive Events Now consider the general case of two events \\(A\\) and \\(B\\), not necessarily mutually exclusive. Again look at the Venn diagram We have \\[\\begin{align*} \\mathbb{P}(A\\cup B) &amp; = \\mathbb{P}(A \\cap B^c) + \\mathbb{P}(A \\cap B) + \\mathbb{P}(A^c \\cap B) \\\\ &amp; = \\Big[\\mathbb{P}(A \\cap B^c) + \\mathbb{P}(A \\cap B) \\Big] + \\Big[\\mathbb{P}(A^c \\cap B) + \\mathbb{P}(A \\cap B)\\Big] - \\mathbb{P}(A \\cap B) \\\\ &amp; = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B) \\end{align*}\\] where the first equation is because the three events are mutually exclusive, the second equation add then subtract \\(\\mathbb{P}(A \\cap B)\\). In words, the probability of \\(A\\cup B\\) is the sum of the probability of the pink, purple, and blue regions. When we sum the probability of \\(A\\) (pink and purple) and the probability of \\(B\\) (blue and purple), we count the purple region twice. Thus, we need to minus the purple region to get the probability of \\(A\\cup B\\). In summary, the general addition rule for any event \\(A\\) and \\(B\\) is \\[\\mathbb{P}(A\\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\] Example 3.10 Suppose that I am collecting the sample for my rent study. The probability of selecting a female student is \\(0.6\\), the probability of selecting an undergraduate student is \\(0.3\\), and the probability of selecting a female undergraduate student is \\(0.2\\). What is the probability of selecting a non-female student? the probability of selecting either a female student or an undergraduate student? the probability of selecting a student who is not female and not undergraduate? Solution: Let \\(A\\) be the event of selecting a female student \\(B\\) be the event of selecting an undergraduate student, then \\[\\mathbb{P}(A) = 0.6, \\hspace{5mm}\\mathbb{P}(B) = 0.3, \\hspace{5mm} \\text{and} \\hspace{5mm}\\mathbb{P}(A\\cap B) = 0.2\\] \\(\\mathbb{P}(A^c) = 1-\\mathbb{P}(A) = 1 - 0.6 = 0.4\\). \\(\\mathbb{P}(A\\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A\\cap B) = 0.6 + 0.3 - 0.2 = 0.7\\) \\(\\mathbb{P}(A^c \\cap B^c) = \\mathbb{P}\\Big[(A\\cup B)^c\\Big] = 1 - \\mathbb{P}(A\\cup B) = 1-0.7 = 0.3\\). Exercise 3.6 Continue with the rent study. Suppose the probability of selecting an undergraduate student is \\(0.3\\), the probability of selecting an Arts students is \\(0.2\\), the probability of selecting an Engineering student is \\(0.4\\), the probability of selecting either an undergraduate student or an Arts student is \\(0.4\\). What is the probability of selecting a graduate student (suppose there are only two types of students: undergraduate and graduate)? selecting an undergraduate Arts student? selecting a student who major in either Arts or Engineering? 3.3 Multiplication Rule 3.3.1 Conditional Probability In Example 3.1, we discussed that the results of tossing a fair coin can be affected by many factors. Usually, for a fair coin, we believe that there is half a chance to observe a head and half a chance to observe a tail. But, what if we know that the person will be tossing the coin is, say, myself, do I know better about the probability? That is, will the half-half probabilities change? Do I myself tend to toss heads slightly more often than tails? This is the main idea of conditional probability. The conditional probability \\(\\mathbb{P}(A|B)\\) is the probability of event \\(A\\) given event \\(B\\), i.e. the probability that event \\(A\\) happens if event \\(B\\) happens with certainty. Let us look at the picture above. If event \\(B\\) happens with certainty, it means that only the outcomes inside of \\(B\\) will be able to happen. This shrinks our sample space from \\(\\Omega\\) to \\(B\\). Now, the only cases where \\(A\\) can happen are when the outcomes lie in both \\(A\\) and \\(B\\), i.e., \\(A\\cap B\\). So the conditional probability of \\(A\\) given \\(B\\) is the proportion of outcomes of \\(A\\cap B\\) in event \\(B\\), that is, \\[\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\] Example 3.11 Continue with Example 3.10, we know the probability of selecting an undergraduate student is \\(\\mathbb{P}(B) = 0.3\\) and the probability of selecting a female undergraduate student is \\(\\mathbb{P}(A\\cap B) = 0.2\\). So the probability of selecting a female student given the student is an undergraduate is \\[\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)} = \\frac{0.2}{0.3} = \\frac{2}{3}\\] 3.3.2 Multiplication Rule We have the multiplication rule \\[\\mathbb{P}(A\\cap B) = \\mathbb{P}(A| B)\\mathbb{P}(B)\\] 3.3.3 Independence We say that event \\(A\\) is independent of event \\(B\\) if we know that no matter \\(B\\) happens or not, the probability of \\(A\\) does not change, that is \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\] Exercise 3.7 Prove that if \\(A\\) is independent of \\(B\\), then \\(B\\) is independent of \\(A\\). Hint: Use the definition of \\(\\mathbb{P}(A|B)\\) and the definition of \\(A\\) being independent of \\(B\\). From Exercise 3.7, we say that two events \\(A\\) and \\(B\\) are independent if either \\[\\mathbb{P}(A|B) = \\mathbb{P}(A) \\hspace{5mm} \\text{or} \\hspace{5mm} \\mathbb{P}(B|A) = \\mathbb{P}(B) \\hspace{5mm} \\text{or} \\hspace{5mm} \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B)\\] Exercise 3.8 In Examples 3.10 and 3.11, is selecting a female student and selecting an undergraduate mutually exclusive? independent? And why? 3.4 Law of Total Probability 3.4.1 Conditional Probability of the Complement In Section 3.3.1, we discussed that \\(\\mathbb{P}(A|B)\\) is the proportion of outcomes in both \\(A\\) and \\(B\\) out of the outcomes in \\(B\\). Therefore, \\(1-\\mathbb{P}(A|B)\\) is the remaining proportion of outcomes not in \\(A\\) and but in \\(B\\). In the diagram below, \\(\\mathbb{P}(A|B)\\) is the ratio of the pink part to the whole circle of \\(B\\) \\(1-\\mathbb{P}(A|B)\\) is the ratio of the blue part to the whole circle of \\(B\\). The blue part contains the outcomes that is not in \\(A\\) but in \\(B\\), i.e., \\(A^c \\cap B\\). So, we have the following equality for the conditional probability of the complement \\[\\mathbb{P}(A^c|B) = 1-\\mathbb{P}(A|B)\\] Example 3.12 Continue with Example 3.11, the probability of selecting a non-female student given that the student is an undergraduate is \\[\\mathbb{P}(A^c|B) = 1 - \\mathbb{P}(A|B) = 1 - \\frac{0.2}{0.3} = \\frac{1}{3}\\] 3.4.2 Law of Total Probability Let us come back to the whole sample space \\(\\Omega\\) in the diagram below. We see that \\(A\\) (pink) and \\(A^c\\) (blue) divide the sample space \\(\\Omega\\) into two mutually exclusive regions. Effectively, they also divide \\(B\\) into two mutually exclusive parts: \\(A \\cap B\\) and \\(A^c \\cap B\\). This means \\[\\mathbb{P}(A\\cap B) + \\mathbb{P}(A^c\\cap B) = \\mathbb{P}(B)\\] This law similarly applies to all events \\(A_1, A_2, ..., A_n\\) that are mutually exclusive and make up the sample space \\(\\Omega\\). This is called the law of total probability: \\[\\text{If }A_1 \\cup A_2 \\cup ... \\cup A_n = \\Omega \\text{ and } A_i\\cap A_j = \\emptyset \\text{ for all }i\\ne j, \\text{ then } \\mathbb{P}(B) = \\sum_{i=1}^n \\mathbb{P}(B\\cap A_i)\\] Look at the above diagram, the events \\(A_1, A_2, ..., A_n\\) divide the sample space \\(\\Omega\\) into mutually exclusive parts, and thus effectively also divide \\(B\\) into mutually exclusive parts \\(A_1\\cap B\\), \\(A_2\\cap B\\), …, \\(A_n\\cap B\\). When we add up these parts we will get \\(B\\). Exercise 3.9 Suppose we have the following probabilities of selecting students Arts Engineering Environment Health Math Science Female \\(0.059\\) \\(0.039\\) \\(0.043\\) \\(0.077\\) \\(0.099\\) \\(0.054\\) What is the probability of selecting a female student? What is the probability of selecting an Engineering student given the student is female? What is the probability of selecting a non-Math student given the student is female? Combining the law of total probability and the multiplication rule, we have \\[\\mathbb{P}(B) = \\sum_{i=1}^n\\mathbb{P}(B\\cap A_i) = \\sum_{i=1}^n\\mathbb{P}(B|A_i)\\mathbb{P}(A_i)\\] Exercise 3.10 Use the law of total probability to prove the conditional probability of the complement. From Exercise 3.10, we know that the conditional probability of the complement is just a special case of the law of total probability where the mutually exclusive events making up the sample space are \\(A\\) and \\(A^c\\). 3.4.3 Tree Diagram Consider the example below Example 3.13 Suppose in UWaterloo, \\(40\\%\\) of students are graduate students. \\(60\\%\\) of graduate students choose rooms that are more than 15 minutes away from school, while \\(80\\%\\) of undergraduate students choose rooms that are less than 15 minutes away from school. Then, what is the probability that students choose rooms that are less than 15 minutes away from school? This example may seem a lot more difficult than the ones we considered in this lesson so far, but trust me, you now already have all the probability laws you need to solve this problem. However, the wording may be confusing. A great way illustrate the events, their relationships and probabilities more clearly is to draw a tree diagram. A tree diagram consists of nodes: at each node, a choice is made, i.e., from this node we list all the mutually exclusive events for the choice branches: at each branch, write the probability \\(\\mathbb{P}(|...)\\) where “…” are the events that occurred before \\(A\\). Example 3.14 Consider an experiment where we flip a coin twice. Let us denote \\(H_1\\): the event that we observe a head at the first coin toss \\(T_1\\): the event that we observe a tail at the first coin toss \\(H_2\\): the event that we observe a head at the second coin toss \\(T_2\\): the event that we observe a tail at the second coin toss Then \\(H_1\\) and \\(T_1\\) are mutually exclusive and they make up the sample space for the results of coin 1. \\(H_2\\) and \\(T_2\\) are mutually exclusive and they make up the sample space for the results of coin 2. Thus we can draw the tree diagram as follows. Example 3.15 Now, let us come back to Example 3.13. Let \\(G\\) be the event that the student is a graduate student. Then the event that the student is an undergraduate student is \\(G^c\\). \\(T\\) be the event that the student chooses a room that is less than 15 minutes away from school. Then the event that the student chooses a room that is more than 15 minutes away from school is \\(T^c\\). We can make the following tree diagram in which we calculate the red probabilities using the complement equality. calculate the blue probabilities using the multiplicative rule. To calculate the probability that students choose rooms that are less than 15 minutes away from school, we use the law of total probability: \\[\\mathbb{P}(T) = \\mathbb{P}(T\\cap G) + \\mathbb{P}(T\\cap G^c) = 0.48+0.16 = 0.64.\\] Exercise 3.11 Use the following information Arts Engineering Environment Health Math Science Female \\(80\\) \\(52\\) \\(58\\) \\(104\\) \\(133\\) \\(73\\) Male \\(63\\) \\(157\\) \\(46\\) \\(62\\) \\(177\\) \\(120\\) Others \\(17\\) \\(37\\) \\(21\\) \\(33\\) \\(66\\) \\(46\\) Draw a tree diagram that represent the above probabilities. Find the probability of selecting a male student What is the probability of selecting a male student given that the student is from the Environment faculty? Are the event: the student is non-male-and-female, and the event: the student is from Health faculty independent? Why? Notes: In the tree diagram examples, we can think of each node as a categorical variable in Example 3.14, the variables are the result of coin 1 and result of coin 2, in Example 3.13, the variables are level of study and time from room to schools the branches coming out of the nodes correspond to different categories of the corresponding variables. in Example 3.14, the categories are head and tail for both variables in Example 3.13, level of study has two categories: graduate and undergraduate; time from room to school has two categories: more than 15 minutes or less than 15 minutes the events are that the categorical variables take the value of each of the available categories. in Example 3.14, \\(H_1\\) is the event that the result of coin 1 is head, \\(T_2\\) is the event that the result of coin 2 is tail, etc. in Example 3.13, \\(G\\) is the event that the level of study is graduate, \\(T\\) is the event that the time from room to school is less than 15 minutes. In other texts (that you can find in books or websites), you can find that they describe the branches as the different ways to do something at the node. This also implies that the branches are just the categories/choices of the node’s variable. We will have a closer look at (random) variables and events in Chapter 4. Notes: There seem to be a lot of results derived in this lesson. However, you just need to remember the three general rules and how to draw the Venn diagram because all the other results can be retrieved from those tools or laws. Remember that practice makes perfect, try drawing the Venn diagrams or the tree diagrams by yourself, and practice the exercises given in this chapter. In general, the steps to solve a probability problem is Define (give notation for) the events Draw a tree diagram with the given information Calculate the missing/requested probability using the three laws of probability (addition, multiplication, and the total probability laws) Note that this definition of probabilistic experiment is slightly different from the definition of experiment we discussed in Chapter 1. Therefore here we say “probabilistic experiment” instead of only “experiment” to differentiate the two definitions.↩︎ "],["chap-rv.html", "4 Random Variables 4.1 Random Variables 4.2 Probability Function 4.3 Probability Function for Discrete Random Variables 4.4 Probability Function for Continuous Random Variables 4.5 Comparison of Discrete and Continuous Random Variables", " 4 Random Variables At the end of Chapter 3, we discussed that: in the tree diagram examples, each node is a categorical variable and the branches are events corresponding to categories of the variable. However, in Chapter 3, we focused more on events rather than the variables. In this lesson, we will discuss in more detail about variables in the context of probability. We will also learn the concepts of probability function and expectation. 4.1 Random Variables 4.1.1 What is A Random Variable? Random variable is a function that assigns a value to each of an experiment’s outcomes. Since the value of a random variable is directly related to the outcome of a probabilistic experiment, the value of the random variable is random, i.e., not pre-determined. Example 4.1 Probabilistic experiment: roll a six-sided die. Outcome Number appear Is odd? (\\(1\\) yes, \\(0\\) no) Is divisible by \\(3\\)? (\\(1\\) yes, \\(0\\) no) \\(1\\) \\(1\\) \\(1\\) \\(0\\) \\(2\\) \\(2\\) \\(0\\) \\(0\\) \\(3\\) \\(3\\) \\(1\\) \\(1\\) \\(4\\) \\(4\\) \\(0\\) \\(0\\) \\(5\\) \\(5\\) \\(1\\) \\(0\\) \\(6\\) \\(6\\) \\(0\\) \\(1\\) Whether the number obtained is odd, and whether the number obtained is divisible by 3 are both random variables because they give some values to each possible outcome of the probabilistic experiment. Example 4.2 Probabilistic experiment: randomly pick a person on the street and ask for their information Outcome Age Height (cm) Gender Person A 25 \\(164.4\\) Female Person B 36 \\(173.3\\) Male Person C 49 \\(188.1\\) Others Person D 53 \\(157.8\\) Female \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) The above table lists all people I could have picked on the street, i.e., all the possible outcome of the experiment. So age, height and gender are random variables because they return specific values for each possible person I could have picked. Example 4.3 My own age is a variable because its value changes year by year. But it is is not a random variable, because its value is not random nor does it relate to any probabilistic experiment. Notes: From the examples above, we see that There can be more than one random variables for one probabilistic experiment A random variable can be numeric (quantitative) or categorical (qualitative) A random variable can take finite or infinite number of possible values. The set of possible values that a random variable can take may be different from the sample space of the probabilistic experiment. 4.1.2 Observed Values In Chapter 3, we learn that a probabilistic experiment is a process that is repeatable, and every repetition of the probabilistic experiment is called a run or a trial. Now, it is important to differentiate the (unknown) general value of a random variable in the probabilistic experiment and the (known) value of the random variable in a specific run. Random variable: the unknown and general variable that represents an outcome of a probabilistic experiment often denoted by capitalized letters, for example \\(X, Y, Z\\), etc. Observed value: the known value of the random variable as a result of a specific run of the probabilistic experiment often denoted by lower-case letters, for example, \\(x, y, z\\), etc. observed data: a collection of observed values. Example 4.4 In Example 4.1, the number obtained from rolling a fair six-sided die is a random variable, say \\(X\\). \\(X\\) can take values \\(1, 2, 3, 4, 5, 6\\). Suppose today at this time I roll a die and obtain \\(3\\). Then this number \\(3\\) is an observed value of \\(X\\). Now, if I repeat rolling the die for \\(15\\) minutes and obtain the results \\[6, 1, 1, 3, 2, 5, 4, 5, 2, 6, 1, 4, 5, 6, 4, 6, 5, 6, 2, 1, 4, 4, 4, 5, 6, ...\\] then these are called the observed data of the random variable \\(X\\). 4.1.3 Defining Probabilities and Events with Random Variables Since random variables are directly related to the outcome of a probabilistic experiment, we can use random variables to define events of the probabilistic experiment. In fact, it is often more convenient to do so, compared to the way of defining an event by listing all the outcomes in that event, especially when the sample space is very large. Example 4.5 Some examples about using random variables to define events. Probabilistic experiment: rolling a six-sided die. Sample space: \\(\\Omega = \\{1,2,3,4,5,6\\}\\) \\(X\\): the number obtained. \\(X &lt; 4\\) represents the event \\(\\{1,2,3\\}\\) \\(Y\\): whether the number is divisible by three, 1 for yes and 0 for no. \\(Y = 0\\) represents the event \\(\\{1,2,4,5\\}\\) Probabilistic experiment: tossing two coins. Sample space: \\(\\Omega = \\{HH, HT, TH, TT\\}\\) \\(X\\): the outcome obtained. \\(X \\in \\{HT, TH\\}\\) represents the event \\(\\{HT, TH\\}\\). \\(Y\\): the number of heads \\(Y = 2\\) represents the event \\(\\{HH\\}\\) \\(Y = 1\\) and \\(X \\in \\{HT, TH\\}\\) both represent the event \\(\\{HT, TH\\}\\) Probabilistic experiment: picking a random person on street. sample space: all the people walking on the street that day \\(X\\): their age \\(X &gt; 18\\) represents the event that the person I picked is at least 19 years old. We cannot list all the possible people walking on the street that day who are at least 19 years old to define the event. So it is more convenient to define the event with the random variable \\(X\\). Exercise 4.1 Consider rolling a six-sided die. Let \\(Y\\) be the random variable that \\[Y = \\begin{cases} 0 &amp; \\text{if the number obtained is even} \\\\ 1 &amp; \\text{if the number obtained is odd} \\end{cases}\\] Define the event \\(Y = 0\\) by listing all the possible outcomes in the event. Define the event \\(Y = 1\\) by listing all the possible outcomes in the event. In Chapter 3, we talk about probability as a function of a set of possible outcomes, for example, probability of the sample space (\\(\\mathbb{P}(\\Omega) = 1\\)) and probability of an event \\(A\\) (\\(\\mathbb{P}(A)\\)). Now, we learn that we can use a random variable to define different events of a probabilistic experiment. Thus, we can incorporate random variables into our probability notation. Example 4.6 Consider rolling a six-sided die. Let \\(X\\) be the number obtained. The probability of obtaining a number that is less than three can be denoted as \\[\\mathbb{P}(\\{1,2\\}) = \\mathbb{P}(X \\in \\{1,2\\}) = \\mathbb{P}(X &lt; 3)\\] In Example 4.1, if the die is fair, what is \\(\\mathbb{P}(X &lt; 3)\\)? 4.1.4 Types of Random Variables There are two types of random variables: Discrete random variable: a random variable whose set of possible values are countable. Continuous random variables: a random variable whose set of possible values are uncountable. The word countable comes from pure mathematics, thus the definition of countability may seem technical and we will not deal with the detail of it in this book. But informally, a set is countable if we can count the elements together with integers, say 1, 2, 3, … Example 4.7 Examples of discrete and continuous random variables: \\(X\\): the outcome of tossing a coin. Then \\(X\\) is discrete since we can list the possible values of \\(X\\) as “1. head, 2. tail”. \\(Y\\): the number of heads when tossing 25 coins. Then \\(Y\\) is discrete since we can list the possible values of \\(Y\\) as “1. 1, 2. 2, 3. 3, …” \\(Z\\): the closing price of a stock on the New York Stock Exchange on a random day. Then, the price can be any non-negative real numbers. In this case, we cannot really count all the possible values of \\(Z\\) together with 1, 2, 3, … and hence \\(Z\\) is continuous. Notes: To decide if a random variable is continuous or discrete, use the following facts a finite set is countable. the sets (i) the set of all natural numbers \\(\\mathbb{N}\\), (ii) the set of all integers \\(\\mathbb{Z}\\), and the set of all rational numbers \\(\\mathbb{Q}\\), are countable. \\(\\mathbb{R}\\), the set of all real numbers, and any real intervals are uncountable. Example 4.8 Revisit Example 4.5: \\(X\\) is discrete because its sample space is finite. \\(Y\\) is discrete because its sample space is \\(\\mathbb{N}\\). \\(Z\\) is continuous because its sample space is \\([0, +\\infty)\\), which is a real interval. Notes: It is important to differentiate the set of all possible values for a random variable and the sample space of a probabilistic experiment. Example 4.9 Consider Example 4.1. the sample space is \\(\\{1,2,3,4,5,6\\}\\) the set of all possible values for the first random variable is \\(\\{1,2,3,4,5,6\\}\\) the set of all possible values for both the second and third random variables is \\(\\{0,1\\}\\). Example 4.10 Consider Example 4.2. the sample space is all the people I could have picked on the street the set of all possible values for Age is \\(\\{0, 1, 2, 3, ...\\}\\). the set of all possible values for Height is \\((0, +\\infty)\\). the set of all possible values for Gender is \\(\\{female, male, others\\}\\)9 Notes: In many cases where the random variable is in fact discrete, people tend to treat the variable as if they are continuous, especially when the possible values of the variable are numbers and there are many of them. There are many technical reasons, but basically it is because it is more convenient and simple to do so. An example where discrete variables are often treated as continuous is in linear regression. We will learn about it in Chapter 10 and 11. Example 4.11 Students’ rent is a continuous random variable because it can take any real values, for example \\(\\$ 861.236\\). Exercise 4.2 Is human’s height discrete or continuous, and why? 4.2 Probability Function 4.2.1 Probability Distribution A probability distribution describes how the probabilities are like for all possible values of a random variable. Histograms (from Chapter 2) is a good way to visualize probability distributions. Example 4.12 Let us consider again the students’ rent data in Example @ref{exm:ex-des-hist} of Chapter 2. Looking at the histogram, we can see that the prices range from about 300 to 1200 the prices around 700-800 are the most probable. In fact, the probability that the rent falls between 700 to 800 is more than 0.25 the distribution of the rent price looks symmetric etc. Figure 4.1: Histogram of students’ rent Notes: Since we want to represent a probability distribution, the heights of the bars in the histogram above are standardized into relative frequencies instead of the frequencies (counts). According to the empirical view of probability we learned in Chapter 3, relative frequency can be thought of as a probability for students’ rent in the sample data. Exercise 4.3 Verify that relative frequency satisfy the mathematical definition of a probability in Chapter 3. 4.2.2 Probability Function Probability functions are mathematical functions that take in a possible value of a random variable, and output the probability or the probability density10 at that specific value. Therefore, they decide how probability distributions look like. Example 4.13 Toss a fair coin. \\(X\\): the face obtained. The probability function is \\[p(X) = \\begin{cases} \\frac{1}{2} &amp; \\text{if } X \\text{ is head}, \\\\ \\frac{1}{2} &amp; \\text{if } X \\text{ is tail}, \\\\ \\end{cases}\\] Exercise 4.4 Consider rolling a fair six-sided die. What is the probability function of \\(X\\): the number obtained \\[Y = \\begin{cases} 1 &amp; \\text{if the number obtained is less than 2} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Notes: Difference between probability distributions and probability functions Probability distributions: conceptual understanding of how the probabilities are distributed for a random variable focus on the location (which values are possible), the spread (how the values are different from each other), the shape (how the probabilities are distributed among the possible values), etc. Probability functions: mathematical functions focus on the mathematical relationships/rules between a possible value of a random variable and the probability or the probability density at that value decide what the probability distributions look like. 4.2.3 Discrete and Continuous Probability Functions It is essential to note that probability functions are handled differently for discrete and continuous random variables. The reason is that is is impossible to quantify the probability that a continuous random variable obtains a specific value. More clearly, for example, consider the closing price of a stock in Example 4.7. This is a continuous random variable. The probability that a stock’s closing price is at 4.9375016404165072 is 0, i.e., we almost never can obtain this exact price. suppose on the contrary that we can assign a number for the probability of obtaining any certain value \\(x\\) of \\(X\\) to be \\(k_x = \\mathbb{P}(X = x) &gt; 0\\). Since there are uncountably infinitely many possible values \\(x\\), the sum of all these probabilities \\(k_x\\) will be infinite. This is contradictory to the requirement that \\(\\mathbb{P}(\\Omega) = 1\\).11 when talking about probabilities related to a continuous random variable, we usually only talk about the probabilities of intervals of possible values, for example, \\(\\mathbb{P}(X \\le 5) = \\mathbb{P}((-\\infty, 5])\\), or \\(\\mathbb{P}(1 \\le X &lt; 3) = \\mathbb{P}([1,3))\\) 12 and we let \\(P(X = x) = 0\\) for all possible values \\(x\\). This is the reason why in the definition of probability functions in Section 4.2.2, I say that a probability function takes in a possible value of the random variable and outputs the probability (for discrete random variables) or the probability density (for continuous random variables) at that specific value. We will explain more clearly what the probability density for a continuous variable is in Section 4.4. But first, let us start by considering the probability function for discrete random variables. 4.3 Probability Function for Discrete Random Variables 4.3.1 Probability Mass Function The probability mass function (or pmf) takes in a possible value of a discrete random variable and outputs the probability of obtaining that specific value. \\[p(x) = \\mathbb{P}(X = x)\\] Here, \\(p\\) denotes the probability mass function, \\(\\mathbb{P}\\) denotes probability in general13, \\(X\\) denotes the discrete random variable, and \\(x\\) denotes a specific value of \\(X\\). Using the mathematical definition of probability in Section 3.1.4, we have the properties of a probability mass function \\(p\\) \\(0 \\le p(x) \\le 1\\) \\(\\sum_{\\text{all }x}p(x) = \\mathbb{P}(\\Omega) = 1\\). Example 4.14 A discrete random variable \\(X\\) can take five possible values: \\(2, 3, 5, 8\\) and \\(10\\). Its probability mass function is given in the following table \\(x\\) \\(2\\) \\(3\\) \\(5\\) \\(8\\) \\(10\\) \\(p(x) = \\mathbb{P}(X = x)\\) \\(0.15\\) \\(0.10\\) ? \\(0.25\\) \\(0.25\\) Solve for \\(\\mathbb{P}(X = 5)\\). What is the probability \\(X\\) equals \\(2\\) or \\(10\\)? What is \\(\\mathbb{P}(X \\le 8)\\)? What is \\(\\mathbb{P}(X &lt; 8)\\)? Solution: Since probability of the whole sample space is 1, and the probability of union of mutually exclusive events is the sum of probability of each event \\[\\begin{align*} \\mathbb{P}(\\Omega) = 1 &amp; = \\mathbb{P}(X\\in\\{2,3,5,8,10\\}) \\\\ &amp; = \\mathbb{P}(X = 2) + \\mathbb{P}(X = 3) + \\mathbb{P}(X = 5) + \\mathbb{P}(X = 8) + \\mathbb{P}(X = 10) \\\\ &amp; = 0.15 + 0.10 + \\mathbb{P}(X = 5) + 0.25 + 0.25 \\\\ \\Rightarrow\\mathbb{P}(X = 5) &amp; = 1 - (0.15+0.10+0.25+0.25) = 0.25 \\end{align*}\\] \\(\\mathbb{P}(X \\in \\{2,10\\}) = 0.15+0.25 = 0.4\\). \\(\\mathbb{P}(X \\le 8) = \\mathbb{P}(X = 2) + \\mathbb{P}(X = 3) + \\mathbb{P}(X = 5) + \\mathbb{P}(X = 8)\\) \\(= 0.15 + 0.10 + 0.25 + 0.25 = 0.75\\). \\(\\mathbb{P}(X &lt; 8) = \\mathbb{P}(X = 2) + \\mathbb{P}(X = 3) + \\mathbb{P}(X = 5) = 0.15 + 0.10 + 0.25 = 0.5\\) 14. Exercise 4.5 Consider the following probability mass function of a random variable \\(X\\) \\(x\\) \\(10\\) \\(20\\) \\(30\\) \\(40\\) \\(p(x) = \\mathbb{P}(X = x)\\) \\(0.2\\) \\(0.2\\) \\(0.5\\) \\(0.1\\) Is this a valid probability function? Justify your response. What is the most likely value (mode) of \\(X\\)? What is the conditional probability that \\(X\\) is less than 25, given \\(X\\) is less than 35? 4.3.2 Cumulative Distribution Function The cumulative distribution function (or cdf) is defined as \\[F(x) = \\mathbb{P}(X \\le x)\\] which is the probability that the random variable \\(X\\) is less than or equal to \\(x\\). When \\(X\\) is discrete, the event \\(X \\le x\\) correspond to mutually exclusive events \\(X = z\\) where \\(z \\le x\\). So we can sum over the events and obtain \\[F(x) = \\sum_{z \\le x} p(z)\\] Example 4.15 In Example 4.14, we have \\(x\\) \\(2\\) \\(3\\) \\(5\\) \\(8\\) \\(10\\) \\(p(x) = \\mathbb{P}(X = x)\\) \\(0.15\\) \\(0.10\\) \\(0.25\\) \\(0.25\\) \\(0.25\\) \\(F(x) = \\mathbb{P}(X \\le x)\\) \\(0.15\\) \\(0.25\\) \\(0.5\\) \\(0.75\\) \\(1.00\\) Notes: For any value other than the possible values \\(2,3,5,8,10\\), \\(\\mathbb{P}(X = x) = 0\\). For example, \\(\\mathbb{P}(X = 2.5) = 0\\) For any value \\(2 &lt; z \\le 3\\), \\(\\mathbb{P}(X \\le z) = 0.15\\). For example, \\(F(2.3965) = \\mathbb{P}(X \\le 2.3965) = 0.15\\). Similarly to other numbers lying between the discrete possible values of \\(X\\). Exercise 4.6 Calculate \\(F(x)\\) for possible discrete values of \\(X\\) given in Exercise 4.5. Properties of the cumulative distribution function: \\(F(x)\\) is non-decreasing function of \\(x\\), that is, if \\(x_1 &lt; x_2\\), then \\(F(x_1) \\le F(x_2)\\). this is because \\(F(x)\\) is cumulative (adding up non-negative probabilities as \\(x\\) increases) \\(0 \\le F(x) \\le 1\\) for all \\(x\\). this is because \\(F(x)\\) is a probability \\(\\lim_{x\\to\\infty}F(x) = 0\\) and \\(\\lim_{x\\to\\infty}F(x) = 1\\), that is, \\(F(x)\\) starts at 0 and ends at 1. this is equivalent to \\(\\mathbb{P}(\\emptyset) = 0\\) and \\(\\mathbb{P}(\\Omega) = 1\\). Exercise 4.7 Continue with Example 4.15, See that the general properties of cumulative distribution function hold for \\(F(x)\\) in Example 4.15. What is \\(F(-0.2463)\\)? What is \\(F(100)\\)? 4.3.3 Expectation 4.3.3.1 Mean Recall in Chapter 2 we talked about the (sample) mean as being the average of all the observed values in the sample. The (sample) mean tells us about the centrality of the distribution of the sample data. Now, let us generalize this idea and discuss a probabilistic definition of the mean as a centrality measure of a probability distribution. Suppose I have \\(n\\) observations for a random variable \\(X\\): \\(x_1, x_2, ..., x_n\\). Then from Chapter2, the sample mean is \\[\\begin{align*} \\bar{x} &amp; = \\frac{1}{n}(x_1 + x_2 + ... + x_n) \\\\ &amp; = \\sum_{\\text{different possible values }z}\\frac{z \\times \\text{number of observed values that have the value }z}{n} \\\\ &amp; = \\sum_{\\text{different possible values }z} \\Big[z \\times \\text{relative frequency of }z \\text{ in the observed data} \\Big] \\end{align*}\\] Example 4.16 Consider the observed data \\(2, 1, 1, 2, 2, 3, 4, 2, 4, 4, 3, 3, 2, 4, 4, 1, 4, 1, 2, 3\\). The sample mean is \\[\\bar{x} = \\frac{2+1+1+...+2+3}{20} = 1\\times\\frac{4}{20} + 2\\times\\frac{6}{20} + 3\\times\\frac{4}{20} + 4\\times\\frac{6}{20} = 2.6.\\] In Chapter 3, we learn that according to empirical view of probability, if the probabilistic experiment is repeated infinitely many times, so the relative frequency of an outcome is the probability of such outcome. Therefore, in the above formula of the sample mean, by replacing the relative frequency with the probability notation, we have the probabilistic definition of the mean (or expected value) of a discrete random variable \\(X\\): \\[\\begin{align*} \\mu_X = \\mathbb{E}[X] &amp; = \\sum_{\\text{all values }x \\text{ of } X}x\\times \\mathbb{P}(X = x) \\\\ &amp; = \\sum_{\\text{all values }x \\text{ of } X}x p(x) \\end{align*}\\] In this formulation, \\(\\mu_X\\) denotes the mean of \\(X\\) and \\(\\mathbb{E}\\) denotes the expectation operator. So, the mean of \\(X\\) is a weighted average of all possible values of \\(X\\) where the weights are the probabilities that \\(X\\) obtains each value \\(x\\). The mean is also called the long-run average due to the empirical probability argument above. 4.3.3.2 Variance Using the same argument above for the mean, from the formula of the sample variance, i.e., \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar{x})^2\\] we come to the probabilistic definition of the variance of a discrete random variable15. \\[\\begin{align*} \\sigma^2_X = \\text{var}(X) = \\mathbb{E}[(X - \\mu_X)^2] &amp; = \\sum_{\\text{all values }x \\text{ of } X} (x - \\mu_X)^2 \\times \\mathbb{P}(X=x) \\\\ &amp; = \\sum_{\\text{all values }x \\text{ of } X} (x - \\mu_X)^2p(x) \\end{align*}\\] The standard deviation is \\[\\sigma_X = \\text{sd}(X) = \\sqrt{\\sigma^2_X}\\] Example 4.17 Consider the probability mass function in Example 4.14 \\(x\\) \\(2\\) \\(3\\) \\(5\\) \\(8\\) \\(10\\) \\(p(x) = \\mathbb{P}(X = x)\\) \\(0.15\\) \\(0.10\\) \\(0.25\\) \\(0.25\\) \\(0.25\\) The expected value (mean) of \\(X\\) is \\[\\mathbb{E}(X) = (2\\times 0.15)+(3\\times 0.10)+(5\\times 0.25)+(8 \\times 0.25)+(10 \\times 0.25) = 6.35\\] The variance of \\(X\\) is \\[\\begin{align*} \\mathrm{var}(X) &amp; = (2 - 6.35)^2\\times 0.15 + (3 - 6.35)^2\\times 0.10 + (5 - 6.35)^2 \\times 0.25 \\\\ &amp; \\qquad + (8 - 6.35)^2 \\times 0.25 + (10 - 6.35)^2 \\times 0.25 \\\\ &amp; = 8.4275 \\end{align*}\\] Exercise 4.8 Calculate the expected value and variance of \\(X\\) in Exercise 4.5. 4.3.3.3 Expectation of a Function In general, the expected value of a function \\(g(X)\\) for a discrete random variable \\(X\\) is defined as \\[\\begin{align*} \\mathbb{E}[g(X)] &amp; = \\sum_{\\text{all values }x \\text{ of } X} g(x) \\times \\mathbb{P}(X = x) \\\\ &amp; = \\sum_{\\text{all values }x \\text{ of } X} g(x) p(x) \\end{align*}\\] 4.3.3.4 Properties of Expectation For constants \\(a\\) and \\(b\\) and for all random variables \\(X\\) \\[\\begin{align*} \\mathbb{E}(a + bX) &amp; = a + b\\mathbb{E}(X) \\\\ \\mathrm{var}(a + bX) &amp; = b^2\\mathrm{var}(X) \\end{align*}\\] Exercise 4.9 Use the definition of variance and the first property of expectation to prove that \\[\\mathrm{var}(X) = \\mathbb{E}(X^2) - [\\mathbb{E}(X)]^2 \\] The result in Exercise 4.9 is very handy for calculating the variance of a random variable. Example 4.18 In Example 4.14, we can alternatively calculate the variance of \\(X\\) by first calculate \\[\\mathbb{E}(X^2) = (2^2\\times 0.15)+(3^2\\times 0.10)+(5^2\\times 0.25)+(8^2 \\times 0.25)+(10^2 \\times 0.25) = 48.75\\] and then \\[\\mathrm{var}(X) = \\mathbb{E}(X^2) - [\\mathbb{E}(X)]^2 = 48.75 - (6.35^2) = 8.4275.\\] 4.3.4 Bernoulli Distribution Suppose we conduct an experiment a single times, i.e., have 1 trial. In this trial we have two mutually exclusive possible outcomes, labeled success vs. failure, where \\[\\mathbb{P}(\\text{success}) = p\\] and hence \\[\\mathbb{P}(\\text{failure}) = 1-p\\] We say that the random variable representing the outcome of the trial \\(X\\) follows a Bernoulli distribution with probability \\(p\\). Denote \\[X \\sim \\text{Bernoulli}(p)\\hspace{5mm} \\text{where} \\hspace{5mm}X = \\begin{cases} 1 &amp; \\text{if the outcome is a success} \\\\ 0 &amp; \\text{if the outcome is a failure} \\end{cases}\\] The probability mass function of \\(X\\) is \\[P(X = x) = p^x(1-p)^{1-x} \\hspace{5mm} \\text{for } x = 0,1.\\] We can plot this function: Figure 4.2: Bernoulli distribution Example 4.19 Toss a (not necessarily fair) coin, let \\[X = \\begin{cases} 1 &amp; \\text{if head is obtained} \\\\ 0 &amp; \\text{if tail is obtained} \\end{cases}\\] Then \\(X\\) is a Bernoulli random variable. Example 4.20 Pick a random person on a street. Let \\[X = \\begin{cases} 1 &amp; \\text{if the person is female} \\\\ 0 &amp; \\text{if the person is non-female} \\end{cases}\\] Then \\(X\\) is a Bernoulli random variable. The expected value of \\(X\\) is \\[\\mathbb{E}(X) = \\sum_{x=0}^1 xp(x) = 0 \\times (1-p) + 1 \\times p = p\\] Exercise 4.10 Prove that if \\(X \\sim \\text{Bernoulli}(p)\\), then \\[\\mathrm{var}(X) = p(1-p)\\] Notes: In this book, I only list the Bernoulli distribution as an example of discrete probability distributions. There are other popular distributions such as Binomial distribution, Geometric distribution, Poisson distribution, etc. The pmf and cdf of these distributions also possess the general properties of pmf and cdf presented in this Section. 4.4 Probability Function for Continuous Random Variables 4.4.1 Corresponding Definitions for Continuous Random Variables In Section 4.2.3, we have discussed that for a continuous random variable \\(X\\), \\(P(X = x) = 0\\) for all possible values \\(x\\). Hence, we need to define the probability distribution using the probability density function (or pdf) \\(f(x)\\). We also need to replace the sum in the discrete case by the integral for the continuous case. Specifically, for a continuous random variable \\(X\\) The probability density function \\(f(x)\\) represents the height of the density curve at the point \\(x\\). \\(f(x) \\ge 0\\) for all \\(x\\). The probability of an interval \\((a,b)\\) is calculated as the area under the \\(f(x)\\) curve from \\(x = a\\) to \\(x = b\\), which is the integral \\[\\mathbb{P}((a,b)) = \\mathbb{P}(X \\in (a,b)) = \\mathbb{P}(a &lt; X &lt; b) = \\int_{a}^bf(x)dx\\] Because \\(\\mathbb{P}(X = x) = 0\\), we have \\[\\mathbb{P}(a &lt; X &lt; b) = \\mathbb{P}(a \\le X &lt; b) = \\mathbb{P}(a &lt; X \\le b) = \\mathbb{P}(a \\le X \\le b)\\] Figure 4.3: Area under the curve The cumulative distribution function (cdf) of \\(X\\) is \\[F(x) = \\mathbb{P}(X \\le x) = \\mathbb{P}(X\\in(-\\infty, x]) = \\int_{-\\infty}^x f(z)dz\\] The total area under the curve is \\(1\\), i.e., \\[\\mathbb{P}(\\Omega) = F(+\\infty) = \\int_{-\\infty}^{+\\infty}f(x)dx = 1.\\] The expectation of \\(X\\) is \\[\\mu_X = \\mathbb{E}(X) = \\int_{-\\infty}^{+\\infty}xf(x)dx\\] The variance of \\(X\\) is \\[\\sigma^2_X = \\mathrm{var}(X) = \\mathbb{E}[(X-\\mu_X)^2] = \\int_{-\\infty}^{+\\infty}xf(x)dx\\] The expected value of \\(g(X)\\) for a function \\(g\\) is \\[\\mathbb{E}[g(X)] = \\int_{-\\infty}^{+\\infty} g(x)f(x)dx\\] It is best to explain these new concepts by an example of a continuous probability distribution. 4.4.2 Continuous Uniform Distribution For constants \\(c,d\\), if \\(X\\) is a random variable taking on values in the interval \\([c,d]\\) where \\(f(x)\\) is the same for all \\(x \\in [c,d]\\), then we say that \\(X\\) follows a Continuous uniform distribution on interval \\([c,d]\\). Denote \\[X \\sim \\text{Unif}(c,d)\\] The probability distribution: suppose \\(f(x) = k\\) for all \\(c \\le x \\le d\\) and some constant \\(k\\). Because \\(\\mathbb{P}(\\Omega) = 1\\), \\(f(x)\\) has to satisfy \\[\\mathbb{P}(\\Omega) = \\int_{-\\infty}^{+\\infty}f(x)dx = \\int_c^d k dx = kx\\Big|_{c}^d = 1 \\hspace{5mm}\\Rightarrow \\hspace{5mm} k = \\frac{1}{d-c}\\] Therefore, the probability density function of \\(X \\sim \\text{Unif}(c,d)\\) is \\[f(x) = \\begin{cases} \\frac{1}{d-c} &amp; \\text{for } c \\le x \\le d \\\\ 0 &amp; \\text{otherwise}\\end{cases}\\] and it can be plotted as follows. Figure 4.4: Continuous uniform distribution The cumulative distribution of \\(X\\) is \\[F(x) = \\int_{-\\infty}^{x}f(z)dz = \\int_{c}^x\\frac{1}{d-c}dx = \\begin{cases} 0 &amp; \\text{for } x &lt; c \\\\ \\frac{x-c}{d-c} &amp; \\text{for } c \\le x &lt; d \\\\ 1 &amp; \\text{for } x \\ge d \\end{cases} \\] Exercise 4.11 Prove that the expected value and variance of \\(X \\sim \\text{Unif}(c,d)\\) are \\[\\mathbb{E}(X) = \\frac{d+c}{2} \\hspace{5mm} \\text{and} \\hspace{5mm} \\mathrm{var}(X) = \\frac{(d-c)^2}{12}\\] Example 4.21 Suppose the area of a room for rent for a student is a uniform random variable with values between \\(5\\) to \\(15\\) \\(\\mathrm{m}^2\\). Let \\(X\\): the the area of a student room. Graph the probability distribution of \\(X\\). On average, how big is a student room? And what is the standard deviation? What is the probability that a room is less than 11.5 \\(\\mathrm{m}^2\\)? Eighty percent of the time, the area of the student room falls below what value (i.e., what is the 0.8 quantile)? Solution: See the graph below. \\(\\mathbb{E}(X) = \\frac{15+5}{2} = 10 (\\mathrm{m}^2)\\) \\(\\mathrm{var}(X) = \\frac{(15-5)^2}{12} = 8.333\\) \\(\\mathrm{sd}(X) = \\sqrt{8.333} = 2.887\\). \\(\\mathbb{P}(X \\le 11.5) = \\frac{11.5-5}{15-5} = 0.65\\) \\(\\mathbb{P}(X \\le k) = \\frac{k-5}{15-5} = 0.8\\), so \\(k = 13\\). Exercise 4.12 The amount of time in minutes, that a person must wait for a bus is uniformly distributed between zero and \\(20\\) minutes. Graph the probability density function. You may use R. Find \\(\\mathbb{P}(X &gt; 15)\\). Find the \\(75\\)th percentile. Notes: Again, there are other examples of continuous probability distributions, for example the normal distribution, \\(F\\) distribution, \\(t\\) distribution, etc. Exceptionally, the normal distribution is a very important probability distribution in statistics. We will go over it in Chapter 5. 4.5 Comparison of Discrete and Continuous Random Variables Discrete RVs Continuous RVs Set of possible values countable uncountable Probability function Probability mass function (pmf): \\(p(x) = \\mathbb{P}(X = x)\\) Probability density function (pdf): \\(f(x)\\) Probability of events Sum of values in the event: \\(\\sum_{x \\text{ of event}}p(x)\\) Integral over the interval of the event: \\(\\mathbb{P}(a &lt; X &lt; b) = \\int_{a}^bf(x)dx\\) Mean \\(\\mathbb{E}(X) = \\sum_{\\text{all }x}xp(x)\\) \\(\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty}xf(x)dx\\) Expectation of a function \\(\\mathbb{E}(g(X)) = \\sum_{\\text{all }x}g(x)p(x)\\) \\(\\mathbb{E}(g(X)) = \\int_{-\\infty}^{\\infty}g(x)f(x)dx\\) Examples Bernoulli, Binomial, Geometric, Poisson distributions Continuous uniform, Normal, \\(F\\)-, \\(t\\)-distributions Notes: First, note that the concepts of cumulative distribution function and expectation only apply to random variables that output numeric values. For example, consider tossing a coin and two random variables \\[X = \\begin{cases} \\text{&quot;yes&quot;} &amp; \\text{if a head is obtained} \\\\ \\text{&quot;no&quot;} &amp; \\text{otherwise} \\end{cases} \\hspace{5mm} \\text{and} \\hspace{5mm} Y = \\begin{cases} 1 &amp; \\text{if a head is obtained} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Then concepts of cumulative distribution function and expectations only apply to \\(Y\\). Second, in this chapter, we learn about important concepts in the subject of probability: random variables, probability functions and expectations, etc. Some of them can be challenging to understand correctly, even for me. Therefore, I went at length to explain those concepts clearly with many examples. To sharpen your understanding, practice the examples, exercises and practice questions in this chapter by yourself. You can also choose to choose other variables with different set of categories to represent “gender”. Here, we are considering the specific variable we defined in Example 4.2.↩︎ We will discuss about what probability density is in Section 4.4 below.↩︎ All possible values of \\(X\\) represent mutually exclusive events that make up the sample space \\(\\Omega\\). Therefore, we can sum the probabilities of these events to get \\(\\mathbb{P}(\\Omega)\\).↩︎ Here, round bracket means we exclude the number, square bracket means we include the number. For example, \\([1,3)\\) means all the numbers from 1 to 3, including 1 but excluding 3.↩︎ The one we discussed in Chapter 3↩︎ Alternative solution: \\(\\mathbb{P}(X &lt; 8) = \\mathbb{P}(X \\le 8) - \\mathbb{P}(X = 8) = 0.75-0.25=0.5\\).↩︎ Here, let us ignore that we use \\(n-1\\) instead of \\(n\\) for the formula of \\(s^2\\) because \\(n\\) and \\(n-1\\) behave very similarly when \\(n\\) is very large.↩︎ "],["chap-normal.html", "5 Normal Distribution 5.1 Normal Distribution 5.2 Standard Normal Distribution 5.3 \\(Z\\)-score 5.4 Calculate Normal Probabilities in R", " 5 Normal Distribution In the Chapter 4, we learn about random variables and their probability functions. We discuss the Bernoulli distribution and Continuous uniform distribution as examples of discrete and continuous distributions. In this lesson, we discuss normal distribution, a continuous distribution that plays the most important role in statistics. 5.1 Normal Distribution 5.1.1 Probability Density Function A random variable \\(X\\) is said to follow a normal distribution (or Gaussian distribution) if it has the probability density function of the form \\[f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}, \\hspace{10mm} \\text{for } x \\in (-\\infty, \\infty)\\] where \\(-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma &gt; 0\\) are two parameters of this distribution. We denote \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), meaning that \\(X\\) follows a normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\). 5.1.2 Expected Value and Variance If \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then The mean (expected value) of \\(X\\) is \\(\\mu = \\mathbb{E}(X)\\) The variance of \\(X\\) is \\(\\sigma^2 = \\mathrm{var}(X)\\) So it is quite straight forward to tell the mean and variance of the \\(\\mathcal{N}(\\mu, \\sigma^2)\\) distribution from the notation. In fact, \\(\\mu\\) and \\(\\sigma^2\\) are called parameters of the normal distribution. 5.1.3 Shape of the Distribution Figure 5.1: A normal distribution The normal distribution has a bell shape is always symmetric about its means (creates mirror images around the mean) the mean is called the location parameter, meaning that it controls the location of the distribution on the \\(x\\)-axis the variance is called the scale parameter, meaning that it controls the spread of the distribution. 5.1.4 Effect of the mean \\((\\mu)\\) The mean shifts the distribution along the \\(x\\)-axis. Therefore it is a called the location parameter. Figure 5.2: Changing \\(\\mu\\) changes the location of the distribution 5.1.5 Effect of the Variance (\\(\\sigma^2\\)) The variance either stretches out or pulls in the distribution. Therefore it is called the scale parameter. Figure 5.3: Changing \\(\\sigma^2\\) changes the spread of the distribution A few more examples Figure 5.4: Examples of normal distribution 5.1.6 The Empirical Rule of Normal Distribution For any normal distribution, 68% of the data will be contained within one standard deviation of the mean. 95% of the data will be contained within two standard deviations of the mean. 99.7% of the data will be contained within three standard deviations of the mean. Figure 5.5: Empirical rule of standard normal distribution 5.1.7 Ubiquity of Normal Distribution A reason why the normal distribution is so important is that the distributions of many variables in real life has bell-curve shapes which resemble the shape of normal distributions. For example, the distribution of human’s height is roughly normally distributed. Think about it, majority of, say male, are around may be 170cm - 180 cm (about 66 to 70 inches) with less people from 160cm - 170 cm or 180 - 190 cm, and many fewer shorter than 160 cm or taller than 190 cm. This dictates a bell-curve shape for the distribution of height. The figure below shows the distribution of self-reported heights for 812 males given in the data set heights from package dslabs in R. Figure 5.6: Distribution of self-reported heights for 812 males. Another example is the one-day return of a stock. The figure below shows the distribution of Google stock’s daily return from 2005-02-07 to 2005-07-07 given in the data set google from package UsingR in R. We can see that this distribution also roughly has a bell shape curve. Figure 5.7: Distribution of Google stock’s daily return from 2005-02-07 to 2005-07-07. 5.2 Standard Normal Distribution 5.2.1 Probability Density Function A normal distribution with mean of 0 (\\(\\mu=0\\)) and variance of 1 (\\(\\sigma^2=1\\)) is called a standard normal distribution. Notation: \\(\\mathcal{N}(0,1)\\). A standard normal random variable is usually denoted by \\(Z\\). We write \\(Z\\sim \\mathcal{N}(0,1)\\) and we say \\(Z\\) follows a standard normal distribution. Figure 5.8: Standard normal distribution The pdf (probability density function) of the standard normal distribution is \\[f(z) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}\\] From Chapter 4, we know that for any continuous random variable, to calculate the probability that the random variable lies within an interval \\((a,b)\\), we need to integrate the pdf from \\(a\\) to \\(b\\). Unfortunately, except for the Continuous uniform distribution we learn in 4, it is hard to calculate the integral for any continuous distributions by hand. Hence, usually we use a computer to calculate the probability, or we need to refer to a probability table. 5.2.2 \\(Z\\)-table For standard normal distribution, we have the \\(Z\\)-table. This table provides us with the cumulative distribution, \\(F(z) = \\mathbb{P}(Z \\le z)\\), i.e., the area to the left of the observed value of \\(z\\). 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.00 0.5000 0.4960 0.4920 0.4880 0.4840 0.4801 0.4761 0.4721 0.4681 0.4641 -0.10 0.4602 0.4562 0.4522 0.4483 0.4443 0.4404 0.4364 0.4325 0.4286 0.4247 -0.20 0.4207 0.4168 0.4129 0.4090 0.4052 0.4013 0.3974 0.3936 0.3897 0.3859 -0.30 0.3821 0.3783 0.3745 0.3707 0.3669 0.3632 0.3594 0.3557 0.3520 0.3483 -0.40 0.3446 0.3409 0.3372 0.3336 0.3300 0.3264 0.3228 0.3192 0.3156 0.3121 -0.50 0.3085 0.3050 0.3015 0.2981 0.2946 0.2912 0.2877 0.2843 0.2810 0.2776 -0.60 0.2743 0.2709 0.2676 0.2643 0.2611 0.2578 0.2546 0.2514 0.2483 0.2451 -0.70 0.2420 0.2389 0.2358 0.2327 0.2296 0.2266 0.2236 0.2206 0.2177 0.2148 -0.80 0.2119 0.2090 0.2061 0.2033 0.2005 0.1977 0.1949 0.1922 0.1894 0.1867 -0.90 0.1841 0.1814 0.1788 0.1762 0.1736 0.1711 0.1685 0.1660 0.1635 0.1611 -1.00 0.1587 0.1562 0.1539 0.1515 0.1492 0.1469 0.1446 0.1423 0.1401 0.1379 -1.10 0.1357 0.1335 0.1314 0.1292 0.1271 0.1251 0.1230 0.1210 0.1190 0.1170 -1.20 0.1151 0.1131 0.1112 0.1093 0.1075 0.1056 0.1038 0.1020 0.1003 0.0985 -1.30 0.0968 0.0951 0.0934 0.0918 0.0901 0.0885 0.0869 0.0853 0.0838 0.0823 -1.40 0.0808 0.0793 0.0778 0.0764 0.0749 0.0735 0.0721 0.0708 0.0694 0.0681 -1.50 0.0668 0.0655 0.0643 0.0630 0.0618 0.0606 0.0594 0.0582 0.0571 0.0559 -1.60 0.0548 0.0537 0.0526 0.0516 0.0505 0.0495 0.0485 0.0475 0.0465 0.0455 -1.70 0.0446 0.0436 0.0427 0.0418 0.0409 0.0401 0.0392 0.0384 0.0375 0.0367 -1.80 0.0359 0.0351 0.0344 0.0336 0.0329 0.0322 0.0314 0.0307 0.0301 0.0294 -1.90 0.0287 0.0281 0.0274 0.0268 0.0262 0.0256 0.0250 0.0244 0.0239 0.0233 -2.00 0.0228 0.0222 0.0217 0.0212 0.0207 0.0202 0.0197 0.0192 0.0188 0.0183 -2.10 0.0179 0.0174 0.0170 0.0166 0.0162 0.0158 0.0154 0.0150 0.0146 0.0143 -2.20 0.0139 0.0136 0.0132 0.0129 0.0125 0.0122 0.0119 0.0116 0.0113 0.0110 -2.30 0.0107 0.0104 0.0102 0.0099 0.0096 0.0094 0.0091 0.0089 0.0087 0.0084 -2.40 0.0082 0.0080 0.0078 0.0075 0.0073 0.0071 0.0069 0.0068 0.0066 0.0064 -2.50 0.0062 0.0060 0.0059 0.0057 0.0055 0.0054 0.0052 0.0051 0.0049 0.0048 -2.60 0.0047 0.0045 0.0044 0.0043 0.0041 0.0040 0.0039 0.0038 0.0037 0.0036 -2.70 0.0035 0.0034 0.0033 0.0032 0.0031 0.0030 0.0029 0.0028 0.0027 0.0026 -2.80 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021 0.0020 0.0019 -2.90 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015 0.0014 0.0014 -3.00 0.0013 0.0013 0.0013 0.0012 0.0012 0.0011 0.0011 0.0011 0.0010 0.0010 -3.10 0.0010 0.0009 0.0009 0.0009 0.0008 0.0008 0.0008 0.0008 0.0007 0.0007 -3.20 0.0007 0.0007 0.0006 0.0006 0.0006 0.0006 0.0006 0.0005 0.0005 0.0005 -3.30 0.0005 0.0005 0.0005 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0003 -3.40 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0002 Using the fact that standard normal distribution is symmetric round 0 and the total area under the curve is 1, we can use the numbers from the table to solve for the probability of any interval that we may want. Example 5.1 Find that probability that \\(Z &lt; -1.37\\) \\(Z &gt; 1.5\\) \\(-1 &lt; Z &lt; 1.15\\) \\(|Z| &gt; 0.5\\) Find the \\(z\\) value that corresponds to the highest 20% Find the \\(z\\) value that corresponds to the lowest 3% (i.e., \\(3\\)rd percentile) Find the \\(z\\) value that corresponds to the 95% percentile Solution: To find \\(\\mathbb{P}(Z &lt; -1.37)\\), we first look at the leftmost column, and find the row -1.30. Then we follow this row and look at column 0.07. The element in this cell is 0.0853. So \\[\\mathbb{P}(Z &lt; -1.37) = \\mathbb{P}(Z \\le -1.37) = 0.0853\\] Because the standard normal distribution is symmetric around 0, we have \\[\\mathbb{P}(Z &gt; 1.5) = \\mathbb{P}(Z &lt; -1.5) = 0.0668\\]. \\[\\begin{align*} \\mathbb{P}(-1 &lt; Z &lt; 1.15) &amp; = \\mathbb{P}(Z &lt; 1.15) - \\mathbb{P}(Z \\le -1) \\\\ &amp; = \\mathbb{P}(Z &gt; -1.15) - \\mathbb{P}(Z &lt; -1) \\\\ &amp; = (1 - \\mathbb{P}(Z &lt; -1.15)) - \\mathbb{P}(Z &lt; -1) \\\\ &amp; = (1 - 0.1251) - 0.1587 = 0.7162 \\end{align*}\\] \\[\\begin{align*} \\mathbb{P}(|Z| &gt; 0.5) &amp; = \\mathbb{P}(Z &gt; 0.5) + \\mathbb{P}(Z &lt; -0.5) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.5) + \\mathbb{P}(Z &lt; -0.5) \\\\ &amp; = 2\\mathbb{P}(Z &lt; -0.5) \\\\ &amp; = 2\\times 0.3085 = 0.6170. \\end{align*}\\] \\(\\mathbb{P}(Z &gt; z) = 0.2\\) so \\(\\mathbb{P}(Z &lt; -z) = 0.2\\). Now we look at the \\(Z\\)-table and find the cell that has value \\(0.2\\). The cell \\(-0.84\\) has value \\(0.2005\\), which is closest to \\(0.2\\). So \\(-z = -0.84\\) and \\(z = 0.84\\). \\(\\mathbb{P}(Z &lt; z) = 0.03\\). The cell \\(-1.88\\) has value \\(0.0301\\) is closest to 0.03, so \\(z = -1.88\\). \\(\\mathbb{P}(Z &lt; z) = 0.95\\) so \\(\\mathbb{P}(Z &lt; -z) = 0.05\\). Now cell \\(-1.64\\) has value \\(0.0505\\) and cell \\(-1.65\\) has value \\(0.0495\\) both close to \\(0.05\\). We then take the average of the two cells \\[-z = \\frac{-1.64-1.65}{2} = -1.645 \\hspace{5mm} \\Rightarrow \\hspace{5mm} z = 1.645.\\] Notes: The given \\(Z\\)-table has only negative values of \\(z\\) and probability values less than 0.5. So when looking for positive \\(z\\) or probability more than \\(0.5\\), we can use the following properties: complememnt: \\(\\mathbb{P}(Z &gt; z) = 1-\\mathbb{P}(Z \\le z)\\). \\(\\mathbb{P}(a &lt; Z \\le b) = \\mathbb{P}(Z \\le b) - \\mathbb{P}(Z \\le a)\\). symmetry around 0: \\(\\mathbb{P}(Z &lt; z) = \\mathbb{P}(Z &gt; -z)\\) Note that The first and the second property is true for any probability function, even for discrete probability mass function. As we discuss in Chapter 4, for continuous random variable, \\(\\mathbb{P}(X = x) = 0\\), so \\(&lt;\\) and \\(\\le\\) can be used interchangeably for continuous random variable. Same for \\(&gt;\\) and \\(\\ge\\). The third property is true for only distributions that are symmetric around 0. 5.3 \\(Z\\)-score 5.3.1 Linearity of Normal Distribution In Chapter 4, we learn the two properties of expectation: \\[\\begin{align*} \\mathbb{E}(a + bX) &amp; = a + b\\mathbb{E}(X) \\\\ \\mathrm{var}(a + bX) &amp; = b^2\\mathrm{var}(X) \\end{align*}\\] for constants \\(a\\) and \\(b\\). So, if \\(X\\) has mean \\(\\mu\\) and variance \\(\\sigma^2\\) and \\(Y = aX + b\\) then \\[\\mathbb{E}[Y] = a + b\\mu \\hspace{5mm} \\text{and} \\hspace{5mm} \\mathrm{var}(Y) = b^2\\sigma^2\\] This means if we know the mean and variance of \\(X\\), we know the mean and variance of \\(Y\\). However, we may not know the distribution of \\(Y\\). However, there is a special property of the normal distribution, called the linearity of normal distribution, that says \\[\\text{If } X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\hspace{5mm} \\text{ then } \\hspace{5mm} Y = a + bX \\sim \\mathcal{N}(a + b\\mu, b^2\\sigma^2) \\] So a linear transformation of a normal distribution is another normal distribution with the mean and the variance calculated based on the linearity property of expectation. 5.3.2 \\(Z\\)-score In the above linearity property of normal distribution, if we let \\(a = -\\frac{\\mu}{\\sigma}\\) and \\(b = \\frac{1}{\\sigma}\\), then \\[\\text{If } X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\hspace{5mm} \\text{ then } \\hspace{5mm} Z = \\frac{X-\\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)\\] \\(Z = \\frac{X-\\mu}{\\sigma}\\) is called the \\(Z\\)-score of \\(X\\) and \\(X\\) is said to be standardized to be \\(Z\\). So, any normal distribution can be transformed into a standard normal distribution by standardizing them into a \\(Z\\)-score. This is extremely helpful when we calculate the probabilities for we normal random variables, because we do not need to do integration for every different normal distribution with different \\(\\mu\\) and \\(\\sigma^2\\). I will illustrate this with examples in the next section. 5.3.3 Examples Example 5.2 Suppose the random variable \\(X\\) has a normal distribution with a mean of \\(\\mu = 120\\) and a standard deviation of \\(\\sigma = 20\\). Find \\(\\mathbb{P}(X &lt; 105)\\)? \\(\\mathbb{P}(92 &lt; X &lt; 108)\\)? \\(\\mathbb{P}(X = 84)\\)? The interquartile range of the distribution of \\(X\\)? Solution: \\(X \\sim \\mathcal{N}(\\mu = 120, \\sigma^2 = (20)^2)\\) \\[\\begin{align*} \\mathbb{P}(X &lt; 105) &amp; = \\mathbb{P}\\left(\\frac{X-\\mu}{\\sigma} &lt; \\frac{105-\\mu}{\\sigma}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z &lt; \\frac{105-120}{20}\\right) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.75) = 0.2266 \\end{align*}\\] \\[\\begin{align*} \\mathbb{P}(92 &lt; X &lt; 108) &amp; = \\mathbb{P}\\left(\\frac{92-120}{20} &lt; Z &lt; \\frac{108-120}{20}\\right) \\\\ &amp; = \\mathbb{P}(-1.4 &lt; Z &lt; -0.6) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.6) - \\mathbb{P}(Z &lt; -1.4) \\\\ &amp; = 0.2743 - 0.0808 = 0.1935 \\end{align*}\\] \\(\\mathbb{P}(X = 84) = 0\\) since \\(X\\) is a continuous random variable. Recall that \\(IQR = Q3 - Q1\\). Now Q1 is the 0.25 quantile and Q1 is the 0.75 quantile. \\(\\mathbb{P}(Z &lt; z) = 0.25\\). Because the cell \\(-0.67\\) is \\(0.2514\\) and the cell \\(-0.68\\) is \\(0.2483\\) both close to 0.25, then \\[z = \\frac{-0.67-0.68}{2} = -0.675\\] \\(\\mathbb{P}(Z &lt; z) = 0.75\\) so \\(\\mathbb{P}(Z &gt; -z) = 0.75\\). Then \\(\\mathbb{P}(Z &lt; -z) = 0.25\\). So \\(-z = -0.675\\) from above and \\(z = 0.675\\) Q1 for \\(X\\) is \\(-0.675 \\times 20 + 120 = 106.5\\). Q3 for \\(X\\) is \\(0.675 \\times 20 + 120 = 133.5\\) The IQR for distribution of \\(X\\) is \\(Q3-Q2 = 133.5-106.5 = 27\\) 16. Example 5.3 Suppose the students’ rent is normally distributed with mean price of \\(800\\) dollars and standard deviation of \\(100\\) dollars. What fraction of the rent will be within one standard deviation from the mean? What price range will the middle \\(80\\%\\) of the prices lie between? What fraction of the price is more than \\(\\$900\\)? Suppose school has some subsidy for students’ rent. How much should school give to each student so that less than \\(5\\%\\) of students pay more than \\(900\\) dollars? Without the subsidy, how much the standard deviation should be so that less than \\(5\\%\\) of students pay more than \\(900\\) dollars? Solution: Let \\(X\\) be the student’s rent. Then \\(X \\sim \\mathcal{N}(\\mu = 800, \\sigma^2 = (100)^2)\\) \\[\\begin{align*} \\mathbb{P}(800-100 &lt; X &lt; 800+100) &amp; = \\mathbb{P}(700 &lt; X &lt; 900) \\\\ &amp; = \\mathbb{P}\\left(\\frac{700-\\mu}{\\sigma} &lt; \\frac{X - \\mu}{\\sigma} &lt; \\frac{900-\\mu}{\\sigma} \\right) \\\\ &amp; = \\mathbb{P}\\left(\\frac{700-800}{100} &lt; Z &lt; \\frac{900-800}{100}\\right) \\\\ &amp; = \\mathbb{P}(-1&lt;Z&lt;1) \\\\ &amp; = \\mathbb{P}(Z &lt; 1) - \\mathbb{P}(Z &lt; -1) \\\\ &amp; = \\mathbb{P}(Z &gt; -1) - \\mathbb{P}(Z &lt; -1) \\\\ &amp; = (1 - \\mathbb{P}(Z &lt; -1)) - \\mathbb{P}(Z &lt; -1) \\\\ &amp; = 1 - 2\\times \\mathbb{P}(Z &lt; -1) \\\\ &amp; = 1 - 2\\times 0.1587 = 0.6826 \\end{align*}\\] The interval covers \\(80\\%\\). Therefore \\[\\mathbb{P}(a &lt; X &lt; b) = 0.8 \\hspace{5mm} \\Rightarrow \\hspace{5mm} \\mathbb{P}\\left(\\frac{a-800}{100} &lt; Z &lt; \\frac{b-800}{100}\\right) = 0.8 \\] Now, because this is the middle \\(80\\%\\), so the interval should be symmetric around the mean. Hence, \\[- \\frac{a-800}{100} = \\frac{b-800}{100}\\] Let \\(z = \\frac{b-800}{100}\\). We have \\[\\begin{align*} \\mathbb{P}(-z &lt; Z &lt; z) = 0.8 &amp; = \\mathbb{P}(Z &lt; z) - \\mathbb{P}(Z &lt; -z) \\\\ &amp; = \\mathbb{P}(Z &gt; -z) - \\mathbb{P}(Z &lt; -z) \\\\ &amp; = (1 - \\mathbb{P}(Z &lt; -z)) - \\mathbb{P}(Z &lt; -z) \\\\ &amp; = 1 - 2\\times\\mathbb{P}(Z &lt; -z) \\\\ 0.1 &amp; = \\mathbb{P}(Z &lt; -z) \\end{align*}\\] From the table, \\(-z = -1.28\\). Now \\[\\frac{b-800}{100} = 1.28 \\Rightarrow b = 928\\] \\[\\frac{a-800}{100} = -1.28 \\Rightarrow a = 672\\] So the middle \\(80\\%\\) of the rent price falls between \\(672\\) and \\(928\\) dollars. \\(\\mathbb{P}(X &gt; 900) = \\mathbb{P}\\left(Z &gt; \\frac{900-800}{100}\\right) = \\mathbb{P}(Z &gt; 1) = \\mathbb{P}(Z &lt; -1) = 0.1587\\). We want \\(\\mathbb{P}(X &gt; 900) &lt; 0.05\\). If school gives each student \\(x\\) dollars for rent, the mean rent to be paid will decrease by \\(x\\) dollars. This means \\[\\begin{align*} \\mathbb{P}\\left(Z &gt; \\frac{900 - (800-x)}{100}\\right) &amp; &lt; 0.05 \\\\ \\Rightarrow \\mathbb{P}\\left(Z &lt; \\frac{(800-x)-900}{100}\\right) &amp; &lt; \\mathbb{P}(Z &lt; -1.645) \\\\ \\Rightarrow \\frac{(800-x)-900}{100} &amp; &lt; -1.645 \\\\ \\Rightarrow x &gt; 64.5 \\end{align*}\\] So school needs to give each student at least 64.5 dollars. We want to find a new standard deviation. Suppose it is \\(x\\). Then \\[\\begin{align*} \\mathbb{P}\\left(Z &gt; \\frac{900 - 800}{x}\\right) &amp; &lt; 0.05 \\\\ \\Rightarrow \\mathbb{P}\\left(Z &lt; \\frac{800-900}{x}\\right) &amp; &lt; \\mathbb{P}(Z &lt; -1.645) \\\\ \\Rightarrow \\frac{(800-900}{x} &amp; &lt; -1.645 \\\\ \\Rightarrow x &lt; 60.79 \\end{align*}\\] So the standard deviation has to be at most 60.79 dollars. Exercise 5.1 Verify the empirical rule of normal distribution. Hint: Look at part a of Example 5.3. 5.4 Calculate Normal Probabilities in R We can also skip looking at the \\(Z\\)-table and directly calculate normal probabilities in R. To calculate \\(Z\\)-probabilities in R, we use two functions pnorm() and qnorm(). The function pnorm(a) will give you the probability \\(\\mathbb{P}(Z &lt; a)\\) for any number \\(a\\). For example, pnorm(2) ## [1] 0.9772499 is the probability \\(\\mathbb{P}(Z &lt; 2)\\). The function qnorm(q) will give you the number \\(a\\) that satisfy \\(\\mathbb{P}(Z &lt; a) = q\\). For example, qnorm(0.75) ## [1] 0.6744898 tells us that \\(\\mathbb{P}(Z &lt; 0.674) = 0.75\\). Besides \\(Z\\), you can also calculate similar probabilities for \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) by specifying arguments \\(\\mu\\) and \\(\\sigma\\) to parameters mean and sd of the two functions pnorm() and qnorm(). For example, pnorm(2, mean = 1, sd = 2) ## [1] 0.6914625 is the probability \\(\\mathbb{P}(X &lt; 2)\\) if \\(X \\sim \\mathcal{N}(1, 2^2)\\). Notes: In this Chapter, we focus only on the normal distribution. The reason is that it is the most important distribution in statistics! The problems related to this lesson can be new and challenging. But again there is no short cut but to practice and practice! Try to practice looking up the \\(Z\\)-table, and work out the examples and exercises in this lesson by yourself. Alternative solution: IQR for \\(Z\\) is \\(0.675 - (-0.675) = 1.35\\). So IQR for \\(X\\) is \\(1.35*20 = 27\\). You don’t need to add 120 because when we subtract Q1 from Q3, 120 got canceled.↩︎ "],["chap-mean.html", "6 Sampling Distribution of the Mean 6.1 Sampling Distribution 6.2 Sample Mean", " 6 Sampling Distribution of the Mean The last two chapters has prepared us with foundation to start learning about statistical inference promised in Chapter 1. Recall that while descriptive statistics focuses on describing the sample data, inferential statistics tries to make conclusion about the population. In this chapter, we will focus on the conclusion about the mean parameter in the population. Think about it: we usually want to ask questions like: What is the average height of Canadian men/women? How much does a Canadian earn on average? … In these cases, the mean of the population is exactly what we are interested in. 6.1 Sampling Distribution 6.1.1 Sample Random Variables Consider a probabilistic experiment and \\(n\\) trials of that specific probabilistic experiment. Suppose we are interested in the random variable \\(X\\) of the probabilistic experiment. Since the outcome of each trial (run) is unknown unless we actually conduct the trials, the value of \\(X\\) for each trial is also unknown. Thus, they are also random variables, defined as follows. \\(X_1\\): the value of \\(X\\) based on the outcome of the first trial Because I do not know the outcome of the first trial unless I conduct it, the value of \\(X\\) for the first trial is random. So, \\(X_1\\) is a random variable according to Chapter 4. \\(X_2\\): the value of \\(X\\) based on the outcome of the second trial Similar to \\(X_1\\), \\(X_2\\) is also a random variable … \\(X_n\\): a variable based on the outcome of the \\(n\\)th trial Now \\(X_1, X_2, ...., X_n\\) are random variables. Their values are random (unknown). Example 6.1 Consider a familiar probabilistic experiment: tossing a coin and the variable \\(X\\) defined by \\[X = \\begin{cases} 1 &amp; \\text{if a head is obtained} \\\\ 0 &amp; \\text{if a tail is obtained} \\end{cases}\\] If we toss \\(n\\) coins, i.e., if we have \\(n\\) trials of the coin tossing experiment, then \\(X_1\\): whether a head is obtained in the first trial \\(X_2\\): whether a head is obtained in the second trial … \\(X_n\\): whether a head is obtained in the \\(n\\)th trial Example 6.2 Consider randomly selecting one student. I do not know who I will pick before I actually pick them. Hence, randomly picking one student is a probabilistic experiment. The student’s rent is the variable of interest and suppose we denote it as \\(Y\\). This is a random variable which is based on the student I pick, i.e., the outcome of my probabilistic experiment. If I randomly select \\(1000\\) students. Then \\(Y_1\\): rent of the first student. \\(Y_2\\): rent of the second student. … \\(Y_{1000}\\): rent of the \\(1000\\)th student. 6.1.2 Independent Trials \\(n\\) trials are said to be independent if the outcome of one trial does not affect the outcome of the another trial. Example 6.3 Tossing a coin. Suppose the result of one coin toss, whether it is head or tail, does not affect the result of the next or next next or the next next … next coin toss. Then we says that the \\(n\\) coin tosses are independent trials. Example 6.4 Suppose I pick the first random person on the street and ask about their rent (\\(X_1\\)). Then I let them suggest the next person to pick. I do not know the second person I am going to pick before I pick the first person. So the next actual person I am going to pick is also random and the second person’s rent \\(X_2\\) is also a random variable. However, the outcome of the second pick depends on the first pick. So \\(X_2\\) and \\(X_1\\) are not independent. If our \\(n\\) trials are independent, then the random variables \\(X_1, X_2, ...., X_n\\) based on \\(n\\) independent trials are called independent. 6.1.3 Population Distribution If the random variable \\(X\\) of the probabilistic experiment follows a probability distribution, then such probability distribution is called the population distribution. Since \\(X_1, X_2, ..., X_n\\) are based on the results of \\(n\\) trials of the probabilistic experiments, they also follow the same distribution as \\(X\\). Notation: \\[X_i \\sim X, \\hspace{10mm} i = 1, 2, ..., n\\] We say that \\(X_1, X_2, ..., X_n\\) are identically distributed. Equivalently, we say \\(X_1, X_2, ..., X_n\\): are drawn from distribution \\(X\\) are sampled from distribution \\(X\\) are identically distributed following \\(X\\) follow distribution \\(X\\) If in addition, \\(X_1, ..., X_n\\) are independent, we say that \\(X_1, X_2, ..., X_n\\) are independently and identically distributed (iid). Notation \\[X_i \\overset{\\text{iid}}{\\sim} X, \\hspace{10mm} i = 1, 2, ..., n\\] Example 6.5 If \\(X\\) follows a \\(\\mathrm{Unif}(c, d)\\) distribution, then we write \\[X_i \\sim \\mathrm{Unif}(c, d), \\hspace{10mm} i = 1, 2, ..., n\\] and if \\(X_1, ..., X_n\\) are also independent, then we write \\[X_i \\overset{\\text{iid}}{\\sim} \\mathrm{Unif}(c, d), \\hspace{10mm} i = 1, 2, ..., n\\] 6.1.4 Sampling Distribution A statistic is a function of \\(X_1, X_2, ..., X_n\\). Being a function of random variables, the statistic itself is also a random variable. The probabilistic distribution of a statistic is called the sampling distribution of the statistic. This means that if we repeat collecting different samples of fixed sample size \\(n\\) many times, and calculate the corresponding statistic from those different samples, we will obtain the sampling distribution of the statistic. These definitions may seem very abstract. To understand these concepts thoroughly, it is best to put them into context. In that sense, let us discuss one of the most important example: the sample mean statistic. 6.2 Sample Mean 6.2.1 Sample Mean Random Variable Suppose the random variable of interest \\(X\\) of the probabilistic experiment follows a distribution that has mean \\(\\mu\\). That is, \\(\\mu\\) is the population mean and hence a population parameter. The sample mean/average of \\(X_1, X_2, ..., X_n\\) is \\[\\bar{X}_n = \\frac{X_1 + X_2 + ... + X_n}{n} = \\frac{1}{n}\\sum_{i=1}^n X_i,\\] which is a function of \\(X_1, X_2, ..., X_n\\). Therefore, it is a sample statistic. Here, the notation \\(\\bar{X}_n\\) stresses that the sample mean is calculated based on \\(n\\) trials. But sometimes we write \\(\\bar{X}\\) instead of \\(\\bar{X}_n\\) for simplicity. Because \\(\\bar{X}_n\\) is calculated based on \\(n\\) random variables, it is itself also a random variable. 6.2.2 Observed Sample Means Now, it is important to differentiate the sample mean random variable and the observed sample mean we learn from Chapter 2: \\(\\bar{X}_n\\): the random/unknown sample mean, i.e., the statistic, which is calculated from the trials’ random variables \\(X_1, X_2, ..., X_n\\) \\(\\bar{x}_n\\): the observed/realized sample mean, i.e., the observed statistic, which is calculated from the observed data the observed sample mean \\(\\bar{x}_n\\) (or for short, \\(\\bar{x}\\)) is calculated from the observed values \\(x_1, ..., x_n\\) of the random variables \\(X_1, ..., X_n\\) when the \\(n\\) trials are conducted/realized. I will write \\(\\bar{x}_n\\) when I explain concepts and I will write \\(\\bar{x}\\) for short when I solve example questions. Example 6.6 Revisit Example 6.2. \\(Y\\): rent of the student picked. Suppose one day, I go and collect data from 5 randomly chosen students. The results are as follows: General unknown outcome of trial General unknown value of \\(Y\\) Known &amp; realized outcome of trial Known &amp; realized value of \\(Y\\) Person 1 \\(Y_1\\): rent of person 1 Tom \\(y_1 = 830\\) Person 2 \\(Y_2\\): rent of person 2 Mary \\(y_2 = 490\\) Person 3 \\(Y_3\\): rent of person 3 Steve \\(y_3 = 720\\) Person 4 \\(Y_4\\): rent of person 4 John \\(y_4 = 670\\) Person 5 \\(Y_5\\): rent of person 5 Sarah \\(y_5 = 1010\\) Then \\[\\bar{Y}_5 = \\frac{Y_1 + Y_2 + ... + Y_5}{5} = \\text{the average rent of the 5 randomly selected people}\\] and \\[\\bar{y}_5 = \\frac{y_1 + y_2 + ... + y_5}{5} = \\frac{830+490+720+670+1010}{5} = 744\\] So here, \\(\\bar{Y}_5\\) is a general and unknown quantity, a random variable. And \\(\\bar{y}_5 = 744\\) is an actual number. 6.2.3 Sampling Distribution of the Sample Mean If we repeat collecting random samples of the same sample size \\(n\\), each time we will obtain a different set of observed values \\(x_1, x_2, ..., x_n\\) and hence we will get a different value of \\(\\bar{x}_n\\). These different observed values of \\(\\bar{x}_n\\) together form the distribution of the random variable \\(\\bar{X}_n\\). This is called the sampling distribution of \\(\\bar{X}_n\\). Example 6.7 Revisit Example 6.1: tossing a coin and consider the random variable \\(X\\) which equals to \\(1\\) if a head is obtained, and equals to 0 if a tail is obtained. Suppose that the coin is fair, then \\[p_X(x) = \\mathbb{P}(X = x) = \\begin{cases} \\frac{1}{2} &amp; \\text{if } x = 1 \\\\ \\frac{1}{2} &amp; \\text{if } x = 0\\end{cases}\\] \\(p_X(x)\\) is the population distribution of \\(X\\). Now, suppose I conduct 3 trials of the probabilistic experiment, i.e., I toss the coin 3 times. \\(X_1\\): whether I obtain a head in trial 1 \\(X_2\\): whether I obtain a head in trial 2 \\(X_3\\): whether I obtain a head in trial 3 If I repeat tossing the coins 3 times for many times, each possible value of the trio \\((X_1, X_2, X_3)\\) has the same probability of happening. There are in total 8 possible combinations of \\((X_1, X_2, X_3)\\) so the probability of each to happen is \\(1/8\\). Therefore we have Probability of happening Possible values of \\(\\{X_1, X_2, X_3\\}\\) Value of \\(\\bar{X}_3\\) \\(1/8\\) \\(\\{0,0,0\\}\\) \\(0\\) \\(1/8\\) \\(\\{0,0,1\\}\\) \\(1/3\\) \\(1/8\\) \\(\\{0,1,0\\}\\) \\(1/3\\) \\(1/8\\) \\(\\{0,1,1\\}\\) \\(2/3\\) \\(1/8\\) \\(\\{1,0,0\\}\\) \\(1/3\\) \\(1/8\\) \\(\\{1,0,1\\}\\) \\(2/3\\) \\(1/8\\) \\(\\{1,1,0\\}\\) \\(2/3\\) \\(1/8\\) \\(\\{1,1,1\\}\\) \\(1\\) Based on the table, the sampling distribution of \\(\\bar{X}_3\\) is \\[p_{\\bar{X}_3}(z) = \\mathbb{P}(\\bar{X}_3 = z) = \\begin{cases} 1/8 &amp; \\text{if } z = 0 \\\\ 3/8 &amp; \\text{if } z = \\frac{1}{3} \\\\ 3/8 &amp; \\text{if } z = \\frac{2}{3} \\\\ 1/8 &amp; \\text{if } z = 1 \\end{cases}\\] Note here that the sampling distribution of \\(\\bar{X}_3\\) is different from the population distribution of \\(X\\). Exercise 6.1 In Example 6.7, what is the sampling distribution of \\(\\bar{X}_4\\)? 6.2.4 Expectation and Variance 6.2.4.1 Expectation and Variance of a Linear Combination of Random Variables Generally, if \\(X\\) and \\(Y\\) are two random variables, then \\[\\mathbb{E}(aX + bY) = a\\mathbb{E}(X) + b\\mathbb{E}(Y) \\] If further \\(X\\) and \\(Y\\) are independent, then \\[\\mathrm{var}(aX + bY) = a^2\\mathrm{var}(X) + b^2\\mathrm{var}(Y)\\] 6.2.4.2 Expectation and Variance of the Sample Mean Now, apply the above result to the sample mean. If \\(X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} X\\) where \\(\\mathbb{E}(X) = \\mu\\) and \\(\\mathrm{var}(X) = \\sigma^2\\). Then \\[\\mathbb{E}(\\bar{X}_n) = \\mathbb{E}\\left(\\frac{X_1 + ... + X_n}{n}\\right) = \\frac{1}{n}\\Big[\\mathbb{E}(X_1) + ... + \\mathbb{E}(X_n) \\Big] = \\frac{1}{n}n\\mu = \\mu\\] The variance is \\[\\mathrm{var}(\\bar{X}_n) = \\mathrm{var}\\left(\\frac{X_1 + ... + X_n}{n}\\right) = \\frac{1}{n^2}\\Big[\\mathrm{var}(X_1) + ... + \\mathrm{var}(X_n) \\Big] = \\frac{1}{n}n\\sigma^2 = \\frac{\\sigma^2}{n}\\] So the sample mean \\(\\bar{X}\\) has mean \\(\\mu\\) and variance \\(\\sigma^2/n\\). 6.2.5 Sample Mean of Independent Normal Draws Property (Linearity of normal distribution): A linear combination (i.e., \\(a_1Y_1 + a_2Y_2 + ... + a_mY_m\\)) of normally distributed random variables (\\(Y_1, Y_2, ..., Y_m\\)) is also a normal random variable. Now, if we suppose further that \\(X_1, X_2, ..., X_n\\) are normally distributed. That is, \\[X_i \\overset{iid}{\\sim} \\mathcal{N}(\\mu, \\sigma^2), \\hspace{5mm} i = 1, 2, ..., n\\] then using the linearity property of normal distribution, we can deduce that the sample mean \\(\\bar{X}_n\\) is also normally distributed with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\). That is, \\[\\text{If } X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2), \\hspace{5mm} \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] This means that the sample mean random variable \\(\\bar{X}_n\\), i.e., the average calculated from \\(n\\) independent normal draws is also normally distributed. is unbiased for \\(\\mu\\) because its bias is zero, in the sense that \\[\\mathbb{E}[\\bar{X}_n] = \\mu \\hspace{5mm} \\text{and} \\hspace{5mm} \\mathbb{E}[\\bar{X}_n] - \\mu = 0\\] Let me explain this in words. Suppose I have \\(m\\) samples, for each sample I conduct \\(n\\) trials. For each sample I can calculate a value \\(\\bar{x}_n\\) of \\(\\bar{X}_n\\), say \\(\\bar{x}_n^{(1)}, \\bar{x}_n^{(2)}, ..., \\bar{x}_n^{(m)}\\). Then, if \\(m\\) is large, the average of values \\(\\bar{x}_n^{(i)}\\), \\(i = 1, 2, ..., m\\) will be the true parameter \\(\\mu\\). We say that \\(\\bar{X}\\) is unbiased of \\(\\mu\\). increases in precision as \\(n\\) increases: when \\(n\\) becomes larger, \\(\\mathrm{var}(\\bar{X}_n)\\) becomes smaller \\[\\lim_{n\\to\\infty}\\mathrm{var}(\\bar{X}_n) = \\lim_{n\\to\\infty}\\frac{\\sigma^2}{n} = 0\\] This means that The spread of the distribution of \\(\\bar{X}_n\\) becomes smaller. Hence, as \\(n\\) increases, realized values \\(\\bar{x}_n\\) will tend to be closer to each other, and we say that \\(\\bar{X}_n\\) becomes more precise (i.e., its variance decreases) as \\(n\\) increases (Recall the definition of precision in Chapter 1). Smaller variance \\(\\mathrm{var}(\\bar{X}_n)\\) also means that the distribution of \\(\\bar{X}_n\\) pulls more towards the center as \\(n\\) increases, i.e., the distribution becomes closer to the mean \\(\\mathbb{E}(\\bar{X}_n)\\). But \\(\\mathbb{E}(\\bar{X}_n) = \\mu\\). In this sense, the sample mean \\(\\bar{X}_n\\) will get closer to the population mean \\(\\mu\\) as our population becomes bigger. This is logical because if our sample size is large, our sample will contain more units from the population. Thus our sample will get closer to population. We say that \\(\\bar{X}_n\\) estimates the population mean \\(\\mu\\) better when \\(n\\) is large. This is the reason why we want large sample size \\(n\\) in Chapter 1. The figure below shows the distribution of \\(\\bar{X}_n\\) for \\(n = 1, 2, 4\\) if \\(X \\sim \\mathcal{N}(0, 10)\\). Figure 6.1: The sampling distribution of \\(\\bar{X}_n\\) if we are sampling from a \\(\\mathcal{N}(0, 10)\\) population. Example 6.8 Suppose that 16 observations are randomly selected from a normally distributed population where \\(\\mu = 10\\) and \\(\\sigma^2 = 625\\). What is the distribution of \\(\\bar{X}\\)? What is \\(\\mathbb{P}(\\bar{X} &gt; 30)\\)? What is \\(\\mathbb{P}(0 &lt; \\bar{X} &lt; 20)\\)? Solution: Because \\(\\bar{X}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\). Hence, for this problem17 \\[\\bar{X} \\sim \\mathcal{N}\\left(10, \\frac{625}{16}\\right)\\] Because \\(\\bar{X}\\) is normally distributed, we use the standard normal distribution to solve for probability of \\(\\bar{X}\\). \\[\\begin{align*} \\mathbb{P}(\\bar{X} &gt; 30) &amp; = \\mathbb{P}\\left(Z &gt; \\frac{30-10}{\\sqrt{\\frac{625}{16}}}\\right) \\\\ &amp; = \\mathbb{P}(Z &gt; 3.2) \\\\ &amp; = \\mathbb{P}(Z &lt; -3.2) \\\\ &amp; = 0.0007 \\end{align*}\\] \\[\\begin{align*} \\mathbb{P}(0 &lt; \\bar{X} &lt; 20) &amp; = \\mathbb{P}\\left(\\frac{0-10}{\\sqrt{\\frac{625}{16}}} &lt; Z &lt; \\frac{20-10}{\\sqrt{\\frac{625}{16}}}\\right) \\\\ &amp; = \\mathbb{P}(-1.6 &lt; Z &lt; 1.6) \\\\ &amp; = \\mathbb{P}(Z &lt; 1.6) - \\mathbb{P}(Z &lt; -1.6) \\\\ &amp; = \\mathbb{P}(Z &gt; -1.6) - \\mathbb{P}(Z &lt; -1.6) \\\\ &amp; = 1 - \\mathbb{P}(Z &lt; -1.6) - \\mathbb{P}(Z &lt; -1.6) \\\\ &amp; = 1 - 2\\times \\mathbb{P}(Z &lt; -1.6) \\\\ &amp; = 1 - 2 \\times 0.0548 = 0.8904 \\end{align*}\\] Example 6.9 Assume that the the price of games sold on iPad is normally distributed with a mean of \\(\\$3.48\\) and a standard deviation of \\(\\$2.23\\). What is the probability that the price of one randomly selected game lies between \\(\\$3.50\\) and \\(\\$4.00\\)? If 40 games are randomly selected, what is the probability that the average price of these games lies between \\(\\$3.50\\) and \\(\\$4.00\\)? Solution: Let \\(X\\): price of a randomly selected iPad game. Then \\(X \\sim \\mathcal{N}(3.48, 2.23^2)\\). \\[\\begin{align*} \\mathbb{P}(3.5 &lt; X &lt; 4) &amp; = \\mathbb{P}\\left(\\frac{3.5-3.48}{2.23} &lt; Z &lt; \\frac{4-3.48}{2.23} \\right) \\\\ &amp; = \\mathbb{P}(0.01 &lt; Z &lt; 0.23) \\\\ &amp; = \\mathbb{P}(Z &lt; 0.23) - \\mathbb{P}(Z &lt; 0.01) \\\\ &amp; = \\mathbb{P}(z &gt; -0.23) - \\mathbb{P}(z &gt; -0.01) \\\\ &amp; = (1 - \\mathbb{P}(Z &lt; -0.23)) - (1 - \\mathbb{P}(Z &lt; -0.01)) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.01) - \\mathbb{P}(Z &lt; -0.23) \\\\ &amp; = 0.4960 - 0.4090 = 0.0870 \\end{align*}\\] Because there are 40 samples, \\(\\bar{X} \\sim \\mathcal{N}\\left(3.48, \\frac{2.23^2}{40}\\right)\\). So \\[\\begin{align*} \\mathbb{P}(3.5 &lt; \\bar{X} &lt; 4) &amp; = \\mathbb{P}\\left(\\frac{3.5-3.48}{\\frac{2.23}{\\sqrt{40}}} &lt; Z &lt; \\frac{4-3.48}{\\frac{2.23}{\\sqrt{40}}}\\right) \\\\ &amp; = \\mathbb{P}(0.06 &lt; Z &lt; 1.47) \\\\ &amp; = \\mathbb{P}(Z &lt; 1.47) - \\mathbb{P}(Z &lt; 0.06) \\\\ &amp; = (1-\\mathbb{P}(Z &lt; -1.47)) - (1-\\mathbb{P}(Z &lt; -0.06)) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.06) - \\mathbb{P}(Z &lt; -1.47) \\\\ &amp; = 0.4761 - 0.0708 = 0.4053 \\end{align*}\\] 6.2.6 Central Limit Theorem Suppose \\(X_1, X_2, ..., X_n\\) are independent and identically distributed random variables from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). That is \\[X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} (\\mu, \\sigma^2)\\] where \\(\\sigma^2 &lt; \\infty\\). Then the Central Limit Theorem states that when \\(n\\) is large the sample mean \\(\\bar{X}_n\\) is approximately normally distributed with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{n}\\). That is, \\[\\text{If } X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} (\\mu, \\sigma^2), \\text{ when $n$ is large:} \\hspace{5mm} \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_n \\overset{\\cdot}{\\sim} \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] where \\(\\overset{\\cdot}{\\sim}\\) means approximately follows. Notes: Regardless of the population distribution that we are sampling from, if the sample size \\(n\\) is large, the sample mean \\(\\bar{X}_n\\) will always be approximately normal. Under most conditions, sample size \\(\\mathbf{n &gt; 30}\\) will be sufficient for good approximation. The Central Limit Theorem shows us once again the importance of normal distribution in Statistics. Example 6.10 Suppose that house prices in a very large city are known to have a mean of \\(\\$389,000\\) and a standard deviation of \\(\\$120,000\\). What is the probability that the mean price of \\(100\\) randomly selected houses is more than \\(\\$400,000\\)? Solution: Let \\(X\\): price of a randomly selected house in the city. We do not now the distribution of \\(X\\), only its mean and standard deviation. We are interested in the mean price. The sample size \\(100\\) is greater than 30. For the above reasons, we can use the Central Limit Theorem and have \\[\\bar{X} \\sim \\mathcal{N}\\left(389000, \\frac{120000^2}{100}\\right)\\] Now we can solve for the probability asked in the question \\[\\begin{align*} \\mathbb{P}(\\bar{X} &gt; 400000) &amp; = \\mathbb{P}\\left(Z &gt; \\frac{400000 - 389000}{120000/\\sqrt{100}}\\right) \\\\ &amp; = \\mathbb{P}(Z &gt; 0.92) \\\\ &amp; = \\mathbb{P}(Z &lt; -0.92) = 0.1788. \\end{align*}\\] Example 6.11 A manufacturer of automobile batteries claims that the distribution of the length of life of its batteries has a mean of \\(54\\) months and a standard deviation of \\(6\\) months. Recently, the manufacturer has received a rash of complaints from unsatisfied customers whose batteries have died earlier than expected. Suppose a consumer group decides to check the manufacturer’s claim by purchasing a sample of \\(50\\) of these batteries and subjecting them to tests that determine battery life. Assuming that the manufacturer’s claim is true, what is the probability that the consumer group’s sample has A mean life of \\(52\\) or fewer months? A mean life between 52 and 56 months? Solution: Let \\(X\\): life times of randomly selected battery. We again do not know the distribution of \\(X\\), only its mean and standard deviation. We are interested in the sample mean of \\(50 &gt; 30\\) randomly selected batteries. So we can use the central limit theorem: \\[\\bar{X} \\sim \\mathcal{N}\\left(54, \\frac{6^2}{50}\\right)\\] \\[\\mathbb{P}(\\bar{X} \\le 52) = \\mathbb{P}\\left( Z \\le \\frac{52 - 54}{\\frac{6}{\\sqrt{50}}} \\right) = \\mathbb{P}(Z \\le -2.36) = 0.0091\\] \\[\\begin{align*} \\mathbb{P}(52 \\le \\bar{X} \\le 56) &amp; = \\mathbb{P}\\left(\\frac{52-54}{\\frac{6}{\\sqrt{50}}} \\le Z \\le \\frac{56-54}{\\frac{6}{\\sqrt{50}}}\\right) \\\\ &amp; = \\mathbb{P}(-2.36 \\le Z \\le 2.36) \\\\ &amp; = \\mathbb{P}(Z \\le 2.36) - \\mathbb{P}(Z \\le -2.36) \\\\ &amp; = 1 - 2\\times \\mathbb{P}(Z \\le -2.36) \\\\ &amp; = 1 - 2\\times 0.0091 = 0.9818. \\end{align*}\\] Notes: This chapter started by introducing concepts that are very abstract, especially when I talk about sampling distribution. My intention is to give you some feeling about the abstract concepts. And at the same time I do not want to make you misunderstand the concepts by explaining something too casually. However, I have included a lot of examples to help illustrate what I say. You can go back and forth between the explanation and the examples to try to understand things more clearly. We write \\(\\bar{X}\\) instead of \\(\\bar{X}_n\\) for simplicity.↩︎ "],["chap-confint.html", "7 Confidence Intervals 7.1 Pivotal Quantity 7.2 Confidence Intervals 7.3 Confidence Interval for \\(\\mu\\) When \\(\\sigma\\) is Known 7.4 Confidence Interval for \\(\\mu\\) When \\(\\sigma\\) is Unknown 7.5 Summary", " 7 Confidence Intervals In the last chapter, we have learned about sampling distribution and a very important theorem in Statistics: the Central Limit Theorem, which states that no matter which distribution you start with, as long as you have a large enough independent sample from an identical distribution, your sample mean will always approximately follow normal distribution. Now we have all the tools we need to do statistical inference, which was promised in Chapter 1. Recall that statistical inference helps you use information from a sample to make conclusions about the population. There are two popular types of conclusions that we can make: (i) the intervals where the population parameter can lie in with high probabilities, i.e., confidence intervals; and (ii) the probabilistic answers to yes-no questions about the population parameter, i.e., hypothesis tests. In this chapter, we will first learn about confidence intervals. 7.1 Pivotal Quantity An important tool for confidence intervals and hypothesis tests is pivotal quantity. A pivotal quantity \\(Q_{\\theta}\\) for a population parameter \\(\\theta\\) is a function of the random sample data \\(X_1, X_2, ..., X_n\\) and \\(\\theta\\), i.e., \\[Q_{\\theta} = g(X_1, X_2, ..., X_n, \\theta),\\] so that the distribution of \\(Q_{\\theta}\\) is the same for all values of \\(\\theta\\). Notes: In this chapter, we will be studying confidence intervals for the population mean \\(\\mu\\). Therefore, we will focus our attention on pivotal quantities for \\(\\mu\\). 7.1.1 Linearity of Normal Distribution Recall in Chapter 5, we learn that \\[\\text{If } X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\text{ then } a+bX \\sim \\mathcal{N}(a + b\\mu, b^2\\sigma^2)\\] 7.1.2 Pivotal Quantity for the Mean of a Normal Population Suppose our population is normal, i.e., \\(X\\sim \\mathcal{N}(\\mu, \\sigma^2)\\), and we have the sample \\(X_1, X_2, ..., X_n\\). Then from Chapter 6, we learn the property of normal distribution that \\[\\bar{X}_n = \\frac{X_1 + X_2 + ... + X_n}{n} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] Note that \\(\\bar{X}_n\\) here is not a pivotal quantity for \\(\\mu\\) because the formula of \\(\\bar{X}_n\\) does not contain \\(\\mu\\), the distribution of \\(\\bar{X}_n\\) changes as \\(\\mu\\) changes. How do we create a pivotal quantity for \\(\\mu\\)? Because \\(\\bar{X}_n\\) is normally distributed, let’s use the linearity property of normal distribution to transform \\(\\bar{X}_n\\) into a \\(Z\\)-score \\[\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)\\] Now, \\(\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}}\\) is a pivotal quantity for \\(\\mu\\) because it is a function of both the sample data \\(X_1, X_2, ..., X_n\\) and \\(\\mu\\) its distribution is always \\(\\mathcal{N}(0, 1)\\) and does not change with \\(\\mu\\). 7.1.3 Pivotal Quantity for Population Mean via CLT Suppose our population has mean \\(\\mu\\) and variance \\(\\sigma^2\\), i.e., \\(X \\sim (\\mu, \\sigma^2)\\) and again we have the data \\(X_1, X_2, ..., X_n\\). But now, if we do not assume a normal distribution and we know that our sample size is large enough, we can use the Central Limit Theorem, which says that the sample mean is approximately normal: \\[\\bar{X}_n = \\frac{X_1 + X_2 + ... + X_n}{n} \\overset{\\cdot}{\\sim} N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] to similarly construct the pivotal quantity for \\(\\mu\\) when \\(n\\) is large \\[\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\overset{\\cdot}{\\sim} \\mathcal{N}(0,1)\\] Notes: So, for normal population \\((\\mu, \\sigma^2)\\) and sample size \\(n\\); any population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and a large enough sample size (\\(n &gt; 30\\)); the pivotal quantity for \\(\\mu\\) is \\[Q_{\\mu} = \\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)\\] 7.1.4 Pivotal Quantity for \\(\\mu\\) When \\(\\sigma\\) is Unknown We can see that the pivotal quantity in the previous sections involves not only \\(\\mu\\) but also \\(\\sigma\\). If \\(\\sigma\\) is unknown, then it will no longer be a pivotal quantity for \\(\\mu\\). We however can approximate \\(\\sigma\\) by the sample standard deviation \\(S\\): \\[S = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X})^2}\\] Note here that different from \\(s\\) (actual obtained value from the data set), \\(S\\) here is a random variable calculated from the random sample data \\(X_1, X_2, ..., X_n\\). When we replace \\(\\sigma\\) by \\(S\\), we have the following result \\[\\frac{\\bar{X}_n - \\mu}{S/\\sqrt{n}} \\sim t(n-1)\\] where \\(t(n-1)\\) is the (central) \\(t\\)-distribution of \\(n-1\\) degrees of freedom18. Now, this is a pivotal quantity because it is a function of both the sample data \\(X_1, X_2, ..., X_n\\) and \\(\\mu\\). (\\(S\\) is also a function of the data) its distribution is always \\(t(n-1)\\) and does not change with \\(\\mu\\). Thus, when \\(\\sigma\\) is unknown, we can construct a confidence interval for \\(\\mu\\) using this pivotal quantity. 7.1.4.1 The \\(t\\)-distribution The (central) \\(t\\)-distribution has only one parameter, which is the degree of freedom \\(df\\). The \\(t\\)-distribution is similar to the standard normal distribution in the sense that it is symmetric around 0 unimodal bell-shaped It is different from the standard normal distribution in the sense that It is more spread out. The spread of the distribution is controlled by the degrees of freedom parameter. There is more probability in the tail areas (heavy-tailed). Therefore, there is less probability in the center of the distribution Notes: As the degrees of freedom increase, the \\(t\\)-curve becomes closer to the standard normal curve. Figure 7.1: The probability curves of the \\(t\\)-distributions. Notice that the tails of the \\(t\\)-distributions are fatter than that of the standard normal distribution. We say that the \\(t\\)-distributions are . We will see how pivotal quantity helps us construct confidence intervals and hypothesis tests in the following sections and in Chapter 8. 7.2 Confidence Intervals 7.2.1 What is a Confidence Interval? It is intuitive to use the sample mean \\(\\bar{X}_n\\) to give an estimate of the population mean \\(\\mu\\). However, this one point estimate (the observed actual number \\(\\bar{x}_n\\)) does not reflect the uncertainty related to the random process to obtain the sample itself (i.e., each time we collect the data we will get a different set of observed values and a different number \\(\\bar{x}_n\\)). That is, it does not say how sure we are when we say \\(\\mu \\approx \\bar{x}_n\\) or how far \\(\\bar{x}_n\\) is from \\(\\mu\\). To the rescue, confidence intervals provide an interval estimate, i.e., the range of plausible values where the population parameter of interest (here, \\(\\mu\\)) lies within for a given probability. In this sense, confidence intervals acknowledge and quantify the uncertainty of the sample. All confidence intervals are based on a level of confidence, \\((1-\\alpha)100\\%\\). Here \\(\\alpha\\) is called the significance level. For example, for \\(\\alpha = 0.05\\), we have a \\(95\\%\\) confidence interval, and for \\(\\alpha = 0.01\\), we have \\(99\\%\\) confidence interval, … 7.2.2 Interpret a Confidence Interval Now we are interested in the population mean \\(\\mu\\). A \\((1-\\alpha)100\\%\\) confidence interval of \\(\\mu\\) is of the form \\((L_n, U_n)\\) so that \\[\\mathbb{P}(L_n &lt; \\mu &lt; U_n) = 1-\\alpha\\] where \\(L_n\\) and \\(U_n\\) are calculated from the sample \\(X_1, ..., X_n\\). \\(L_n\\) and \\(U_n\\) are functions of random variables \\(X_1, ..., X_n\\). Therefore, they are also random variables. This means that with different observed sample data \\(x_1, x_2, ..., x_n\\), we will obtain a different confidence interval \\((l_n, u_n)\\). \\(\\mathbb{P}(L_n &lt; \\mu &lt; U_n) = 1-\\alpha\\) means: if we repeat collecting the sample of the same size \\(n\\) for many times and each times obtain a different interval \\((l_n, u_n)\\), then \\((1-\\alpha)100\\%\\) of those intervals will contain the true population parameter \\(\\mu\\). Figure 7.2: Suppose we are interested in the mean \\(\\mu\\). If we repeat collecting samples many times and then calculate many \\(95\\%\\) confidence intervals for \\(\\mu\\), then \\(95\\%\\) of those intervals will contain the true population parameter \\(\\mu\\) (will cross the green vertical line) and \\(5\\%\\) of the intervals will not contain \\(\\mu\\). Notes: There are two types of interpretations for a \\((1-\\alpha)100\\%\\) confidence interval of the population parameter \\(\\mu\\) the exact interpretation: If we repeat collecting samples and calculating confidence intervals, then \\((1-\\alpha)100\\%\\) of those intervals will contain \\(\\mu\\) and \\(100\\alpha\\%\\) of the intervals will not contain \\(\\mu\\). the casual interpretation: we are \\((1-\\alpha)100\\%\\) confident that the true but unknown parameter \\(\\mu\\) will lie within the interval. If someone asks: what do you mean by \\((1-\\alpha)100\\%\\) confident? \\(\\rightarrow\\) Go to the exact interpretation. 7.3 Confidence Interval for \\(\\mu\\) When \\(\\sigma\\) is Known 7.3.1 Construct the Confidence Interval From Section 7.1, we know that for normal population \\((\\mu, \\sigma^2)\\) and sample size \\(n\\); any population with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and a large enough sample size \\(n &gt; 30\\); the pivotal quantity for \\(\\mu\\) is \\[Q_{\\mu} = \\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)\\] We can construct a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) from the \\(Q_\\mu\\), starting by finding the interval \\((a, b)\\) such that \\[\\begin{align*} 1-\\alpha &amp; = \\mathbb{P}(a &lt; Q_{\\mu} &lt; b) \\\\ &amp; = \\mathbb{P}\\left(a &lt; \\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}} &lt; b\\right) \\\\ &amp; = \\mathbb{P}\\left(\\bar{X}_n - b\\times \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X}_n - a\\times \\frac{\\sigma}{\\sqrt{n}}\\right) \\end{align*}\\] So for \\((a, b)\\) such that \\(\\mathbb{P}(a &lt; Q_{\\mu} &lt; b) = 1-\\alpha\\), \\[\\left(\\bar{X}_n - b\\times \\frac{\\sigma}{\\sqrt{n}}, \\bar{X}_n - a\\times \\frac{\\sigma}{\\sqrt{n}}\\right)\\] is a confidence interval for \\(\\mu\\). But how to find \\(a\\) and \\(b\\)? Because \\(Q_{\\mu} \\sim \\mathcal{N}(0, 1)\\), any number \\(a\\) or \\(b\\) which satisfy \\(\\mathbb{P}(a &lt; Z &lt; b) = 1-\\alpha\\) will work! Note that this is why I always say “a” confidence interval, not “the” confidence interval. Now, let \\(z_{q}\\) denote the \\(q\\)-quantile of \\(Z\\) (i.e., \\(\\mathbb{P}(Z &lt; z_q) = q\\)). Then some choices of \\(a\\) and \\(b\\) are \\(a = -\\infty\\) and \\(b = z_{1-\\alpha}\\) \\(a = z_{\\alpha}\\) and \\(b = +\\infty\\) \\(a = z_{m}\\) and \\(b = z_{1-\\alpha+m}\\) for some number \\(0 &lt; m &lt; \\alpha\\). The three choices are illustrated by the following figure Figure 7.3: Possible choices of interval \\((a,b)\\) that satisfy \\(\\mathbb{P}(a &lt; Z &lt; b) = 1-\\alpha\\) Convention: choose the third option and let \\(m = \\alpha/2\\), meaning \\(a = z_{\\alpha/2}\\) and \\(b = z_{1-\\alpha/2}\\), so that \\[a = -b = -z_{1-\\alpha/2}\\] and the interval \\((a,b)\\) will be symmetric. This is because the standard normal distribution is symmetric around 0! This choice is illustrated by the figure below. Figure 7.4: We choose a symmetric interval \\((a,b)\\) where \\(a = z_{\\alpha/2}\\) and \\(b = z_{1-\\alpha/2}\\) Replace \\(a\\) and \\(b\\) into our formula of confidence intervals, we have a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) \\[\\left(\\bar{X}_n - z_{1-\\alpha/2}\\times \\frac{\\sigma}{\\sqrt{n}}, \\bar{X}_n + z_{1-\\alpha/2}\\times \\frac{\\sigma}{\\sqrt{n}}\\right)\\] So, the probability that \\(\\mu\\) lies between \\(\\left(\\bar{X}_n - z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\bar{X}_n + z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is \\(1-\\alpha\\) and therefore it is a confidence interval for \\(\\mu\\). Short notation for such a confidence interval: \\[\\bar{X}_n \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\] If we replace \\(\\bar{X}_n\\) by the actual observed value \\(\\bar{x}_n\\), we obtain an actual value of the \\((1-\\alpha)100\\%\\) confidence interval \\[\\bar{x}_n \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\] Example 7.1 The Organisation for Economic Co-operation and Development (OECD) surveyed \\(1000\\) people at random in each of its member countries in \\(2011\\). They found that the number of hours worked by Canadians was \\(1699\\) hours per year on average, whereas for the average OECD country it was \\(1739\\). Assume that the population standard deviation for Canadians is 486 hours. Do Canadians work less than the average OECD countries? Create a \\(95\\%\\) confidence interval for the population mean? Create a \\(99\\%\\) confidence interval for the population mean? Solution: For this question, \\(n = 1000\\), \\(\\bar{x} = 1699\\), \\(\\sigma = 486\\). \\(95\\%\\) confidence interval means \\(1-\\alpha = 0.95\\) and \\(\\alpha = 0.05\\). Then \\(\\alpha/2 = 0.05\\) and \\(z_{1-\\alpha/2} = z_{0.975}\\). Look at the table, \\(z_{0.975} = -z_{0.025} = 1.96\\). Hence, a \\(95\\%\\) confidence interval for \\(\\mu\\) is \\[\\bar{x} \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = 1699 \\pm 1.96\\times \\frac{486}{\\sqrt{1000}} = (1668.88, 1729.12)\\] Because this confidence interval lies entirely below the value of \\(1739\\) and hence at a \\(95\\%\\) confidence level the data suggests that on average Canadians work less than the OECD average. \\(99\\%\\) confidence interval means \\(\\alpha = 0.01\\) and \\(z_{1-\\alpha/2} = z_{0.995} = -z_{-0.005} = 2.575\\). So a \\(99\\%\\) confidence interval for \\(\\mu\\) is \\[\\bar{x} \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = 1699 \\pm 2.575\\times \\frac{486}{\\sqrt{1000}} = (1659.43, 1738.57)\\] Again, even when we increase the confidence level and obtain a wider the confidence interval, it still lies entirely below the value of \\(1739\\). Hence at a \\(99\\%\\) confidence level the data suggests that on average Canadians work less than the OECD average. Notes: In Example 7.1, we can interpret the \\(95\\%\\) confidence interval \\((1668.88, 1729.12)\\) as “we are \\(95\\%\\) confident that the true but unknown average number of hours worked by Canadians lies between 1668.88 hours and 1729.12 hours”. Exercise 7.1 A management consulting firm has installed a new computer-based billing system in a trucking company. To determine if this system is better than the old billing system, the firm collects the payment times of \\(65\\) new payments. They find that the mean of the \\(65\\) payment times is \\(18.11\\) and from previous knowledge it is known that \\(\\sigma=4.2\\). Create a \\(95\\%\\) confidence interval for the mean payment time using the new system. The mean payment time using the old billing system was approximately \\(39\\) days. Based on the interval found in (a), does the new system appear to be better? Explain. 7.3.2 Margin of Error For the confidence interval \\(\\bar{x}_n \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\): the length of the confidence interval is \\(2\\times z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\) the margin of error is \\(m = z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\). So, the size of a confidence interval (CI) depends on the confidence level \\((1-\\alpha)100\\%\\): When the confidence level increases, the CI gets wider because \\(z_{a} &lt; z_{b}\\) if \\(a &lt; b\\). If we want to be more confident about our conclusion, we need to include more possible values for \\(\\mu\\) and so the CI needs to be wider. the standard deviation \\(\\sigma\\): When \\(\\sigma\\) increases, the CI gets wider. Notice that \\(\\sigma\\) describes the variability in the population distribution of \\(X\\) to begin with. When the variability in the population is already large, in repeated sampling, the values of the data \\(X_1, X_2, ..., X_n\\) will vary more and \\(\\bar{X}_n\\) will be often farther from \\(\\mu\\). So the CI needs to become wider to make sure that \\(\\mu\\) is covered. the sample size \\(n\\): When \\(n\\) increases, the CI gets narrower. When the sample size increases, the sample we collect become closer to the population, making us closer to the true parameter \\(\\mu\\), hence the CI becomes narrower. Notes: A good CI will give us precise conclusion with high confidence, hence it should be narrow and of high confidence level. 7.3.3 Determining the Minimum Sample Size \\(n\\) From the formula \\[m = z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\] we can determine the sample size before the sample is collected if we specify the margin of error, the level of confidence and if we know \\(\\sigma\\). For example, if we want that our margin of error to be of certain level \\(m\\), i.e., we want our estimate \\(\\bar{x}_n\\) to be at most \\(m\\) away from the true population parameter \\(\\mu\\) at a confidence level \\(1-\\alpha\\), then solving \\[z_{1-\\alpha/2} \\times \\frac{\\sigma}{\\sqrt{n}} \\le m\\] we have \\[n \\ge \\left(\\frac{z_{1-\\alpha/2}\\times\\sigma}{m} \\right)^2\\] This is the minimum sample size required when you are planning on drawing a simple random sample (SRS) from the population (i.e., you obtain iid sample data \\(X_1, X_2, ..., X_n\\)) This provides you with a guideline to the required sample size Sometimes, you may find that this value is not achievable in real-life due to time and budget constraints, or it is simply not practical. Example 7.2 In a survey done of Black Friday shoppers, one question was “How many hours do you usually spend shopping on Black Friday?” How many shoppers should be included in a sample designed to estimate the average number of hours spent shopping on Black Friday if you want the to estimate to deviate no more than \\(0.5\\) hour from the true mean? Assume you want \\(95\\%\\) confidence and \\(\\sigma=3\\). Solution: In this question the desired margin of error is \\(m = 0.5\\) the confidence level is \\(95\\%\\) so we use \\(z_{0.975} = 1.96\\) the population standard deviation \\(\\sigma = 3\\) Hence, the sample size required is \\[\\begin{align*} n &amp; \\ge \\left(\\frac{z_{1-\\alpha/2}\\times \\sigma}{m}\\right)^2 \\\\ &amp; \\ge \\left(\\frac{1.96 \\times 3}{0.5}\\right)^2 \\\\ &amp; \\ge 138.2976 \\\\ n &amp; \\ge 139 \\end{align*}\\] So we need to interview at least \\(139\\) people. Exercise 7.2 In Example 7.2, what will happen if the desired level of confidence is \\(90\\%\\)? 7.4 Confidence Interval for \\(\\mu\\) When \\(\\sigma\\) is Unknown 7.4.1 Construct the Confidence Interval In reality, it is common that we do not know \\(\\sigma\\) of the population. Therefore cannot use the method of Section 7.3 to construct confidence intervals. We have to alter the method and use another pivotal quantity from Section 4.1.4. \\[\\frac{\\bar{X}_n-\\mu}{S/\\sqrt{n}} \\sim t(n-1)\\] Let \\(t_{q,d}\\) denote the \\(q\\)-quantile of the \\(t\\)-distribution with \\(d\\) degrees of freedom, i.e., if \\(T \\sim t(d)\\) then \\(\\mathbb{P}(T \\le t_{q,d}) = q\\). Since \\(t\\) is also symmetric around 0, we can construct the confidence interval in a similar way as we did in Section 7.3 where \\(\\sigma\\) is known, but replace the population standard deviation \\(\\sigma\\) by the sample standard deviation \\(s\\); the standard normal distribution by the \\(t(n-1)\\) distribution, then we can obtain a \\((1-\\alpha)\\%\\) confidence interval for \\(\\mu\\) as \\[\\bar{x}_n \\pm t_{1-\\alpha/2,n-1}\\frac{s}{\\sqrt{n}}.\\] Similar as the normal distribution, we need to look up for \\(t_{1-\\alpha/2,n-1}\\) from the \\(t\\)-table. The \\(t\\)-table shows the \\(t_{d,q}\\) values for different degrees of freedom \\(d\\) and quantile \\(q\\). \\(t\\)-table: df \\(t_{0.80}\\) \\(t_{0.85}\\) \\(t_{0.90}\\) \\(t_{0.95}\\) \\(t_{0.975}\\) \\(t_{0.99}\\) \\(t_{0.995}\\) \\(t_{0.999}\\) \\(t_{0.9995}\\) 1 1.3764 1.9626 3.0777 6.3138 12.7062 31.8205 63.6567 318.3088 636.6192 2 1.0607 1.3862 1.8856 2.9200 4.3027 6.9646 9.9248 22.3271 31.5991 3 0.9785 1.2498 1.6377 2.3534 3.1824 4.5407 5.8409 10.2145 12.9240 4 0.9410 1.1896 1.5332 2.1318 2.7764 3.7469 4.6041 7.1732 8.6103 5 0.9195 1.1558 1.4759 2.0150 2.5706 3.3649 4.0321 5.8934 6.8688 6 0.9057 1.1342 1.4398 1.9432 2.4469 3.1427 3.7074 5.2076 5.9588 7 0.8960 1.1192 1.4149 1.8946 2.3646 2.9980 3.4995 4.7853 5.4079 8 0.8889 1.1081 1.3968 1.8595 2.3060 2.8965 3.3554 4.5008 5.0413 9 0.8834 1.0997 1.3830 1.8331 2.2622 2.8214 3.2498 4.2968 4.7809 10 0.8791 1.0931 1.3722 1.8125 2.2281 2.7638 3.1693 4.1437 4.5869 11 0.8755 1.0877 1.3634 1.7959 2.2010 2.7181 3.1058 4.0247 4.4370 12 0.8726 1.0832 1.3562 1.7823 2.1788 2.6810 3.0545 3.9296 4.3178 13 0.8702 1.0795 1.3502 1.7709 2.1604 2.6503 3.0123 3.8520 4.2208 14 0.8681 1.0763 1.3450 1.7613 2.1448 2.6245 2.9768 3.7874 4.1405 15 0.8662 1.0735 1.3406 1.7531 2.1314 2.6025 2.9467 3.7328 4.0728 16 0.8647 1.0711 1.3368 1.7459 2.1199 2.5835 2.9208 3.6862 4.0150 17 0.8633 1.0690 1.3334 1.7396 2.1098 2.5669 2.8982 3.6458 3.9651 18 0.8620 1.0672 1.3304 1.7341 2.1009 2.5524 2.8784 3.6105 3.9216 19 0.8610 1.0655 1.3277 1.7291 2.0930 2.5395 2.8609 3.5794 3.8834 20 0.8600 1.0640 1.3253 1.7247 2.0860 2.5280 2.8453 3.5518 3.8495 21 0.8591 1.0627 1.3232 1.7207 2.0796 2.5176 2.8314 3.5272 3.8193 22 0.8583 1.0614 1.3212 1.7171 2.0739 2.5083 2.8188 3.5050 3.7921 23 0.8575 1.0603 1.3195 1.7139 2.0687 2.4999 2.8073 3.4850 3.7676 24 0.8569 1.0593 1.3178 1.7109 2.0639 2.4922 2.7969 3.4668 3.7454 25 0.8562 1.0584 1.3163 1.7081 2.0595 2.4851 2.7874 3.4502 3.7251 26 0.8557 1.0575 1.3150 1.7056 2.0555 2.4786 2.7787 3.4350 3.7066 27 0.8551 1.0567 1.3137 1.7033 2.0518 2.4727 2.7707 3.4210 3.6896 28 0.8546 1.0560 1.3125 1.7011 2.0484 2.4671 2.7633 3.4082 3.6739 29 0.8542 1.0553 1.3114 1.6991 2.0452 2.4620 2.7564 3.3962 3.6594 30 0.8538 1.0547 1.3104 1.6973 2.0423 2.4573 2.7500 3.3852 3.6460 40 0.8507 1.0500 1.3031 1.6839 2.0211 2.4233 2.7045 3.3069 3.5510 60 0.8477 1.0455 1.2958 1.6706 2.0003 2.3901 2.6603 3.2317 3.4602 80 0.8461 1.0432 1.2922 1.6641 1.9901 2.3739 2.6387 3.1953 3.4163 100 0.8452 1.0418 1.2901 1.6602 1.9840 2.3642 2.6259 3.1737 3.3905 1000 0.8420 1.0370 1.2824 1.6464 1.9623 2.3301 2.5808 3.0984 3.3003 z 0.8416 1.0364 1.2816 1.6449 1.9600 2.3263 2.5758 3.0902 3.2905 Example 7.3 Using the \\(t\\)-table Find \\(\\mathbb{P}(T &lt; 0.92)\\) if \\(T\\) has \\(5\\) \\(\\mathrm{df}\\). Solution: Look at the \\(5\\)th row and first column, the cell is approximately \\(0.92\\). The column is \\(t_{0.80}\\) so \\(q = 0.8\\) and \\(\\mathbb{P}(T &lt; 0.92) = 0.8\\). Find \\(c\\) so that \\(\\mathbb{P}(T &lt; c) = 0.975\\). Solution: Look at the \\(8\\)th row and column \\(t_{0.975}\\), we find \\(c = 2.3060\\). Find \\(c\\) so that \\(\\mathbb{P}(-c &lt; T &lt; c) = 0.95\\). Solution: \\(1-\\alpha = 0.95\\), so \\(\\alpha = 0.05\\) and \\(1-\\alpha/2 = 0.975\\). So \\(c = t_{0.975,8} = 2.3060\\). Example 7.4 A random sample of \\(20\\) purchases at an Internet music site a mean purchase amount of \\(\\$45.26\\), and a standard deviation of \\(\\$20.67\\). Construct a \\(90\\%\\) confidence interval for the mean purchases of all customers. How would the confidence interval change if you had assumed that the population standard deviation \\(\\sigma\\) is known to be \\(\\$20\\). Solution: In this question: \\(n = 20\\), \\(\\bar{x} = 45.26\\) and \\(s = 20.67\\) Since we want a \\(90\\%\\) CI, \\(\\alpha = 1-0.9=0.1\\) and we need to look for \\(t_{1-\\alpha/2,n-1} = t_{0.95,19} = 1.7291\\). So a \\(90\\%\\) CI for \\(\\mu\\) is \\[\\bar{x} \\pm t_{1-\\alpha/2,n-1} \\frac{s}{\\sqrt{n}} = 45.26 \\pm 1.7291\\times\\frac{20.67}{\\sqrt{20}} = (37.27, 53.25)\\] If we assume that \\(\\sigma=20\\) then we would use it instead of \\(s\\). In addition, our critical value would come from the standard normal table, where \\(z_{1-\\alpha/2}=1.645\\). Hence collectively this would cause our interval in a) to get narrower. You can also conveniently use R to find the quantiles and probabilities of the \\(t\\)-distribution using functions qt() and pt(). It is in the similar fashion with the normal distribution. For example qt(0.7, df = 13) ## [1] 0.5375041 means that \\(P(X &lt; 0.538) = 0.7\\) where \\(X\\sim t(13)\\). Another example is pt(3, df = 10) ## [1] 0.9933282 means that \\(P(X &lt; 3) = 0.9933\\) where \\(X\\sim t(10)\\). Notes: When the degree of freedom is greater than 30, i.e., we can not look for the exact value in the \\(t\\)-table, we choose the closest degree of freedom to ours. If the degree of freedom is greater than 1000, we can just use the \\(Z\\)-table instead (or the last row of the \\(t\\)-table). In this section, we have assumed that the data is normally distributed. However, even when the data is not normal, by approximation, you can still use the \\(t\\)-table if either \\(n &lt; 15\\) and the data is close to normal (symmetric, unimodal, no outliers) or \\(n \\ge 15\\), except if the data distribution is strongly skewed or if it has outliers or \\(n \\ge 40\\): any data distribution is fine. 7.4.2 Determine the Sample Size In the \\(t\\)-distribution case, we cannot use the same analogy as in the case where \\(\\sigma\\) is known, i.e., we cannot use \\[n \\ge \\left(\\frac{t_{1-\\alpha/2,n-1}\\times s}{m}\\right)^2\\] because we need to know \\(n\\) because the degree of freedom is required for \\(t_{1-\\alpha/2,n-1}\\). Instead, we will always calculate the sample size using the \\(Z\\)-table and the formula \\[n \\ge \\left(\\frac{z_{1-\\alpha/2}\\times s}{m}\\right)^2\\] where \\(s\\) can be an guess of \\(\\sigma\\) or a known \\(s\\) from previous data. 7.5 Summary For normal population or for \\(n\\) large: Cases Pivotal Quantity \\((1-\\alpha)100\\%\\) CI Minimum \\(n\\) so CI margin not exceed \\(m\\) \\(\\sigma\\) known \\(\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}}\\) \\(\\bar{x}_n \\pm z_{1-\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\) \\(n \\ge \\left(\\frac{z_{1-\\alpha/2}\\times \\sigma}{m}\\right)^2\\) \\(\\sigma\\) unknown \\(\\frac{\\bar{X}_n - \\mu}{s/\\sqrt{n}}\\) \\(\\bar{x}_n \\pm t_{1-\\alpha/2, n-1}\\frac{s}{\\sqrt{n}}\\) \\(n \\ge \\left(\\frac{z_{1-\\alpha/2}\\times s}{m}\\right)^2\\) This is to differentiate between central \\(t\\)-distribution and non-central \\(t\\)-distribution. From now on, we will call it \\(t\\)-distribution for short because we will only look at central \\(t\\)-distributions in this book.↩︎ "],["chap-hyp.html", "8 Hypothesis Tests 8.1 Hypotheses 8.2 Hypothesis Test 8.3 Error of a Test 8.4 One-sample Hypothesis Tests of the Mean", " 8 Hypothesis Tests 8.1 Hypotheses 8.1.1 Research Questions Recall in Chapter 1, we discussed that a data analysis starts with a research questions. Hypothesis tests help us answer questions regarding the values or the range of values of population parameters. Example 8.1 Examples of questions that can be answered using hypothesis tests Is the mean highway fuel consumption of a new car model different from what the manufacturer claims? Has the average age of online consumers increased over the last few years? On average does a bottle filling machine under-fill bottles? We can see that the all the above questions are related to the values of population mean \\(\\mu\\), but for different random variables. 8.1.2 Hypotheses Once we have a research question in mind, we need to form hypotheses. In hypothesis testing, we have two types of hypotheses: the null hypotheses: the statement/claim to be tested/challenged Typically, a null hypothesis is a statement of “no effect” or “no difference” We usually want to find evidence against the null hypothesis. Notation: \\(H_0\\) the alternative hypotheses: the statement/claim against the null hypothesis Evidence supporting the alternative hypothesis is evidence against the null hypothesis. Notation: \\(H_1\\) or \\(H_A\\). In this chapter, I will use \\(H_0\\) for null hypothesis and \\(H_1\\) for alternative hypothesis. Example 8.2 Continue with Example 8.1 Is the mean highway fuel consumption of a new car model different from what the manufacturer claims? \\(H_0\\): mean highway fuel consumption of the new car model is the same as what the manufacturer claims. \\(H_1\\): mean highway fuel consumption of the new car model is different from what the manufacturer claims. Has the average age of online consumers increased over the last few years? \\(H_0\\): The average age of online consumers this year is the same with the average age of online consumers a few years ago. \\(H_1\\): The average age of online consumers this year is more than with the average age of online consumers a few years ago. On average does a bottle filling machine under-fill bottles? \\(H_0\\): On average, the bottle filling machine fill the bottles at the exact required amount. \\(H_1\\): On average, the bottle filling machine fill the bottles less than the required amount. 8.1.3 Example: Hypothesis About A Specific Value of \\(\\mu\\) Suppose we are interested in the unknown population mean \\(\\mu\\) and we want to challenge the claim that \\(\\mu\\) is in fact \\(\\mu_0\\) (\\(\\mu_0\\) is called the hypothesized value of \\(\\mu\\)). Then our null hypothesis is \\[H_0: \\mu = \\mu_0\\] The alternative hypothesis can be one of the three possibilities \\(H_1: \\mu \\ne \\mu_0\\). The combination of \\(H_0\\) and this \\(H_1\\) is called a two-sided test, meaning \\(H_1\\) is on both sides of \\(\\mu_0\\). \\(H_1: \\mu &gt; \\mu_0\\). This is called a one-sided upper-tail test, meaning \\(H_1\\) is on the upper side of \\(\\mu_0\\). \\(H_1: \\mu &lt; \\mu_0\\). This is called a one-sided lower-tail test, meaning \\(H_1\\) is on the lower side of \\(\\mu_0\\). Example 8.3 In Example 8.2: Suppose the mean highway fuel consumption claimed of the new car model is \\(30\\) miles per gallon (mpg). Let \\(\\mu\\): the actual mean highway fuel consumption of the new model. Then our hypotheses are \\[H_0: \\mu = 30 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1: \\mu \\ne 30\\] Suppose the average age of online consumers a few years ago was \\(23.3\\). Let \\(\\mu\\) be the mean age of online consumers nowadays. Then we have \\[H_0: \\mu = 23.3 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1: \\mu &gt; 23.3\\] Suppose the required amount of cola that the machine should fill is \\(355\\) ml. Let \\(\\mu\\) be the actual mean amount of colar fill be the filling machine. Then our hypotheses are \\[H_0: \\mu = 355 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1: \\mu &lt; 355\\] Exercise 8.1 Which one of the above is a two-sided test? a one-sided test? 8.2 Hypothesis Test A hypothesis test is a decision rule to answer the research questions. In statistics, we usually establish decision rules based on an event \\(R\\) which is in favor of \\(H_1\\) and against \\(H_0\\). If event \\(R\\) actually happens in our obtained data, we reject \\(H_0\\). The fact that event \\(R\\) happens is an evidence supporting \\(H_1\\) and against \\(H_0\\). If event \\(R\\) does not happen, then we say we do not reject \\(H_0\\). We do not say: “we accept \\(H_0\\)” because we never know what is the truth in the population, if we only based our decision on the sample data. But because we do not have good evidence against \\(H_0\\), we say that we do not reject \\(H_0\\). Example 8.4 In Example 8.3 The fuel consumption case: Suppose we sample \\(n\\) cars from that new car model and calculate the sample mean highway fuel consumptions from those cars \\(\\bar{X}_n\\) (\\(\\bar{X}_n\\) is random). An evidence supporting \\(H_1: \\mu \\ne 30\\) will be that the sample mean we obtain from data will be either a lot greater than \\(30\\) or a lot smaller than \\(30\\). Then the event \\(R\\) can be, for example, \\(|\\bar{X}_n| &gt; 40\\). We reject \\(H_0\\) if the actual data we collect \\(\\bar{x}_n\\) has the absolute value that is greater than 40 and we do not reject \\(H_0\\) otherwise. The online consumers case: Suppose we survey \\(n\\) online consumers about their age and calculate their mean age \\(\\bar{X}_n\\). An evidence supporting \\(H_1: \\mu &gt; 23.3\\) will be that the sample mean is a lot greater than 23.3. For example, we can let \\(R\\) be the event that \\(\\bar{X}_n &gt; 30\\). If the mean age of people in the survey we conduct \\(\\bar{x}_n\\) is greater than 30, we reject \\(H_0\\). We do not reject \\(H_0\\) otherwise. Exercise 8.2 Give a possible event \\(R\\) for the bottle filling case in Example 8.3. Notes: \\(H_0, H_1\\), \\(R\\) define a hypothesis test. \\(R\\) is called the critical event of the test, i.e., when this event is observed, we reject the null hypothesis \\(H_0\\). 8.3 Error of a Test We cannot just define whatever critical event \\(R\\) we want, that would not be called a statistical test! We have to take into account the fact that \\(\\bar{X}_n\\) can vary according to the sample. Because of that, any test rule can never be correct all the time. To conduct the test and make conclusions with confidence, we need to control the error of the test. A hypothesis test can make two types of possible error: Type I error: When we reject \\(H_0\\) while in fact \\(H_0\\) is true Type II error: When we do not reject \\(H_0\\) while in fact \\(H_1\\) is true \\(H_0\\) is true \\(H_1\\) is true Reject \\(H_0\\) Type I error Correct Do not reject \\(H_0\\) Correct Type II error The probability that Type I error happens is called the significance level or the size of the test. Notation: \\(\\alpha\\) \\[\\alpha = \\mathbb{P}(\\text{Reject }H_0|H_0\\text{ is true})\\] Type II error happens is denoted as \\[\\beta = \\mathbb{P}(\\text{Do not reject }H_0|H_1\\text{ is true})\\] Then \\(1-\\beta\\) is called the power of the test. \\[1-\\beta = \\mathbb{P}(\\text{Reject }H_0\\text{ correctly}|H_1\\text{ is true})\\] In Hypothesis Testing, it is most common19 that we want to control for Type I error, i.e., we want Type I error of the test to be controlled at a specified significance level \\(\\alpha\\). Most popularly, we specify \\(\\alpha = 5\\% = 0.05\\). Recall in Chapter 7, the confidence level is defined as \\(1-\\alpha\\). By specifying the significance level \\(\\alpha\\), we can derive the test we want, i.e., we can find an appropriate critical event \\(R\\) such that Type I error is controlled at \\(\\alpha\\). Similar to confidence level, we will also use pivotal quantities to construct the tests. 8.4 One-sample Hypothesis Tests of the Mean 8.4.1 Testing \\(H_0: \\mu = \\mu_0\\) When \\(\\sigma\\) is Known Recall from Section 7.1, if either \\(X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)\\), or \\(X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} (\\mu, \\sigma^2)\\) for \\(n &gt; 30\\) and \\(\\sigma\\) is known, then we have the pivotal quantity \\[\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal{N}(0,1)\\] Because the distribution of the pivotal quantity above is known, we can construct the critical event based on this, calculate and hence control for type I error. Let us re-state our problem: Suppose that I have the data \\(X_1, ..., X_n\\) coming from either a normal distribution of mean \\(\\mu\\) and variance \\(\\sigma^2\\), or an arbitrary distribution of mean \\(\\mu\\) and variance \\(\\sigma^2\\) but \\(n\\) is large Further suppose that I know the population variance \\(\\sigma^2\\), I want to conduct a hypothesis test on the population mean \\(\\mu\\). 8.4.1.1 Two-sided Test To begin with, a two-sided test for \\(\\mu\\) is of the form \\[H_0: \\mu = \\mu_0 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1: \\mu \\ne \\mu_0\\] where \\(\\mu_0\\) is some value that \\(\\mu\\) is believed to be, but we want to challenge this belief. To construct a statistical hypothesis test in this case, we need to define a critical event \\(R\\) so that if \\(R\\) actually happens, that is an evidence against \\(H_0\\) and supporting \\(H_1\\). Type I error of the test is controlled at a significance level \\(\\alpha\\), i.e., \\(\\mathbb{P}(R|H_0) = \\alpha\\). First, because the test is two-sided, a supporting evidence for \\(H_1\\) is that \\(\\bar{X}_n\\) is either much bigger or much smaller compared to \\(\\mu_0\\). This is equivalent to the event that \\(\\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}}\\) is very far from 0. Therefore, a critical event will be of the form \\[\\Big|\\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}} \\Big| &gt; c,\\] where \\(c\\) is called the critical value. Second, I want to control Type I error of the test at a significance level \\(\\alpha\\), that is \\[\\begin{align*} \\alpha &amp; = \\mathbb{P}(R|H_0) \\\\ &amp; = \\mathbb{P}\\left(|\\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}}| &gt; c | \\mu = \\mu_0\\right) \\\\ &amp; = \\mathbb{P}\\left(\\Big|\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}}\\Big| &gt; c\\right) \\\\ &amp; = \\mathbb{P}(|Z| &gt; c) \\\\ &amp; = 2\\times \\mathbb{P}(Z &lt; -c) \\\\ \\frac{\\alpha}{2} &amp; = \\mathbb{P}(Z &lt; -c) \\end{align*}\\] This means that \\(-c\\) is the \\(\\frac{\\alpha}{2}\\)-quantile of the standard normal distribution \\(Z\\), i.e., \\(-c = z_{\\alpha/2}\\). This is equivalent to \\(c = z_{1-\\alpha/2}\\) (From Chapter 5). Notes: \\(T = \\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}}\\) is called the test statistic, i.e., a statistic that has a known distribution under the null hypothesis. Here, you can see that if \\(H_0: \\mu = \\mu_0\\) is in fact true, then the distribution of \\(T\\) is known \\[T = \\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}} \\hspace{3mm} \\overset{H_0: \\mu = \\mu_0}{\\sim} \\hspace{3mm} \\mathcal{N}(0,1)\\] The observed test statistic is \\[t = \\frac{\\bar{x}_n - \\mu_0}{\\sigma/\\sqrt{n}}\\] We reject \\(H_0\\) if event \\(R\\) happens, i.e., the observed test statistic actually exceed the critical value: \\(|t| &gt; c = z_{1-\\alpha/2}\\). Example 8.5 The Classic Bottling Company has just installed a new bottling procedure that will fill \\(355\\) ml cans of its Classic Cola soft drink. Under-filling leads to customer complaints, and over-filling costs the company considerable money. The bottling company wants to set up a hypothesis to test if on average the machine fills an amount different from \\(355\\)ml. To test this hypothesis the company drew a random sample of \\(36\\) filled cans and found that the sample average was \\(354.96\\). Suppose that the standard deviation for the population is \\(\\sigma = 0.1\\). Conduct a statistic test at \\(5\\%\\) level of significance. Solution: Step 1: Hypotheses: \\(H_0: \\mu = 355\\) vs. \\(H_1: \\mu \\ne 355\\). Step 2: Calculate the test statistic \\[t = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{354.96-355}{0.1/\\sqrt{36}} = -2.4\\] Step 3: Because we have a two-sided test at \\(\\alpha = 5\\% = 0.05\\) level of significance, our critical value is \\(c = z_{1-\\alpha/2} = z_{0.975} = 1.96\\). Step 4: Since \\(|t| = 2.4 &gt; z_{1-\\alpha/2} = z_{0.975} = 1.96\\), we reject the null hypothesis \\(H_0\\) at 5% level of significance. We conclude that there is sufficient evidence in our data to support the claim that the machine fills an amount different from the claimed amount of \\(355\\)ml. 8.4.1.2 \\(p\\)-value Another popular way to conduct the test is to calculate the probability \\[p = \\mathbb{P}(|T| &gt; |t| \\Big| H_0)\\] We call this the \\(p\\)-value, i.e., the probability of getting a test statistic that is at least as extreme as the one observed, assuming the null hypothesis is true. We know that \\(\\mathbb{P}(|T| &gt; z_{1-\\alpha/2} \\Big| \\mu = \\mu_0) = \\alpha\\) and we will reject \\(H_0\\) if \\(|t| &gt; z_{1-\\alpha/2}\\) However, for any \\(0 &lt; a &lt; b\\) and any random variable \\(X\\), then \\(\\mathbb{P}(|X| &gt; a) &gt; \\mathbb{P}(|X| &gt; b)\\). So the test is equivalent to rejecting \\(H_0\\) if \\(p &lt; \\alpha\\) and do not reject \\(H_0\\) otherwise. In the two-sided test, \\[p = \\mathbb{P}(|T| &gt; |t|\\Big|\\mu = \\mu_0) = \\mathbb{P}(|Z| &gt; |t|)\\] For majority of the tests (not only the test about the population mean \\(\\mu\\)) run by statistical software, the \\(p\\)-value will be printed and we will always reject the null hypothesis at \\(\\alpha\\) level of significance if \\(p &lt; \\alpha\\). The following figure illustrate the two-sided test. Figure 8.1: Regions of rejecting \\(H_0: \\mu = \\mu_0\\) when \\(H_1: \\mu \\ne \\mu_0\\). Example 8.6 Conduct the test in Example 8.5 using the \\(p\\)-value method. Solution: Step 1: Hypotheses: \\(H_0: \\mu = 355\\) vs. \\(H_1: \\mu \\ne 355\\). Step 2: Calculate the test statistic \\[t = \\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{354.96-355}{0.1/\\sqrt{36}} = -2.4\\] Step 3: The \\(p\\)-value is \\[p = \\mathbb{P}(|Z| &gt; |t|) = \\mathbb{P}(|Z| &gt; 2.4) = 2\\times \\mathbb{P}(Z &lt; -2.4) = 0.0164\\] Step 4: Since \\(p = 0.0164 &lt; \\alpha = 0.05\\), we reject the null hypothesis \\(H_0\\) at 5% level of significance. We conclude that there is sufficient evidence in our data to support the claim that the machine fills an amount different from the claimed amount of \\(355\\)ml. 8.4.1.3 Lower-tail One-sided Test Suppose we are interested in testing \\[H_0: \\mu = \\mu_0 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1:\\mu&lt;\\mu_0\\] The critical event that supports \\(H_1\\) would be that the sample mean \\(\\bar{X}_n\\) is much less than \\(\\mu_0\\), i.e., \\(\\frac{\\bar{X}_n-\\mu_0}{\\sigma/\\sqrt{n}}\\) is much less than 0. So, the critical event should be of the form \\[T = \\frac{\\bar{X}_n-\\mu_0}{\\sigma/\\sqrt{n}} &lt; c\\] To control the Type I error at \\(\\alpha\\), we have \\[\\begin{align*} \\alpha &amp; = \\mathbb{P}\\left(\\frac{\\bar{X}_n-\\mu_0}{\\sigma/\\sqrt{n}} &lt; c\\Big|\\mu = \\mu_0\\right) \\\\ &amp; = \\mathbb{P}\\left(\\frac{\\bar{X}_n-\\mu}{\\sigma/\\sqrt{n}} &lt; c\\right) \\\\ &amp; = \\mathbb{P}(Z &lt; c) \\end{align*}\\] so \\(c = z_{\\alpha}\\). Similar to the two-sided test, we will use either the critical value or the \\(p\\)-value to conduct the test. In particular, we will reject \\(H_0\\) if either the observed test statistic is smaller than the critical value \\[t = \\frac{\\bar{x}_n-\\mu_0}{\\sigma/\\sqrt{n}} &lt; z_{\\alpha}\\] the \\(p\\)-value is less than \\(\\alpha\\): \\[p = \\mathbb{P}(T &lt; t \\Big| \\mu = \\mu_0) = \\mathbb{P}(Z &lt; t) &lt; \\alpha\\] The following figure illustrate the lower-tail one-sided test. Figure 8.2: Regions of rejecting \\(H_0: \\mu = \\mu_0\\) when \\(H_1: \\mu &lt; \\mu_0\\). Example 8.7 Suppose in Example 8.5, we want to test a lower-tail one-sided test instead of a two-sided test. What will be our test result? Solution: Step 1: We want to know if the bottling machine is under filling the bottles, so my hypotheses are \\(H_0: \\mu = 355\\) vs. \\(H_1: \\mu &lt; 355\\). Step 2: The test statistic \\(t\\) is still the same: \\(t = -2.4\\). Step 3: Our critical value is \\(c = z_{\\alpha} = z_{0.05} = -1.645\\). Step 4: Because \\(-2.4 = t &lt; c = z_{0.05} = -1.645\\), we reject the null hypothesis \\(H_0\\) at \\(5\\)% level of significance and conclude that there is sufficient evidence that the machine fills an amount that is less than the claimed amount of \\(355\\)ml. 8.4.1.4 Upper-tail One-sided Test The upper-tail one-sided test is of the form \\[H_0: \\mu = \\mu_0 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_1:\\mu &gt; \\mu_0\\] Similar to the lower-tail case, the test statistic is \\[T = \\frac{\\bar{X}_n - \\mu_0}{\\sigma/\\sqrt{n}}.\\] But we need to reject \\(H_0\\) if \\(t &gt; z_{1-\\alpha}\\) or if \\(p = \\mathbb{P}(Z &gt; t) &lt; \\alpha\\). The following figure illustrate the upper-tail one-sided test. Figure 8.3: Regions of rejecting \\(H_0: \\mu = \\mu_0\\) when \\(H_1: \\mu &gt; \\mu_0\\). Example 8.8 Step 1: Suppose we are testing \\(H_0: \\mu = 355\\) vs. \\(H_1: \\mu &gt; 355\\) in Example 8.5. Step 2: The test statistic \\(t\\) is the same. Step 3: The \\(p\\)-value for this test is \\[\\mathbb{P}(Z &gt; t) = \\mathbb{P}(Z &gt; -2.4) = 1 - \\mathbb{P}(Z &lt; -2.4) = 0.9918\\] Step 4: Because \\(p = 0.9918 &gt; \\alpha = 0.05\\), we do not reject the null hypothesis \\(H_0\\) at \\(5\\)% significance level and we conclude that there is not sufficient evidence to claim that the machine fills an amount that is greater than \\(355\\)ml. 8.4.1.5 One-sided or Two-sided Test? We can see that for different choices of \\(H_1\\), the results of our test can be very different. It is very important to note that, we need to state the the hypotheses before we see the data, not after we see the data. For example, suppose the sample mean of our collected data happen to be less than our hypothesized \\(\\mu_0\\) value, and after that, we decide for a one-sided test \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu &lt; \\mu_0\\). In this case our Type I error is more than \\(\\alpha\\) because there are some chance that even if in fact \\(\\mu &gt; \\mu_0\\), we still obtain \\(\\bar{x}_n &lt; \\mu_0\\), which leads us to choose to test the lower-tail one-sided test. But we are not accounting for such an uncertainty when conducting \\(H_0: \\mu = \\mu_0\\) vs. \\(H_1: \\mu &lt; \\mu_0\\). Now, our Type I error will be actually more than what we claim. And that is a scientific fraud! 8.4.1.6 Summary of Hypothesis Tests for the Mean When \\(\\sigma\\) is Known Test: For a significance level of \\(\\alpha\\): \\(H_0\\) \\(H_1\\) Test statistic \\(t\\) Critical value \\(c\\) Reject \\(H_0\\) \\(p\\)-value Illustration \\(\\mu \\ne \\mu_0\\) \\(z_{1-\\alpha/2}\\) \\(|t| &gt; c\\) \\(\\mathbb{P}(|Z| &gt; |t|)\\) \\(\\mu = \\mu_0\\) \\(\\mu &gt; \\mu_0\\) \\(t = \\frac{\\bar{x}_n-\\mu_0}{\\sigma/\\sqrt{n}}\\) \\(z_{1-\\alpha}\\) \\(t &gt; c\\) \\(\\mathbb{P}(Z &gt; t)\\) \\(\\mu &lt; \\mu_0\\) \\(z_{\\alpha}\\) \\(t &lt; c\\) \\(\\mathbb{P}(Z &lt; t)\\)} Conclusion statements: Reject \\(H_0\\) at \\(\\alpha\\) level of significance. Conclude: There is sufficient evidence in our data that …. Do not reject \\(H_0\\) at \\(\\alpha\\) level of significance. Conclude: There is not sufficient evidence in our data that …. Type I and type II error: Figure 8.4: Type I and Type II error20 Notes: The above summaries about the conclusion statements and types of errors are applicable for most hypothesis tests, not only the tests for \\(H_0: \\mu = \\mu_0\\) when \\(\\sigma\\) is known. 8.4.2 Testing \\(H_0: \\mu = \\mu_0\\) when \\(\\sigma\\) is unknown For \\(X_1, X_2, ..., X_n \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) and \\(\\sigma\\) is unknown, we use the following pivotal quantity \\[\\frac{\\bar{X_n} - \\mu}{S/\\sqrt{n}} \\sim t(n-1)\\] When \\(H_0: \\mu = \\mu_0\\) is true, we have the test statistic \\[T = \\frac{\\bar{X_n} - \\mu_0}{S/\\sqrt{n}} \\hspace{3mm} \\overset{H_0: \\mu = \\mu_0}{\\sim} \\hspace{3mm} t(n-1)\\] whose distribution is known under \\(H_0: \\mu = \\mu_0\\). Now, the test for \\(H_0: \\mu = \\mu_0\\) is very similar to the case where \\(\\sigma\\) is known. We just need to replace \\(\\sigma\\) by \\(s\\) and \\(z\\) by \\(t(n-1)\\) in the summary table in Section 8.4.1.6 above to obtain the test table for the case where \\(\\sigma\\) is unknown. Summary: Suppose \\(Y\\) is a random variable that follows \\(t(n-1)\\) distribution \\(H_0\\) \\(H_1\\) Test statistic \\(t\\) Critical value \\(c\\) Reject \\(H_0\\) \\(p\\)-value \\(\\mu \\ne \\mu_0\\) \\(t_{1-\\alpha/2, n-1}\\) \\(|t| &gt; c\\) \\(\\mathbb{P}(|Y| &gt; |t|)\\) \\(\\mu = \\mu_0\\) \\(\\mu &gt; \\mu_0\\) \\(t = \\frac{\\bar{x}_n - \\mu_0}{s/\\sqrt{n}}\\) \\(t_{1-\\alpha,n-1}\\) \\(t &gt; c\\) \\(\\mathbb{P}(Y &gt; t)\\) \\(\\mu &lt; \\mu_0\\) \\(t_{\\alpha,n-1}\\) \\(t &lt; c\\) \\(\\mathbb{P}(Y &lt; t)\\) Example 8.9 The average age of online consumers a few years before \\(2007\\) was \\(23.3\\) years. As older individuals gain confidence with the Internet, it is believed that the average age has increased. A random sample of \\(40\\) individuals who made an online purchase during \\(2007\\) was drawn and it was found that their average age was \\(24.2\\) years and the standard deviation in the sample was \\(5.3\\) years. Test the hypothesis at a \\(1\\)% level of significance. Solution: Step 1: Let \\(\\mu\\) be the population mean age of online consumers in \\(2007\\). Hypotheses: \\[H_0: \\mu = 23.3 \\hspace{5mm} \\text{vs.} \\hspace{5mm} H_1: \\mu &gt; 23.3\\] Step 2: The test statistic is \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{24.2-23.3}{5.3/\\sqrt{40}} = 1.074\\] Step 3: \\(\\alpha = 0.01\\), and the test is a upper-tail one-sided test. We need to use the critical value \\(c = t_{1-\\alpha, n-1} = t_{0.99, 39} = 2.4233\\) from the \\(t\\) table. (We use \\(df = 40\\) because it is closest to \\(39\\)) Step 3: Because \\(t = 1.074 &lt; c = t_{1-\\alpha, n-1} = 2.423\\) and this is a upper-tail test, we do not reject the null hypothesis and conclude that there is not sufficient evidence to suggest that the average age has increased. Exercise 8.3 What will the result change if the test is two sided? one-sided lower-tail? Notes: NEVER decide the hypotheses after seeing the data. Hypotheses must be decided before you collect the data. If you do not know which direction from \\(\\mu_0\\) that the true \\(\\mu\\) lies, always choose the two-sided test. When you conduct tests using computer software, they will always output the \\(p\\)-values. For all kinds of hypothesis tests, you reject the null hypothesis if \\(p &lt; \\alpha\\) where \\(\\alpha\\) is the significance level you decide. Caveats: The \\(p\\)-value will change whether you use a two-sided test or a on-sided one, so when use software, make sure you set the correct test you want. Convention in scientific research: \\(\\alpha = 0.05\\). It is very useful to draw a some graphics like Figure 7.1-3 to have a sense of which kind of critical value or rejection criteria to use. When doing so, remember that you are trying to find evidence to support the alternative hypothesis. This will always help you navigate to the correct answer. one-sided test: look at one of the two tails of the distribution. two-sided test: look at both tails of the distribution. We have learned the two most important concepts in inferential statistics: confidence intervals and hypothesis testing. Both uses pivotal quantity to construct the intervals or the test. CI give us the range of possible values of the population parameter, while hypothesis testing helps us evaluate some claims about the population parameter. Usually, in a data analysis, although it is possible to choose freely, by convention, people choose confidence level = 1 - significance level. 8.4.3 Some More Examples Example 8.10 A pet food manufacturer suspects that one of its machines is not filling the amount required for bags of dog food that are supposed to contain \\(20\\)kg of food. They want to test the null hypothesis that the mean fill is \\(20\\)kg against the alternative hypothesis that it is different from \\(20\\)kg. They sample \\(100\\) randomly selected bags and find a sample mean of \\(20.1\\) kg. Assume that the population standard deviation is known to be \\(0.5\\)kg. Test their hypothesis at a \\(10\\)% level of significance. Solution: Information \\(\\mu_0 = 20\\) \\(\\bar{x} = 20.1\\) \\(n = 100\\) \\(\\sigma = 0.5\\) \\(\\alpha = 0.1\\) Step 1: The \\(\\mu\\) be the mean weight of food that the machine fills for each bag of dog food. In this question, \\(\\sigma\\) is known and we need to use a two-sided test. Hypotheses: \\[H_0: \\mu = 20 \\hspace{5mm} \\text{vs.} \\hspace{5mm} H_1: \\mu \\ne 20\\] Step 2: The test statistic is \\[t = \\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}} = \\frac{20.1-20}{0.5/\\sqrt{100}} = 2\\] Step 3: \\(\\alpha = 0.1\\) and the test is two-sided test for \\(\\sigma\\) is known. So the critical value is \\[c = z_{1-\\alpha/2} = z_{0.95} = 1.645\\] Step 4: Because \\(|t| = 2 &gt; c = 1.645\\) and the test is two-sided, we reject the null hypothesis at \\(10\\%\\) level of significance. We conclude that there is sufficient evidence in the data that the machine is not filling the required amount of dog food. Example 8.11 A study by the Center for Science in the Public Interest found that movie theatre popcorn is often very high in calories and saturated fat. The CSPI found that on top of the poor stated nutritional characteristics, in reality the popcorn had much higher calorie and fat content than what was claimed. Suppose you wish to investigate the calorie content of large bags of popcorn at your local theatre, where the theatre chain claims that a large untopped bag of their popcorn contains \\(920\\) calories on average. You purchase \\(14\\) bags of this type of popcorn, and suppose that it is reasonable to think that these \\(14\\) bags represent a random sample from the population of bags of this type. You have these bags of popcorn analyzed and find that the mean calorie content is \\(1082\\) and the standard deviation is \\(60\\). Suppose we wish to carry out a test of the null hypothesis that the population mean calorie content is what the company claims it to be. If we feel a one-sided alternative hypothesis is most appropriate in this situation, what are the appropriate hypotheses? Which procedure (\\(z\\) or \\(t\\)) is appropriate here? Why? What is the value of the appropriate test statistic? What is the critical value for the test statistic at 5% level of significance? Give an appropriate conclusion of the results of the analysis. Solution: Let \\(\\mu\\) be the mean calories of movie theatre popcorn. The hypothesis is \\(H_0: \\mu = 920\\) versus \\(H_0: \\mu &gt; 920\\). Because we collect the standard deviation from the sample, \\(\\sigma\\) is unknown and we need to use the \\(t\\)-table. The test statistic is \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{1082-920}{60/\\sqrt{14}} = 10.1025\\] \\(\\alpha = 0.05\\) and we want to conduct a one-sided upper-tail test where \\(\\sigma\\) is unknown, our critical value need to be \\[c = t_{1-\\alpha, n-1} = t_{0.95, 13} = 1.7709\\] Because \\(t &gt; c\\) and the test is one-sided upper-tail test, we reject the null hypothesis at \\(5\\%\\) level of significance. We conclude that there is sufficient evidence in the data that the average calories is greater than what is claimed. There are other methods or philosophy about constructing a test, such as Bayesian tests, but we will not talk about it in this book.↩︎ Image adapted from https://microbenotes.com/type-i-and-type-ii-error/↩︎ "],["chap-wald.html", "9 Two-sample Tests 9.1 Independent Two-Sample Tests 9.2 Wald-type Tests and Confidence Intervals 9.3 \\(t\\)-test for Paired Data 9.4 \\(t\\)-test for Proportions 9.5 Summary of Wald-type hypothesis tests", " 9 Two-sample Tests In the last chapter, we have studied the basic concepts of hypothesis testing. We have discussed the tests about the value of the mean of a population. What if we want to compare two means of two populations? They are called two-sample tests. They have a similar form to the tests we have seen so far, and they are generally called Wald-type tests. 9.1 Independent Two-Sample Tests 9.1.1 Two-sample Test Suppose we have two population \\(X\\) and \\(Y\\) \\[X\\sim(\\mu_1,\\sigma_1^2) \\hspace{5mm} \\text{and} \\hspace{5mm} Y \\sim(\\mu_2, \\sigma_2^2)\\] and we want to compare \\(\\mu_1\\) and \\(\\mu_2\\). Hence, the null hypothesis is \\[H_0: \\mu_1 = \\mu_2 \\hspace{5mm} \\text{or} \\hspace{5mm} H_0: \\mu_1 - \\mu_2 = 0\\] For the alternative hypotheses, we have three choices: \\(H_1: \\mu_1 \\ne \\mu_2\\) or equivalently \\(H_1: \\mu_1 - \\mu_2 \\ne 0\\) \\(H_1: \\mu_1 &gt; \\mu_2\\) or equivalently \\(H_1: \\mu_1 - \\mu_2 &gt; 0\\) \\(H_1: \\mu_1 &lt; \\mu_2\\) or equivalently \\(H_1: \\mu_1 - \\mu_2 &lt; 0\\) Example 9.1 In the rent example, I want to test whether undergraduate students are paying more than graduate students for rent. Then if I let \\(\\mu_1\\) to be the mean rent that undergraduate students are paying and \\(\\mu_2\\) to be the mean rent that graduate students are paying. Then my hypotheses are \\[H_0: \\mu_1 = \\mu_2 \\hspace{5mm} \\text{vs.} \\hspace{5mm} H_1: \\mu_1 &gt; \\mu_2\\] We may also be interested in testing a specific value of the difference of the two means \\[H_0: \\mu_1 - \\mu_2 = \\mu_0\\] In the above, we were only considering the case \\(\\mu_0 = 0\\), but we can choose any other value \\(\\mu_0\\) for our test. 9.1.2 Distribution of the Difference of Two Normal Means Suppose we have \\(n_1\\) sample data from \\(X\\) \\[X_1, X_2, ..., X_{n_1} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu_1, \\sigma_1^2)\\] and \\(n_2\\) sample data from \\(Y\\) \\[Y_1, Y_2, ..., Y_{n_2} \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu_2, \\sigma_2^2)\\] From the sample data, we can use the sample means \\(\\bar{X}_{n_1}\\) and \\(\\bar{Y}_{n_2}\\) to approximate \\(\\mu_1\\) and \\(\\mu_2\\). An evidence against the null hypothesis \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\) will be that the difference \\(\\bar{X}_{n_1}-\\bar{Y}_{n_2}\\) is very different from \\(\\mu_0\\). Therefore, in order to construct the test, we need to find the distribution of the difference of the two means \\(\\bar{X}_{n_1}-\\bar{Y}_{n_2}\\). Now, suppose that data from \\(X\\) and from \\(Y\\) are sampled independently from each other, i.e., the collection of \\(X_i\\) does not depend on the collection of \\(Y_j\\) and vice versa, for all \\(i = 1, 2, ..., n_1\\) and \\(j = 1, 2, ..., n_2\\). Then, the expectation and variance formula of the sample mean (Chapter 6) and the linearity property of normal distribution (Chapter 5) gives us \\[\\begin{align*} \\bar{X}_{n_1} = \\frac{1}{n_1}\\left(X_1 + ... + X_{n_1}\\right) &amp; \\sim \\mathcal{N}\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1}\\right) \\\\ \\bar{Y}_{n_2} = \\frac{1}{n_2}\\left(Y_1 + ... + Y_{n_1}\\right) &amp; \\sim \\mathcal{N}\\left(\\mu_2, \\frac{\\sigma_2^2}{n_2}\\right) \\\\ \\end{align*}\\] We can further derive the distribution of the difference of the two means by applying again the linearity property of normal distribution on \\(\\bar{X}_{n_1}\\) and \\(\\bar{Y}_{n_2}\\) and obtain \\[\\bar{X}_{n_1} - \\bar{Y}_{n_2} \\sim \\mathcal{N}\\left(\\mu_1 - \\mu_2, \\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}\\right)\\] Now, we can use this knowledge to construct tests for \\(\\mu_1 - \\mu_2\\). 9.1.3 When \\(\\sigma_1\\) and \\(\\sigma_2\\) are Known Similar to the one-sample case in Section 8.4, when \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, we can construct the two-sample test using the following pivotal quantity \\[\\frac{(\\bar{X}_{n_1} - \\bar{Y}_{n_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} \\sim \\mathcal{N}(0,1)s\\] Thus, to test \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\), we use the test statistic \\[\\frac{(\\bar{X}_{n_1} - \\bar{Y}_{n_2})-\\mu_0}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} \\hspace{3mm} \\overset{H_0: \\mu_1 - \\mu_2 = \\mu_0}{\\sim} \\hspace{3mm} \\mathcal{N}(0,1)\\] When we are testing \\(H_0: \\mu_1 = \\mu_2\\), i.e., \\(\\mu_1 - \\mu_2 = 0\\), we replace \\(\\mu_0\\) in the above formula by 0. The procedure is the same as for the one-sample case, we are just using a different test statistic. Example 9.2 Suppose we want to know whether undergraduate students are paying more than graduate students for rent. We randomly collect \\(100\\) undergraduate students and get an average rent of \\(\\$860\\). Another random sample of \\(70\\) graduate students are collected and the average rent found is \\(\\$800\\). It is known that the standard deviation of rent for undergraduate students is \\(\\$120\\) and that of graduate student is \\(\\$100\\). Conduct a test at 5% significance level. Solution: Let \\(\\mu_1\\) be the average rent for undergraduate students and \\(\\mu_2\\) be the average rent for graduate students. Step 1: Hypotheses: \\[H_0: \\mu_1 = \\mu_2 \\hspace{5mm} \\text{vs.} \\hspace{5mm} H_1: \\mu_1 &gt; \\mu_2\\] Step 2: From step 1, \\(\\mu_0 = 0\\). The test statistic is \\[t = \\frac{\\bar{x}_{n_1} - \\bar{y}_{n_2}}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} = \\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}}} = \\frac{860 - 800}{\\sqrt{\\frac{120^2}{100} + \\frac{100^2}{70}}} = 3.54\\] Step 3: This is a one-sided upper-tail test at \\(\\alpha = 0.05\\) and known \\(\\sigma_1\\) and \\(\\sigma_2\\). So the critical value is \\[c = z_{1-\\alpha} = z_{0.95} = 1.645\\] Step 4: Since \\(t = 3.54 &gt; c = 1.645\\) and this is a one-sided upper-tail test, we reject the null hypothesis at \\(5\\%\\) level of significance. We conclude that at \\(5\\%\\) significance level, the evidence in the data supports that undergraduate students are paying more than graduate students. Notes: In Example 9.2, we see that only Step 2 changes, other steps remain the same as when we are doing one-sample test. Exercise 9.1 Conduct the same test as Example 9.2 if I let \\(\\mu_0 = 100\\) and I want to test \\(H_0: \\mu_1-\\mu_2 = \\mu_0\\) vs \\(H_0: \\mu_1-\\mu_2 &gt; \\mu_0\\) 9.1.4 When \\(\\sigma_1\\) and \\(\\sigma_2\\) are Unknown and \\(\\sigma_1 = \\sigma_2 = \\sigma\\) Consider the case where we do not know \\(\\sigma_1\\) and \\(\\sigma_2\\) but somehow we know that the two population has the same standard deviation. Suppose \\(S_1\\) and \\(S_2\\) are the sample standard deviations, i.e., \\[S_1^2 = \\frac{1}{n_1}\\sum_{i=1}^{n_1}(X_i - \\bar{X}_{n_1})^2 \\hspace{5mm} \\text{and} \\hspace{5mm} S_2^2 = \\frac{1}{n_2}\\sum_{i=1}^{n_2}(Y_i - \\bar{Y}_{n_2})^2\\] We can try to “pool” \\(S_1\\) and \\(S_2\\) to get a single estimate for \\(\\sigma_1 = \\sigma_2 = \\sigma\\) using the formula \\[S_p = \\sqrt{\\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}}\\] This is called the pooled standard deviation. Then, if our assumptions hold, it can be proved that \\[\\frac{(\\bar{X}_{n_1} - \\bar{Y}_{n_2})-\\mu_0}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\hspace{3mm} \\overset{H_0:\\mu_1-\\mu_2=\\mu_0}{\\sim} \\hspace{3mm} t(n_1+n_2-2)\\] We can now use this test statistic to conduct tests for \\(\\mu_1-\\mu_2\\). Everything is the same as the case where \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, just the denominator of the test statistic is changed and the standard normal distribution become the \\(t\\) distribution with degree of freedom \\(n_1+n_2-2\\). Example 9.3 Suppose we need to compare the performance of two call centers in terms of average call lengths and find out if the difference is statistically significant or the difference is just an occurrence that happens by chance. We randomly select 30 calls from center 1 and 20 calls from center 2. The sample means are 122 seconds and 135 seconds respectively. The standard deviation is 15 seconds and 20 seconds respectively. We know that the standard deviation of lengths of calls from the two centers are the same. Conduct a hypothesis test at \\(5\\%\\) level of significance. Solution: Step 1: Let \\(\\mu_1\\) be the mean length of calls of center 1 and \\(\\mu_2\\) be the mean length of calls of center 2. The hypotheses are \\[H_0: \\mu_1 = \\mu_2 \\hspace{5mm} \\text{vs.} \\hspace{5mm} H_1: \\mu_1 \\ne \\mu_2\\] Step 2: From step 1, \\(\\mu_0 = 0\\). The test statistic can be calculated as follows \\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(30-1)\\times 15^2 + (20-1)\\times 20^2}{30+20-2}} = 17.1543\\] \\[t = \\frac{\\bar{x} - \\bar{y}}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{122-135}{17.1543 \\times \\sqrt{\\frac{1}{30} + \\frac{1}{20}}} = -2.6252\\] Step 3: \\(\\alpha = 0.05\\). Our test is two-sided, so we need to use the \\(t\\) statistic \\[c = t_{1-\\alpha/2, n_1+n_2-2} = t_{0.975, 48} = 2.0211\\] Here we use \\(df = 40\\) in the \\(t\\)-table because it is closest to \\(df=48\\). Step 4: Because \\(|t| = 2.6252 &gt; c = 2.0211\\), we reject the null hypothesis at \\(5\\%\\) level of significance. We conclude that there is sufficient evidence in our data that the lengths of calls of the two centers are different. Notes: When you cannot find the degree of freedom in the \\(t\\)-table You can use the degree of freedom that is closest to the degree of freedom you want. For example, you want \\(df = 39\\), you can use \\(df = 40\\) in the table. If the degrees of freedom in the tables are far from yours, choose the smaller one because that means your critical value is higher and test will be more conservative. For example, you want \\(df = 48\\) which is far from both \\(40\\) and \\(60\\) in the \\(t\\)-table, you choose \\(df = 40\\) because it is more conservative. You can always calculate the exact critical value with R using function qt(). For example, qt(0.975, df = 48) will output 2.010635, which means \\(P(X &lt; 2.010635) = 0.975\\) if \\(X\\) follows a \\(t\\)-distribution with 48 degrees of freedom. 9.1.5 When \\(\\sigma_1\\) and \\(\\sigma_2\\) are Unknown and \\(\\sigma_1 \\ne \\sigma_2\\) When you do not know whether \\(\\sigma_1\\) and \\(\\sigma_2\\) are the same, you can use the Welch procedure to approximate the distributions of the difference in means of the two normal samples. In particular, if we are testing \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\), use the observed test statistic \\[t = \\frac{(\\bar{x} - \\bar{y}) - \\mu_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\] and the \\(t\\)-distribution of the degree of freedom \\[df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{s_1^4}{n_1^2(n_1-1)} + \\frac{s_2^4}{n_2^2(n_2-1)}}\\] Example 9.4 Let us revisit Example 9.3 and suppose we do not know that the two standard deviations are equal. Then Step 1: Same as in Example 9.3. Step 2: The test statistic is \\[t = \\frac{(\\bar{x} - \\bar{y}) - \\mu_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} = \\frac{122-135}{\\sqrt{\\frac{15^2}{30} + \\frac{20^2}{20}}} = -2.4790\\] Step 3: \\(\\alpha = 0.5\\) and the test is two-sided so the quantile we need is \\(1-0.05/2 = 0.975\\)-quantile. The degree of freedom we need to use is \\[df = \\frac{\\left(\\frac{15^2}{30} + \\frac{20^2}{20}\\right)^2}{\\frac{15^4}{30^2(30-1)} + \\frac{20^4}{20^2(20-1)}} = 32.89 \\approx = 33\\] The critical value is then \\[c = t_{0.975, 33} \\approx t_{0.975, 30} = 2.0423\\] Step 4: Because \\(|t| = 2.4790 &gt; c = 2.0423\\), we reject the null hypothesis at \\(5\\%\\) level of significance and conclude that there is sufficient evidence in our data that the lengths of calls of the two centers are different. Notes: When conduct the Welch’s procedure by hand, we can use the approximation for the degree of freedom: \\[df \\approx \\min(n_1-1, n_2-1)\\] which means that we use the smaller of \\(n_1-1\\) and \\(n_2-1\\) as our degree of freedom. In the Example 9.4 above, \\(n_1-1 = 29\\) and \\(n_2-1 = 19\\), so we can use \\(19\\) degree of freedom. Exercise 9.2 If we use the approximation, will the test result change? Has the test become more conservative or more lenient? 9.1.6 Summary Consider testing \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\). Case Numerator of \\(t\\) Denominator of \\(t\\) Distribution of \\(t\\) Degree of freedom \\(\\sigma_1, \\sigma_2\\) known \\((\\bar{x}-\\bar{y}) - \\mu_0\\) \\(\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\) \\(z\\) \\(\\sigma_1 = \\sigma_2 = \\sigma\\) unknown \\(\\sqrt{\\left(\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}\\right)\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\\) \\(t\\) \\(n_1 + n_2 - 2\\) \\(\\sigma_1 \\ne \\sigma_2\\) unknown \\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) \\(t\\) \\(\\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{s_1^4}{n_1^2(n_1-1)} + \\frac{s_2^4}{n_2^2(n_2-1)}}\\) or \\(\\min(n_1-1, n_2-1)\\) Notes: When we do not know if the two variance is the same or if they are different, it is better to use the Welch’s procedure directly. When solving for assessments, you can use the approximation \\(\\min(n_1-1, n_2-1)\\). This way, the test will be more conservative, which means that our Type I error is small. However, this also has the risk of increasing Type II error (Figure 8.4. 9.2 Wald-type Tests and Confidence Intervals We can see in all of the tests we have learned so far, the test statistics has the same structure: \\[\\frac{\\hat{\\theta} - \\theta_0}{\\text{sd}(\\hat{\\theta})}\\] where \\(\\theta\\) is a parameter of interest, \\(\\theta_0\\) is the hypothesized value, \\(\\hat{\\theta}\\) is the statistic that estimate the parameter of interest, and \\(sd(\\hat{\\theta})\\) is the standard deviation of that statistic21. Tests using this type of test statistic are called Wald-type tests. The Wald-type tests whose test statistics follow \\(t\\)-distribution are usually called \\(t\\)-tests and the statistic is usually called the \\(t\\)-statistic. Using the \\(t\\)-statistic, we can conduct tests and build confidence intervals for \\(\\theta\\). As mentioned in the Chapter 7, \\[\\text{Confidence level} = 1 - \\alpha.\\] Example 9.5 For the case where \\(\\sigma_1\\) and \\(\\sigma_2\\) is known, \\(\\theta = \\mu_1 - \\mu_2\\) \\(\\hat{\\theta} = \\bar{x} - \\bar{y}\\) \\(\\mathrm{sd}(\\hat{\\theta}) = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\) 9.2.1 From Hypothesis Testing to Confidence Interval Suppose that with significance level \\(\\alpha\\) and a two-sided test, we have the critical value \\(c\\). Then a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\theta\\) can be built by \\[\\hat{\\theta} \\pm c \\times \\text{sd}(\\hat{\\theta})\\] Example 9.6 In Example 9.4, the \\((1-\\alpha)100\\% = 95\\%\\) confidence interval for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align*} &amp; (\\bar{x} - \\bar{y}) \\pm c \\times \\text{Denominator of $t$} \\\\ &amp; = (122-135) \\pm 2.0423 \\times \\sqrt{\\frac{15^2}{30} + \\frac{20^2}{20}} \\\\ &amp; = (-23.7099, -2.2901) \\end{align*}\\] Notes: Note that we used \\(1-\\alpha/2\\) and \\(\\alpha/2\\) quantiles for confidence intervals by convention, which is discussed in Chapter 7. Therefore, the equivalence of the confidence intervals in Chapter 7 would be a two-sided test. 9.2.2 From Confidence Interval to Hypothesis Testing Suppose we have a \\((1-\\alpha)100\\%\\) confidence interval \\((a, b)\\) for \\(\\theta\\). Now we want to test a two-sided test \\[H_0: \\theta = \\theta_0 \\hspace{5mm} \\text{vs} \\hspace{5mm} H_1:\\theta \\ne \\theta_0\\] We just need to see if \\(\\theta_0\\) lies in the confidence interval. If \\[a &lt; \\theta_0 &lt; b\\] Then we do not reject the null hypothesis at \\(100\\alpha\\%\\) level of confidence; and we reject the null hypothesis otherwise. Example 9.7 In Example 9.6, we see that the hypothesized value \\(\\mu_0=0\\) for \\(\\mu_1 - \\mu_2\\) does not lie in the \\(95\\)% confidence interval. Therefore if we are to conduct a two-sided test, we reject the null hypothesis at \\(100\\alpha\\%\\) level of confidence. 9.3 \\(t\\)-test for Paired Data In Section 9.1, we were discussing two populations that are independent of each other. What if the two distributions are paired with each other? Example 9.8 Some examples of paired random variables \\(X\\) and \\(Y\\) \\(X\\): diameter of the left eye of a patient, \\(Y\\): diameter of the right eye of that patient \\(X\\): age of the wife, \\(Y\\): age of the husband \\(X\\): height of the person of the twin who was adopted, \\(Y\\): height of the other person of the twin. In these examples, \\(X\\) and \\(Y\\) are paired and are clearly not independent. For paired data, we have \\(n_1 = n_2 = n\\), i.e., the size of the two samples are the same and each \\(X_i\\) is paired with a specific \\(Y_i\\). Therefore, we can denote our data as \\((X_i, Y_i)\\) for \\(i = 1, 2, ..., n\\). In this case, we cannot use the techniques from Section 9.1 to conduct our tests, because \\(X\\) and \\(Y\\) are dependent on each other. How to remove the dependency? Note that comparing \\(\\mu_1\\) and \\(\\mu_2\\) is equivalent to investigating the difference between the two. Let \\[D = X-Y\\] be the difference of the two random variables \\(X\\) and \\(Y\\). For each pair of data \\((X_i, Y_i)\\) we have the corresponding data \\(D_i\\). Then, a test \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\) is equivalent to testing \\(H_0: \\mu_D = \\mu_0\\). Now, we come back to one-sample tests we discussed in Chapter 8, by replacing \\(\\mu\\) by \\(\\mu_1-\\mu_2\\). The observed test statistic is \\[t = \\frac{\\bar{d} - \\mu_0}{\\mathrm{sd}(\\bar{d})} = \\frac{\\bar{d} - \\mu_0}{s_d/\\sqrt{n}}\\] where \\(\\bar{d}\\) is the mean of the differences \\(d_i = (x_i-y_i)\\) calculated from our observed sample \\(s_d\\) is the standard deviation of the differences \\(d_i\\) we will use \\(n_d-1\\) degree of freedom for our test, where \\(n_d = n\\) is the number of pairs we collected in our sample. Example 9.9 A supermarket chain wants to know if its “buy one, get one free” campaign increases customer traffic enough to justify the cost of the program. For each of \\(10\\) stores it selects two days at random to run the test. For one of those days, the program will be in effect and not for the other. The supermarket chain wants to test the hypothesis that there is no mean difference in traffic against the alternative that the program leads to a difference in the mean traffic. The results from the \\(10\\) stores are presented in the table below. Store # Customer visits with Program Customer visits without Program Difference \\(d_i\\) \\(1\\) \\(140\\) \\(136\\) \\(140-136 = 4\\) \\(2\\) \\(233\\) \\(235\\) \\(233-235 = -2\\) \\(3\\) \\(110\\) \\(108\\) \\(2\\) \\(4\\) \\(42\\) \\(35\\) \\(7\\) \\(5\\) \\(332\\) \\(328\\) \\(4\\) \\(6\\) \\(135\\) \\(135\\) \\(0\\) \\(7\\) \\(151\\) \\(144\\) \\(7\\) \\(8\\) \\(33\\) \\(39\\) \\(-6\\) \\(9\\) \\(178\\) \\(170\\) \\(8\\) \\(10\\) \\(147\\) \\(141\\) \\(6\\) Mean \\(150.1\\) \\(147.1\\) \\(\\bar{d} = 150.1-147.1 = 3\\) Std. dev \\(86.98\\) 86.33 Using the information provided test the null hypothesis of no difference in average traffic at a \\(5\\%\\) level of significance. Solution: Let us start by summarizing the information given to us in the question: The data is paired based on store. In total we have \\(10\\) stores, so \\(10\\) pairs and \\(n_d = 10\\). \\(\\bar{d} = 3\\) \\(s_d = \\sqrt{\\frac{\\sum_{i=1}^{10}(d_i-\\bar{d})^2}{10-1}} = 4.5216\\) Step 1: Hypotheses: \\[H_0: \\mu_D = 0 \\hspace{5mm} \\text{vs} \\hspace{5mm} H_1: \\mu_D \\ne 0\\] Step 2: Test statistic: \\[t = \\frac{\\bar{d}-\\mu_0}{\\mathrm{sd}(\\bar{d})} = \\frac{3-0}{4.5216/\\sqrt{10}} = 2.0981\\] Step 3: In this test, \\(\\alpha = 0.05\\) and we are doing a two-sided test, the degree of freedom is \\(n_d - 1 = 10 - 1 = 9\\). So our critical value is \\[c = t_{1-\\alpha/2, df} = t_{0.975, 9} = 2.2622\\] Step 4: Since \\(|t| = 2.0981 &lt; c = 2.2622\\), we do not reject the null hypothesis at \\(5\\%\\) level of significance. We conclude that there is not sufficient evidence in the data to claim that the programs increases the customer visits. Example 9.10 In Example 9.9, a \\(95\\%\\) confidence interval for the mean difference of customer visits between with and without the program is \\[\\bar{d} \\pm c \\times \\mathrm{sd}(\\bar{d}) = 3 \\pm 2.2622 \\times \\frac{4.5216}{\\sqrt{10}} = (-0.2346, 6.2345)\\] We see that the interval contains \\(\\mu_0 = 0\\) so we do not reject \\(H_0\\) for a two-sided test. This is the same with what we did in Example 9.9. 9.4 \\(t\\)-test for Proportions In Section 9.1 and 9.3, we were interested continuous data. But what if our data is binary? Example 9.11 If we are interested in people’s height, then the data will be a specific number, and we can apply techniques from Section 9.1 and 9.3 to conduct tests. This type of data is continuous. But suppose we are interested in whether Canadians take the Covid vaccine. Then, for each individual in our sample, the data we collect is whether they are vaccinated (yes) or not (no). This data can be further coded as either 1 or 0, respectively. This type of data is called binary. When obtaining binary data, we usually are interested in proportions. Here, it is the proportion of Canadians taking the Covid-19 vaccine. Therefore, the population proportion is our parameter of interest. However, the proportion is nothing so new, it is just the mean of the binary data. Recall the Bernoulli distribution we learn in Chapter 4 which also takes on binary values of \\(0\\) and \\(1\\). We can use Bernoulli distribution to model our data. Let \\(X\\) be our binary random variable of interest. The population distribution follows a Bernoulli distribution: \\[\\mathbb{P}(X = x) = \\begin{cases} p &amp; \\text{if } x=1 \\\\ 1-p &amp; \\text{if } x = 0 \\end{cases}\\] where \\(p\\) is the probability of 1s (successes/yeses), i.e., the probability a person takes the Covid-19 vaccine. Similarly, \\(1-p\\) is the probability that a person does not take the vaccine. In the population where there are a lot of people, \\(p\\), according to the frequentist view of probability in Chapter 3 will be the population proportion of 1s (successes/yeses). Since \\(X \\sim \\text{Bernoulli}(p)\\), from Chapter 4 we know \\[X \\sim (p, p(1-p))\\] that is, \\(\\mathbb{E}(X)=p\\) and \\(\\mathrm{var}(X)=p(1-p)\\). Suppose we collect the data \\(X_1, X_2, ..., X_n \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(p)\\) from the distribution of \\(X\\). We know the expectation and variance of \\(X\\): \\(X \\sim (p, p(1-p))\\). Then by Central Limit Theorem, when \\(n\\) is large, we have the normal approximation \\[\\bar{X}_n = \\frac{X_1 + X_2 +...+X_n}{n} = \\frac{\\text{number of 1s in the sample}}{n} = \\hat{p} \\overset{\\cdot}{\\sim} \\mathcal{N}\\left(p, \\frac{p(1-p)}{n}\\right)\\] Where \\(\\hat{p}\\) is the proportion of yeses in the sample. Now, we can use this approximation to conduct \\(t\\)-tests. We just need to replace \\(\\bar{x}\\) by \\(\\hat{p}\\) and \\(s\\) by \\(\\sqrt{\\hat{p}(1-\\hat{p})}\\). Notes: When we conduct the tests for proportions, always use the standard normal distribution \\(z\\) instead of \\(t\\) distribution because the approximation by the Central Limit Theorem gives normal distribution. The Central Limit Theorem works for large sample only. Conventionally, the tests should be conducted only when the number of 1s (yeses) exceed 15 and the number of 0s (nos) exceed 15. 9.4.1 One-sample Test of Proportion Suppose we are testing \\(H_0: p = p_0\\). Then under the null hypothesis, we know the variance to be \\(\\sigma^2 = p_0(1-p_0)\\). Now we can proceed similar like the one-sample of known variance case. Example 9.12 An importer of electronic goods is considering packaging a new, easy-to-read instruction booklet with DVD players. It wants to package this booklet only if it helps customers more than the current booklet. Previous tests found that only \\(30\\%\\) of customers were able to program their DVD player. An experiment with the new booklet found that \\(16\\) out of \\(60\\) customers were able to program their DVD player. At a \\(5\\%\\) level of significance does the data suggest that more than \\(30\\%\\) of customers were able to program their DVD player with the new manual? Solution: Step 1: In this question, \\(p_0 = 0.3\\), so the hypotheses are \\[H_0: p = 0.3 \\hspace{5mm} vs \\hspace{5mm} H_0: p &gt; 0.3\\] Step 2: The test statistic is \\[t = \\frac{\\bar{x} - \\mu_0}{\\sqrt{\\sigma^2/n}} = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{\\frac{16}{60}-0.3}{\\sqrt{\\frac{0.3(1-0.3)}{60}}} = -0.56\\] Step 3: The test is one-sided upper-tail and the significance level is \\(\\alpha = 0.05\\), so the critical value is \\[c = z_{1-\\alpha} = z_{0.95} = 1.645\\] Step 4: Since \\(-0.56 &lt; 1.645\\) and this is a one-sided upper-tail test, we do not reject the null hypothesis at 5% level of significance and we conclude that the data does not provide sufficient evidence to suggest that the proportion is greater than 30%. Notes: For confidence intervals, we need to use \\[s = \\sqrt{\\hat{p}(1-\\hat{p})}\\] instead of \\(p_0(1-p_0)/n\\) because without the hypothesis test, we do not have the null hypothesis and we do not know \\(p_0\\) or \\(\\sigma\\) anymore. Example 9.13 The \\(95\\%\\) confidence interval for Example 9.12 is \\[\\hat{p} \\pm z_{1-\\alpha/2} \\times \\frac{s}{\\sqrt{n}} = \\frac{16}{60} \\pm 1.96 \\times \\sqrt{\\frac{\\frac{16}{60}\\frac{44}{60}}{60}} = (0.1548, 0.3786)\\] Note that we use \\(z_{1-\\alpha/2}\\) instead of \\(z_{1-\\alpha}\\) because CI is always two-sided. 9.4.2 Two-sample Test of Proportion Suppose we are testing \\(H_0: p_1 - p_2 = 0\\). Then under the null hypothesis, the two variance \\(\\sigma_1\\) and \\(\\sigma_2\\) is the same. We then need to pool the variances together by having \\[s_p = \\sqrt{\\hat{p}(1-\\hat{p})} \\hspace{5mm} \\text{where} \\hspace{5mm} \\hat{p} = \\frac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1+n_2}\\] i.e., \\(\\hat{p}\\) is the proportion of yeses in both populations. Now, the test proceed similarly. Example 9.14 An experiment investigated the claim that taking vitamin C can help to prevent the common cold. Volunteers were randomly assigned to one of two groups: A group that received a \\(1000\\) mg/day supplement of vitamin C and a group that received a placebo. The response variable was whether or not the individual developed a cold during the cold season. In the experiment \\(302\\) out of \\(407\\) people receiving VitC supplement got a cold, \\(335\\) people out of \\(411\\) people receiving a placebo got a cold. Test whether people taking VitC will be less likely to catch a cold at \\(5\\)% significance level. Solution: Step 1: Let \\(p_1\\) be the proportion of people in the VitC group, \\(p_2\\) be the proportion of people in the placebo group. Hypotheses: \\[H_0: p_1 = p_2 \\hspace{5mm} \\text{vs} \\hspace{5mm} H_1: p_1 &lt; p_2\\] Step 2: Since under the null hypothesis, the two variance is the same, we use the pool estimate for \\(\\sigma\\) and we proceed similarly to the case of unknown \\(\\sigma_1 = \\sigma_2 = \\sigma\\): \\[\\begin{align*} \\frac{\\bar{x} - \\bar{y}}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} &amp; = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\\\ &amp; = \\frac{\\frac{302}{407} - \\frac{335}{411}}{\\sqrt{\\left(\\frac{302+335}{407+411}\\right)\\left(1-\\frac{302+335}{407+411}\\right)\\left(\\frac{1}{407} + \\frac{1}{411}\\right)}} \\\\ &amp; = -2.5173 \\end{align*}\\] Step 3: Because this is a proportion, we need to use a \\(z\\)-distribution. A one-sided lower-tail test at \\(\\alpha = 0.05\\) implies the critical value \\[c = z_{\\alpha} = z_{0.05} = -1.645\\] Step 4: Since \\(t = -2.5173 &lt; c = -1.645\\), and this is a one-sided lower-tail test, we reject the null hypothesis at \\(5\\)% significance interval. We conclude that the data provide sufficient evidence to claim that people taking VitC are less likely to catch a cold. Exercise 9.3 Think about Chapter 1, how should the experiment be designed so that the claim in Example 9.14 trustworthy? For confidence intervals, again, without a hypothesis test, we do not have the null hypothesis (we are not sure if it is correct or not), and we do not have \\(p_1 = p_2\\) anymore. Hence, we cannot use the pool variance to plug in our confidence interval. We need to use the variance formula when \\(\\sigma_1 \\ne \\sigma_2\\), i.e., \\(\\frac{\\sigma^2_1}{n_1} + \\frac{\\sigma^2_2}{n_2}\\). We also need to use the two-sided critical value \\(z_{1-\\alpha/2}\\). So a \\((1-\\alpha)100\\%\\) confidence interval for \\(p_1-p_2\\) is \\[\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\] Exercise 9.4 What is the confidence interval for Example 9.14? 9.5 Summary of Wald-type hypothesis tests 9.5.1 Two-sided vs One-sided Tests \\(H_0\\) \\(H_1\\) Quantile Reject \\(H_0\\) \\(p\\)-value \\(\\mu = \\mu_0\\)} \\(\\mu \\ne \\mu_0\\) \\(1-\\alpha/2\\) \\(|t| &gt; c\\) \\(\\mathbb{P}(|T| &gt; t)\\) \\(\\mu &gt; \\mu_0\\) \\(1-\\alpha\\) \\(t &gt; c\\) \\(\\mathbb{P}(T &gt; t)\\) \\(\\mu &lt; \\mu_0\\) \\(\\alpha\\) \\(t &lt; c\\) \\(\\mathbb{P}(T &lt; t)\\) Note here that \\(T\\) is the test statistic and \\(t\\) is the observed test statistic. 9.5.2 Table of Wald-tests Case Test statistic \\(t\\) Dist. of \\(t\\) df \\((1-\\alpha)100\\%\\) CI 1. \\(H_0: \\mu = \\mu_0\\) a. \\(\\sigma\\) known \\(\\frac{\\bar{x} - \\mu_0}{\\sigma/\\sqrt{n}}\\) \\(z\\) \\(\\bar{x} \\pm z_{1-\\alpha/2}\\times \\frac{\\sigma}{\\sqrt{n}}\\) b. \\(\\sigma\\) unknown \\(\\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\) \\(t\\) \\(n-1\\) \\(\\bar{x} \\pm t_{1-\\alpha/2, df}\\times \\frac{s}{\\sqrt{n}}\\) 2. \\(H_0: \\mu_1 - \\mu_2 = \\mu_0\\) a. \\(\\sigma_1, \\sigma_2\\) known \\(\\frac{(\\bar{x} - \\bar{y}) - \\mu_0}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\) \\(z\\) \\((\\bar{x} - \\bar{y}) \\pm z_{1-\\alpha/2} \\times \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\) b. \\(\\sigma_1 = \\sigma_2 = \\sigma\\) unknown \\(\\frac{(\\bar{x} - \\bar{y}) - \\mu_0}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\) \\(t\\) \\(n_1+n_2-2\\) \\((\\bar{x} - \\bar{y}) \\pm t_{df,1-\\alpha/2} \\times s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\) c. \\(\\sigma_1, \\sigma_2\\) unknown \\(\\frac{(\\bar{x} - \\bar{y}) - \\mu_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\) (*) \\(t\\) \\(\\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{s_1^4}{n_1^2(n_1-1)} + \\frac{s_2^4}{n_2^2(n_2-1)}}\\) or \\(\\min(n_1-1, n_2-1)\\) \\((\\bar{x} - \\bar{y}) \\pm t_{df,1-\\alpha/2} \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) 3. \\(H_0: \\mu_1 - \\mu_2 = \\mu_D = \\mu_0\\) for paired two-sample \\(\\frac{\\bar{d} - \\mu_0}{s_d/\\sqrt{n}}\\) \\(t\\) \\(n_D-1\\) \\(\\bar{d} \\pm t_{1-\\alpha/2,df}\\times \\frac{s_D}{\\sqrt{n_D}}\\) 4. \\(H_0: p = p_0\\) \\(\\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\) \\(z\\) \\(\\hat{p} \\pm z_{1-\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) 5. \\(H_0: p_1 = p_2\\) \\(\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\\) (**) \\(z\\) \\(\\hat{p}_1 - \\hat{p}_2 \\pm z_{1-\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\\) where (*) \\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\) (**) \\(\\hat{p} = \\frac{n_1p_1+n_2p_2}{n_1+n_2}\\). Note that the statistic is unknown until we collect the data and its value is subject to chance, so itself has a standard deviation.↩︎ "],["chap-other.html", "10 Other Useful tests 10.1 ANOVA 10.2 Chi-square Test 10.3 Levene’s Test 10.4 QQ plot 10.5 Nonparametric Tests 10.6 Wilcoxon’s Rank Sum Test 10.7 Kruskal-Wallis test 10.8 Summary of the Tests", " 10 Other Useful tests In Chapter 8 and 9, we have been introduced to Wald-type tests, which are fundamental methods in statistics. However, there are other statistical hypothesis tests that will be useful to answer different questions for different situations. In this lesson, we will learn some of those tests. In doing so, we will only focus on the (i) hypotheses of the test; (ii) assumptions made for each test and (iii) the analysis procedure in R. 10.1 ANOVA In Lesson 8 and 9, we learned about tests that (i) compare the mean of one population to a specific number (one-sample \\(t\\)-test) (ii) compare the means of two populations (two-sample \\(t\\)-test). What if we have more than two populations? Example 10.1 Let’s come back to the student’s rent case. We want to know if the rent expense of students from all faculties are the same. Now, suppose I have \\(m\\) populations \\(X^{(1)}, X^{(2)}, ..., X^{(m)}\\) and we want to test whether the means of these population are equal. A useful statistical tool to do so this is the (one-way) ANOVA (analysis of variance). 10.1.1 Assumptions The \\(m\\) populations are independent For each population \\(i\\) in the \\(m\\) populations, \\(X^{(i)} \\sim N(\\mu_i, \\sigma^2)\\) and we have \\(n_i\\) observations \\(X^{(i)}_1, X^{(i)}_2, ..., X^{(i)}_{n_i}\\). This means each population all follow normal distributions the normal distributions may have different populations means \\(\\mu_1, \\mu_2, ..., \\mu_m\\) the normal distributions must have same variance \\(\\sigma^2\\) Example 10.2 Continue with Example 10.1, the ANOVA procedure assumes that the rent expense For Arts: \\(X^{(1)} \\sim N(\\mu_1, \\sigma^2)\\) For Engineering: \\(X^{(2)} \\sim N(\\mu_2, \\sigma^2)\\) For Environment: \\(X^{(3)} \\sim N(\\mu_3, \\sigma^2)\\) For Health: \\(X^{(4)} \\sim N(\\mu_4, \\sigma^2)\\) For Math: \\(X^{(5)} \\sim N(\\mu_5, \\sigma^2)\\) For Science: \\(X^{(6)} \\sim N(\\mu_6, \\sigma^2)\\) We also assume that the rent expense of students coming from one faculty does not affect the rent expense of students coming from another faculty (independence) In our data, we have \\(n_1\\) students coming from Arts, \\(n_2\\) students coming from Engineering, \\(n_3\\) students coming from Environment, \\(n_4\\) students coming from Health, \\(n_5\\) students coming from Math and \\(n_6\\) students coming from Science. Notes: In ANOVA, it is preferred, but not required, that the sample sizes are equal for each population, i.e., \\(n_1 = n_2 = ... = n_m\\). 10.1.2 Hypotheses ANOVA wants to test the following two hypotheses \\(H_0:\\) \\(\\mu_1 = \\mu_2 = ... = \\mu_m = \\mu\\) \\(H_1:\\) at least one of \\(\\mu_i\\) differs from the rest of the population means. Example 10.3 In Example 10.2, we want to test if the mean rent expenses of students coming from the six faculties equal. 10.1.3 Analysis in R To conduct an ANOVA in R, we use function aov(). Example 10.4 To know if the mean rent are different among the different faculty values, we conduct an ANOVA on the data set we have collected fakeRent.csv. We use the following R code #do an anova of variable rent with respect to variable faculty #the two variables are called rent and faculty of the data set called `rent` #memorize the anova results into an object called rentFaculty rentFaculty &lt;- aov(rent~faculty, data = rent) #show the summary of the anova results summary(rentFaculty) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## faculty 5 144104 28821 1.516 0.182 ## Residuals 1339 25453319 19009 There are many information in this summary, however, for the sake of conciseness for this book, we just focus our attention to the \\(p\\)-value (Pr(&gt;F)). The \\(p\\)-value is \\(0.182\\), which is greater than our usual level of significance level \\(0.05\\), so we do not reject the null hypothesis at \\(5\\)% level of significance. We conclude that the data does not show sufficient evidence that the mean rent expenses differ among students of different faculties. Exercise 10.1 Statistically test if the rent expense differ among students of different genders. Notes: Recall in Chapter8, I have emphasized that we reject the null hypothesis if \\(p\\)-value \\(&lt; \\alpha\\) for all kinds of test in statistics. You can read more about ANOVA here. 10.2 Chi-square Test In ANOVA, we can think of the different populations as categories of one categorical variable. Example 10.5 In Example @ref(exm:#ex-other-rent-faculty-assump), Arts, Engineering, Environment, Health, Math and Science can be thought of as 6 categories of a variable about “faculty”. The question whether the rent expense of students from all faculties are the same is equivalent to the question whether faculty affect the mean rent expense of students. This is why in the R code of Example 10.1, we input the variable faculty as a possible explanatory variable of variable rent. In ANOVA, because we assume normal distributions for each of the population, effectively we are working with a continuous response variable and one categorical explanatory variable. But what if both our variables are categorical? In that case, we need to use the chi-square test to test whether the two variables are independent, i.e., if one variable affect the other variable. Example 10.6 Suppose I want to know if the ratios of female, male, or students of other genders are the same among different faculty. This is equivalent to testing whether the distribution of variable gender differs among the different faculties of the variable faculty, which is also equivalent to testing whether faculty and gender are independent. 10.2.1 Assumptions Each observation has to be of one category for each random variable. There are at least five observations for each combination of the categories of the two variables. Example 10.7 In Example 10.6, we when we conduct a Chi-square test, we have assumed that each student is only categorized as either Female, Male or Others, and they cannot be both/all of Female, Male or Others. Similarly, each student belongs to only one faculty among Arts, Engineering, Environment, Health, Math or Science. For each combination of gender and faculty, we collected data from at least five students. For example, we have data about at least five male students in Environment, etc. 10.2.2 Hypotheses Suppose we have two categorical variables \\(X\\) and \\(Y\\). In a Chi-square test, we test the hypotheses \\(H_0\\): \\(X\\) and \\(Y\\) are independent \\(H_1\\): \\(X\\) and \\(Y\\) are dependent Example 10.8 In Example 10.6, we want to test if gender and faculty are independent. 10.2.3 Analysis in R We use the function chisq.test() in R to conduct a Chi-square test. Example 10.9 For the question in Example 10.6, we want to test whether the variable faculty and the variable gender of the data set fakeRent.csv are independent. We use the following code #a chi-square test of the gender and faculty variables #of the data set rent we loaded from fakeRent.csv as in anova chisq.test(rent$gender, rent$faculty) ## ## Pearson&#39;s Chi-squared test ## ## data: rent$gender and rent$faculty ## X-squared = 78.322, df = 10, p-value = 1.07e-12 Again, there are many information included in the output of this test, but we just need to focus on the \\(p\\)-value. The outputed \\(p\\)-value is \\(1.07e-12\\), which is a very small number, and is surely smaller than the usual significance level \\(\\alpha = 0.05\\). Therefore, we reject the null hypothesis at \\(5\\%\\) significance level and conclude that there is sufficient evidence in the data to support the claim that gender and faculty are dependent, or, faculty affects the distribution of gender. Exercise 10.2 Test statistically if study level and gender are independent using the data set fakeRent.csv. Notes: In a hypothesis test, always decide your level of significance before you see the data. If you do not have a level of significance in your mind, you either use the usual \\(\\alpha = 5\\%\\) level of significance report the \\(p\\)-value only and depending on the value of \\(p\\)-value, conclude if the evidence is relatively strong or not (to support your null hypothesis). Read more about the chi-square test here. 10.3 Levene’s Test The Levene’s test is used to compare the variances of different populations. You can think of the Levene’s test as a version of ANOVA, but applied to the variances, instead of the means. The test can be useful in cases where our main interest is the variance, not the mean. For example, we want to know whether a new machine can fill bottles more precisely than the old machine. we want to check the equal variance assumption for our ANOVA procedure. 10.3.1 Hypotheses The Levene’s test tests the hypotheses \\(H_0: \\sigma_1 = \\sigma_2 = ... = \\sigma_m = \\sigma\\) where \\(\\sigma_i\\) is the standard deviations of population \\(i\\), for \\(1 \\le i \\le m\\). \\(H_1:\\) at least one of \\(\\sigma_i\\) differs from the rest of the population standard deviations. 10.3.2 Analysis in R The Levene’s test is basically just an ANOVA test for the absolute differences of the observations from the group means. To conduct the test in R, we use the function leveneTest() from the package car in R. Example 10.10 We used ANOVA back in Example 10.4. Therefore, we would need to check for the assumption of equal variances among the rent expenses of students coming from the different faculties. We will do this using the Levene’s test. First, you need to install the car package if you haven’t already install.packages(&quot;car&quot;) Now, you can run a Levene’s test in R as follows #load the library/package needed library(car) #run the levene test on the rent variable of data set rent #the groups to be compared are different faculties #the data about the faculties are encoded in variable faculty of data set rent leveneTest(rent$rent, rent$faculty) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 0.384 0.86 ## 1339 Again, we just need to focus on the \\(p\\)-value, which is 0.86 in this case. Since the \\(p\\)-value is greater than our usual level of significance \\(\\alpha = 0.05\\), we do not reject the null hypothesis at \\(5\\%\\) level of significance and we conclude that the data suggests the variances of the student’s rent expenses are the same among different faculty. Exercise 10.3 Check the equality of variance assumption for the test in Exercise 9.1 10.4 QQ plot With the Levene’s test, we have checked the equality of variance assumption for ANOVA. What about the assumption of normal distribution? We can use a QQ plot. The QQ plot plots the sample quantiles of the data against the theoretical quantiles of the standard normal distribution. If The points on the plot form a straight line (maybe with some slight departure at the two ends of the diagonal line), we can say that the distribution of the data is roughly normal. Otherwise, the distribution of the data does not follow a normal distribution. It is best to illustrate this using an example in R using the function qqnorm(). Example 10.11 Continue with Example 10.4, now let us check the normality assumption. In the ANOVA, we assumed that the rent expenses of students coming from each faculty follows a normal distribution. To check if the rent expenses of students coming from Arts follows a normal distribution, we use the following code #check whether the distribution of the rent variable #for observations coming from Arts faculty, #i.e., for observations where the value of variable faculty equals to &quot;Arts&quot; qqnorm(rent$rent[rent$faculty == &quot;Arts&quot;]) #add a reference line to the qqnorm plot #0.2 and 0.8 are the quantiles where the line is drawn qqline(rent$rent[rent$faculty == &quot;Arts&quot;], probs = c(0.2, 0.8)) The points on the plot form a reasonably straight line, therefore, the normal assumption is quite reasonable for the rent expenses for Arts students. Now, if we want to plot QQplots for students from all faculties, we use the following code #create a grid of two rows and three columns for the plots par(mfrow=c(2,3)) #arts faculty qqnorm(rent$rent[rent$faculty == &quot;Arts&quot;], main = &quot;Arts students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Arts&quot;], probs = c(0.2, 0.8)) #engineering faculty qqnorm(rent$rent[rent$faculty == &quot;Engineering&quot;], main = &quot;Engineering students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Engineering&quot;], probs = c(0.2, 0.8)) #environment faculty qqnorm(rent$rent[rent$faculty == &quot;Environment&quot;], main = &quot;Environment students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Environment&quot;], probs = c(0.2, 0.8)) #health faculty qqnorm(rent$rent[rent$faculty == &quot;Health&quot;], main = &quot;Health students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Health&quot;], probs = c(0.2, 0.8)) #math faculty qqnorm(rent$rent[rent$faculty == &quot;Math&quot;], main = &quot;Math students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Math&quot;], probs = c(0.2, 0.8)) #science faculty qqnorm(rent$rent[rent$faculty == &quot;Science&quot;], main = &quot;Science students&#39; rent&quot;) qqline(rent$rent[rent$faculty == &quot;Science&quot;], probs = c(0.2, 0.8)) Based on the plots, we can see that our data seem to satisfy the normality (normal distributions) assumption. Exercise 10.4 Check the normality assumption for the test in Exercise 9.1 Notes: There are statistical tests for normality as well. A popular test for normality is called the Shapiro-Wilk test. You can use function shapiro.test() to conduct the test in R. 10.5 Nonparametric Tests To make statistical tests work, we have made some assumptions about our data. For \\(t\\)-tests, we assume the two distributions are normally distributed. However, we learned that the test can still be used if \\(n &lt; 15\\), and the data is close to normal (symmetric, unimodal, no outliers) \\(n \\ge 15\\), except if the distribution is strongly skewed or if it has outliers \\(n \\ge 40\\): any distribution is fine. For ANOVA, we assume that the distributions are normal and have equal variances. ANOVA can still be used when normality assumption is slightly violated (the distribution is not heavily skewed, and there is no outliers) when the largest variance is at most four times more than the smallest variance, as long as the sample sizes are the same (\\(n_1 = n_2 = ... = n_m = n\\)) What should we do if these conditions/assumptions are violated? For example, when the data is so expensive to collect that our sample size is too small? Or when the variances are so strongly different for ANOVA and the sample sizes are not the same? In these cases, we come to hypothesis tests that require less/different assumptions. These alternative tests usually do not require an assumption about population parameters. For such a reason, they are called nonparametric tests. 10.6 Wilcoxon’s Rank Sum Test The Wilcoxon’s rank sum test is a possible alternative for the two-sample \\(t\\)-tests we learn in Lesson 8. 10.6.1 Assumptions The response variable is rankable (i.e., you can rank the observations of the response variable). The two populations are of similar shapes and scales. 10.6.2 Hypotheses Although we can think of the Wilcoxon’s rank sum test as a possible alternative to the \\(t\\)-test, the test tests a slightly different pair of hypotheses: \\(H_0:\\) the median of the two distributions are the same \\(H_1:\\) the median of the two distributions are not the same 10.6.3 Analysis in R In R, we can run both unpaired independent and paired tests using the function wilcox.test() by choosing the correct option for the option paired inside the function. Example 10.12 Suppose we want to know if the medians of rent expense are different among undergraduate and graduate students. We can think about running a two-sample \\(t\\)-test. However, if the assumptions do not work, we can use a Wilcoxon’s rank sum test instead using the following code #use a wilcoxon&#39;s rank sum test for rent expenses of #undergraduate and graduate student wilcox.test(rent$rent[rent$study == &quot;Undergraduate&quot;], rent$rent[rent$study == &quot;Graduate&quot;], alternative = &quot;two.sided&quot;, paired = FALSE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: rent$rent[rent$study == &quot;Undergraduate&quot;] and rent$rent[rent$study == &quot;Graduate&quot;] ## W = 263622, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 We can see that the syntax of function wilcox.test() is quite similar to that of t.test(). Now, the \\(p\\)-value is \\(2.2e-16\\), which is almost 0. Therefore, it is smaller than our usual value \\(\\alpha = 0.05\\). We reject the null hypothesis at \\(5\\%\\) significance level and conclude that the medians of undergraduate and graduate’s rent expense are statistically significantly different. Exercise 10.5 Use the Wilcoxon’s rank sum test to test if the rent price is higher when the room has a private washroom. Notes: Suppose that the two populations you are testing are \\(X\\) and \\(Y\\). And further suppose that we are not willing to make the assumption that the populations of \\(X\\) and \\(Y\\) are of similar shapes and scales, then with the Wilcoxon’s rank sum test (the two-sided version), we are testing \\(H_0: \\mathbb{P}(X &gt; Y) = \\mathbb{P}(X &lt; Y)\\), i.e., the probability of an observation from population \\(X\\) exceeding an observation from population \\(Y\\) is the same as the probability of an observation from \\(Y\\) exceeding an observation from \\(X\\); \\(H_1: \\mathbb{P}(X &gt; Y) \\ne \\mathbb{P}(X &lt; Y)\\). 10.7 Kruskal-Wallis test This test is an ANOVA version of the Wilcoxon’s rank sum test, i.e., it is used to test more than two populations. 10.7.1 Assumptions The response variable is rankable The populations or all groups are similarly shaped and scaled, except for (possibly) any difference in medians 10.7.2 Hypotheses \\(H_0:\\) the median of the distributions are the same \\(H_1:\\) the median of the distributions are not the same 10.7.3 Analysis in R In R, we use the function kruskal.test() to conduct a Kruskal-Wallis test. Example 10.12 Although we can use an ANOVA as we did in Example 10.4, let’s apply the Kruskal-Wallis test to rent expenses of students of different faculties. #kruskal-wallis test of the rent variable #among the different categories of the variable faculty #from the data set rent kruskal.test(rent~faculty, data = rent) ## ## Kruskal-Wallis rank sum test ## ## data: rent by faculty ## Kruskal-Wallis chi-squared = 7.6994, df = 5, p-value = 0.1736 The \\(p\\)-value is \\(0.1736\\), which is greater than \\(\\alpha = 0.05\\). Therefore, we do not reject the null hypothesis at \\(5\\%\\) significance level and conclude that the data suggests the median rent expenses are the same among students of different faculties. Exercise 10.6 Plot a side-by-side boxplot of the student’s rent among students of different genders. Use the Kruskal-Wallis test to test if the medians are the same. Look at the boxplot and see if the assumption of similarly shaped and scaled populations is reasonable. 10.8 Summary of the Tests Test What to test Function in R ANOVA Test equality of means of two or more populations aov() Chi-square test Test if two categorical variables are independent chisq.test() Levene’s test Test equality of variances of two or more populations leveneTest() in package car QQplot Check normality assumption by plots qqnorm() and qqline() Shapiro-Wilk test Test normality assumption shapiro.test() Wilcoxon’s rank sum test Test equality of medians of two populations wilcox.test() Kruskal-Wallis test Test equality of medians of two or more populations kruskal.test() Notes: We have learned many tests in this lesson. The mathematical reasoning behind these tests are involved and skipped for the conciseness and introductory purpose of this book. To use the tests, we should focus on understanding the idea (which test is used for what), the assumptions, and the hypotheses for each the test. Moreover, we need to know how to run the tests in R and interpret the results. These tests will be very helpful to any statistical project you conduct in the future. "],["chap-cor.html", "11 Correlation 11.1 Covariance 11.2 Correlation Coefficient 11.3 Correlation Test 11.4 Summary", " 11 Correlation In the previous chapters, we have learned statistical methods about (i) the relationship between two categorical variables (chi-square test), (ii) the relationship between one categorical variable and one continuous variable (ANOVA). In this lesson, let’s learn about the relationship between two continuous variables via the correlation coefficient. 11.1 Covariance 11.1.1 Population Covariance The covariance of two random variables \\(X\\) and \\(Y\\) is denoted as \\(\\mathrm{cov}(X, Y)\\) and is defined as \\[\\mathrm{cov}(X, Y) = \\mathbb{E}\\Big[ (X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)) \\Big] \\] that is, the covariance is the expectation of the product of the difference between \\(X\\) and the mean of \\(X\\); and the difference between \\(Y\\) and the mean of \\(Y\\). Example 11.1 Suppose the joint distribution of \\(X\\) and \\(Y\\) is \\(x\\) \\(3\\) \\(6\\) \\(4\\) \\(6\\) \\(8\\) \\(y\\) \\(10\\) \\(7\\) \\(4\\) \\(8\\) \\(7\\) \\(p(x, y)\\) \\(0.15\\) \\(0.3\\) \\(0.05\\) \\(0.4\\) ? Find number in the \\(?\\) Calculate the covariance of \\(X\\) and \\(Y\\). Solution: \\(1 - 0.15 - 0.3 - 0.05 - 0.4 = 0.1\\) We have \\(\\mathbb{E}(X) = 3\\times 0.15 + 6 \\times 0.3 + 4\\times 0.05 + 6\\times 0.4 + 8 \\times 0.1 = 5.65\\) \\(\\mathbb{E}(Y) = 10\\times 0.15 + 7 \\times 0.3 + 4\\times 0.05 + 8\\times 0.4 + 7 \\times 0.1 = 7.7\\) \\[\\begin{align*} \\mathrm{cov}(X, Y) &amp; = (3-5.65)\\times (10-7.7) \\times 0.15 + (6-5.65) \\times (7 - 7.7) \\times 0.3 \\\\ &amp; \\qquad + ... + (8-5.65)\\times (7-7.7)\\times 0.1 \\\\ &amp; = -0.805. \\end{align*}\\] Notes: In Example 11.1, \\(p(X, Y)\\) is the probability that \\(X = x\\) and \\(Y = y\\) at the same time, where \\(x\\) and \\(y\\) are possible values of \\(X\\) and \\(Y\\), respectively. That is, we are not talking about individual distributions, but joint distribution here. We can expand the formula of covariance as follows \\[\\begin{align*} \\mathrm{cov}(X, Y) &amp; = \\mathbb{E}\\Big[ (X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)) \\Big] \\\\ &amp; = \\mathbb{E} \\Big[ XY - X\\mathbb{E}(Y) - Y\\mathbb{E}(X) + \\mathbb{E}(X) \\mathbb{E}(Y) \\Big] \\\\ &amp; = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) - \\mathbb{E}(X)\\mathbb{E}(Y) + \\mathbb{E}(X)\\mathbb{E}(Y) \\\\ &amp; = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y). \\end{align*}\\] Therefore, we obtain another (equivalent) formula for covariance \\[\\mathrm{cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) \\] This formula is more convenient for calculation. Example 11.2 Continue with Example 11.1, we can also calculate the covariance in the following way \\(\\mathbb{E}(X) = 5.65\\) as in Example 11.1 \\(\\mathbb{E}(Y) = 7.7\\) as in Example 11.1 \\(\\mathbb{E}(XY) = 3\\times 10\\times 0.15 + 6\\times 7 \\times 0.3 + 4 \\times 4 \\times 0.05 + 6 \\times 8 \\times 0.4 + 8\\times 7 \\times 0.1 = 42.7\\) \\(\\mathrm{cov}(X,Y) = 42.7 - 5.65\\times 7.7 = -0.805\\) 11.1.2 Relationship with Variance Suppose \\(X\\) and \\(Y\\) are any two quantitative random variables, and \\(a\\) and \\(b\\) are two constants, then \\[\\mathrm{var}(aX + bY) = a^2\\mathrm{var}(X) + 2ab\\mathrm{cov}(X, Y) + b^2 \\mathrm{var}(Y)\\] If \\(X\\) and \\(Y\\) are independent, then \\(\\mathrm{cov}(X, Y) = 0\\) and we can retrieve the formula from Lesson 6: \\[\\mathrm{var}(aX + bY) = a^2\\mathrm{var}(X) + b^2 \\mathrm{var}(Y)\\] Example 11.3 For example, the formula for the variance of, say \\(-2X + 3Y\\), can be derived as follows: \\[\\begin{align*} \\mathrm{var}(-2X + 3Y) &amp; = (-2)^2 \\times \\mathrm{var}(X) +2\\times(-2)\\times 3\\times \\mathrm{cov}(X, Y) + 3^2\\times \\mathrm{var}(Y) \\\\ &amp; = 4\\mathrm{var}(X) - 12\\mathrm{cov}(X, Y) + 9 \\mathrm{var}(Y) \\end{align*}\\] 11.1.3 Interpretation The magnitude of covariance is NOT important. It can take values from \\(+\\infty\\) to \\(-\\infty\\). We can only tell the direction of the relationship of \\(X\\) and \\(Y\\) from \\(\\mathrm{cov}(X, Y)\\): If \\(\\mathrm{cov}(X, Y) &gt; 0\\), it suggests a positive association between \\(X\\) and \\(Y\\) (\\(X\\) increases, \\(Y\\) increases) If \\(\\mathrm{cov}(X, Y) &lt; 0\\), it suggests a negative association between \\(X\\) and \\(Y\\) (\\(X\\) increases, \\(Y\\) decreases) To see this, let us look at Figure 11.1 below. In the top right quarter, the \\(x\\)-coordinates are greater than \\(\\mathbb{E}(X)\\) and \\(y\\)-coordinates are greater than \\(\\mathbb{E}(Y)\\). Therefore, \\(X - \\mathbb{E}(X) &gt; 0\\) and \\(Y - \\mathbb{E}(Y) &gt; 0\\) which leads to \\((X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)) &gt; 0\\). In the bottom left quarter, the \\(x\\)-coordinates are smaller than \\(\\mathbb{E}(X)\\) and \\(y\\)-coordinates are smaller than \\(\\mathbb{E}(Y)\\). Therefore, \\(X - \\mathbb{E}(X) &lt; 0\\) and \\(Y - \\mathbb{E}(Y) &lt; 0\\) which leads to \\((X - \\mathbb{E}(X))(Y - \\mathbb{E}(Y)) &gt; 0\\). So, as indicated in Figure 11.1 if \\(\\mathrm{cov}(X, Y) &gt; 0\\), the data \\((X, Y)\\) are more likely to stay in the top right or the bottom left quarter. Similarly, if \\(\\mathrm{cov}(X, Y) &lt; 0\\), the data \\((X, Y)\\) are more likely to stay in the top left or the bottom right quarter. Figure 11.1: Positive and negative covariances 11.1.4 Sample Covariance We can see that \\(\\mathrm{cov}(X, Y)\\) are defined as an expectation. However, note that expectations are only calculated on the population distribution. In practice, we usually don’t know the population distribution. Nevertheless, we can calculate the sample covariance of \\(X\\) and \\(Y\\), denoted as \\(s_{XY}\\) from the data using the following formula \\[s_{XY} = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) = \\frac{1}{n-1} \\left[ \\left(\\sum_{i=1}x_iy_i \\right) - n\\bar{x}\\bar{y}\\right] \\] We can see that the expectation is approximated by the average of the product of the distance between the data point \\(x_i\\) and the sample mean \\(\\bar{x}\\) of \\(X\\) the distance between the data point \\(y_i\\) and the sample mean \\(\\bar{y}\\) of \\(Y\\) According to the empirical view of probability in Chapter 3, when we have infinitely many data point, the relative frequency of a pair of possible values \\((x, y)\\) will approximate the probability that \\((x, y)\\) happens in the population. Therefore, \\(s_{XY}\\) will approach \\(\\mathrm{cov}(X, Y)\\) when the sample size \\(n\\) goes to \\(+\\infty\\). Example 11.4 A new graduate is thinking ahead and wanting to better understand what goes into a good credit score. They manage to collect a random sample of annual income (in 100’s of thousands) and credit scores values for 10 people. Solve for and interpret the covariance. Income (\\(X\\)) Credit score (\\(Y\\)) \\(1.3\\) \\(756\\) \\(1.1\\) \\(728\\) \\(0.8\\) \\(635\\) \\(1.2\\) \\(599\\) \\(1.4\\) \\(760\\) \\(0.9\\) \\(722\\) \\(0.9\\) \\(743\\) \\(1.4\\) \\(726\\) \\(1.2\\) \\(694\\) \\(1.0\\) \\(726\\) Solution: Using the data we can solve for \\(\\bar{x} = 1.12\\) and \\(\\bar{y} = 708.9\\) \\[\\begin{align*} s_{XY} &amp; = \\frac{(\\sum_{i=1}^{10} x_iy_i) - 10 \\bar{x}\\bar{y}}{10-1} \\\\ &amp; = \\frac{[(1.3 \\times 756) + ... + (1.0\\times 726)] - [10 \\times 1.12 \\times 708.9]}{10-1} \\\\ &amp; = 3.15778 \\end{align*}\\] This value suggests a positive association between annual income (\\(X\\)) and credit score (\\(Y\\)). Example 11.5 Many factors affect the length of a professional football game, for example the number of running plays versus the number of passing plays. A study was conducted to determine the relationship between the total number of penalty yards (\\(X\\)) and the time required to complete a game (\\(Y\\), in hours). The table below provides the data. Calculate and interpret the covariance. Total (\\(X\\)) Time to complete game (\\(Y\\)) \\(196\\) \\(4.2\\) \\(164\\) \\(4.1\\) \\(167\\) \\(3.5\\) \\(35\\) \\(3.2\\) \\(111\\) \\(3.2\\) \\(78\\) \\(3.6\\) \\(150\\) \\(4.0\\) \\(121\\) \\(3.1\\) \\(40\\) \\(1.9\\) Solution: Using the data we can solve for \\(\\bar{x} = 118\\) and \\(\\bar{y} = 3.422\\). So \\[s_{XY} = \\frac{(\\sum_{i=1}^{9} x_iy_i) - 9 \\bar{x}\\bar{y}}{9-1} = \\frac{[(196 \\times 4.2) + ... + (40\\times 1.9)] - [9 \\times 118 \\times 3.422]}{9-1} = 30.6\\] This value suggests a positive association between total # of penalty yards (\\(X\\)) and the time to complete the game \\((Y)\\). 11.2 Correlation Coefficient 11.2.1 Population Correlation Coefficient We just learned that the magnitude of the covariance is NOT important. This is because \\(X\\) and \\(Y\\) can have different scales and units. Therefore, it will be hard to compare or get a sense of the strength of the association between two random variables using covariances. The covariance of \\(X\\) and \\(Y\\) can be standardized into the (Pearson) correlation coefficients \\(\\rho_{XY}\\) (or in some other texts, \\(\\mathrm{cor}(X, Y)\\)) using the following formula \\[\\rho_{XY} = \\frac{\\mathrm{cov}(X, Y)}{\\sqrt{\\mathrm{var}(X)\\mathrm{var}(Y)}}\\] With this standardization, the magnitude of the correlation coefficient always lies between \\(-1\\) or \\(1\\), i.e., \\(-1 \\le \\rho_{X, Y} \\le 1\\), no matter the scales or units of \\(X\\) and \\(Y\\). Example 11.6 Continue with Example 11.1, we can calculate the correlation coefficient of \\(X\\) and \\(Y\\) as follows \\(\\mathbb{E}(X^2) = 3^2\\times 0.15 + 6^2 \\times 0.3 + 4^2\\times 0.05 + 6^2\\times 0.4 + 8^2 \\times 0.1 = 33.75\\) \\(\\mathrm{var}(X) = \\mathbb{E}(X^2) - (\\mathbb{E}(X))^2 = 33.75-5.65^2 = 1.8275\\) \\(\\mathbb{E}(Y^2) = 10^2\\times 0.15 + 7^2 \\times 0.3 + 4^2\\times 0.05 + 8^2\\times 0.4 + 7^2 \\times 0.1 = 61\\) \\(\\mathrm{var}(Y) = \\mathbb{E}(Y^2) - (\\mathbb{E}(Y))^2 = 61 - 7.7^2 = 1.71\\) \\(\\rho_{XY} = \\frac{\\mathrm{cov}(X, Y)}{\\sqrt{\\mathrm{var}(X)\\mathrm{var}(Y)}} = \\frac{-0.805}{\\sqrt{1.8275\\times 1.71}} = -0.4553\\). Exercise 11.1 In Example 11.6, calculate \\(\\mathrm{var}(X-Y)\\). 11.2.2 Interpretation As mentioned above, the correlation coefficient \\(\\rho_{XY}\\) is a number between \\(-1\\) and \\(1\\). There are two important features about the correlation coefficient magnitude: the size of the number The closer \\(|r|\\) is to 1 the stronger the linear relationship. Graphically, this means the data distribute tightly about a straight line. If \\(|r| = 1\\), we call it a perfectly linear relationship. The closer \\(|r|\\) is to zero, the more randomly scattered and less linear the data are. See some examples of how data will look like at different values of correlation coefficients below. Figure 11.2: Variety of correlations direction: the sign of the number A positive correlation coefficient indicates that the points have a positive slope A negative correlation coefficient indicates that the points have a negative slope Note: The Pearson correlation coefficient is checking for a linear relationship only, i.e., whether we can fit a straight line to the data. Sometimes even though \\(\\rho_{XY} = 0\\), other types of relationships, say quadratic, exist. This is why it is always important to plot your data. This way you can see the relationship and make proper sense of the correlation value obtained. Other examples can be found below Notes: Correlation measures the strength of only the linear relationship between two quantitative variables. Like the mean and the standard deviation, the correlation is not robust to outliers. Correlation only tells us as \\(X\\) increases, \\(Y\\) increases, or as \\(X\\) increases, \\(Y\\) decreases, i.e., it defines a trend only. It does NOT imply that changes in \\(X\\) induce changes in \\(Y\\), or \\(X\\) causes \\(Y\\). Correlation only allows us to make conclusions of association. If \\(X\\) and \\(Y\\) are independent, \\(\\mathrm{cov}(X, Y) = 0\\) and thus \\(\\rho_{XY} = 0\\). The converse is NOT necessarily correct, i.e., \\(\\mathrm{cov}(X, Y) = 0\\) does not imply \\(X\\) and \\(Y\\) are independent. \\(\\mathrm{cov}(X, Y) = 0\\) implies \\(X\\) and \\(Y\\) are independent only when \\(X\\) and \\(Y\\) have normal distributions. 11.2.3 Sample Correlation Coefficient Similar to covariance, the population correlation coefficient \\(\\rho_{XY}\\) also has a sample version \\(r_{XY}\\): \\[ r_{XY} = \\frac{s_{XY}}{s_Xs_Y} = \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i - \\bar{y})^2}}\\] where \\(s_{XY}\\) is the sample covariance between \\(X\\) and \\(Y\\) \\(s_X\\) is the sample standard deviation of \\(X\\), i.e., \\[s_X = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\sqrt{\\frac{(\\sum_{i=1}^n x_i^2)-n\\bar{x}^2}{n-1}}\\] \\(s_Y\\) is the sample standard deviation of \\(Y\\), i.e., \\[s_Y = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = \\sqrt{\\frac{(\\sum_{i=1}^n y_i^2)-n\\bar{y}^2}{n-1}}\\] Example 11.7 With data from Example 11.4, we can draw a plot as follows To calculate the sample correlation coefficient \\(r_{XY}\\), we calculate the standard deviations: \\[s_X = \\sqrt{\\frac{(\\sum_{i=1}^{10}x_i^2) - 10\\bar{x}^2}{10-1}} = \\sqrt{\\frac{[1.3^2 + ... + 1.0^2] - [10\\times 1.12^2]}{10-1}} = 0.214994\\] Similarly, we can calculate \\(s_Y = 52.5726\\). Hence, \\[r_{XY} = \\frac{s_{XY}}{s_Xs_Y} = \\frac{3.15778}{0.214994\\times 52.5726} = 0.27938\\] This value suggests that we have a fairly weak, positive, linear association between annual income (\\(X\\)) and credit score \\((Y)\\). Example 11.8 Continue with Example 11.5, we can calculate the sample correlation coefficient as follows. First, we need the standard deviations: \\[s_X = \\sqrt{\\frac{(\\sum_{i=1}^{9}x_i^2) - 9\\bar{x}^2}{9-1}} = \\sqrt{\\frac{[196^2 + ... + 40^2] - [9\\times 118^2]}{9-1}} = 57.2887\\] Similarly, we can solve for \\(s_Y = 0.7032\\). Hence, \\[r_{XY} = \\frac{s_{XY}}{s_Xs_Y} = \\frac{30.6}{57.2887\\times 0.7032} = 0.7596\\] This value suggests that we have a strong, positive, linear association between total # of penalty yards (\\(X\\)) and the time to complete the game (\\(Y\\)). The data can be plotted as below. Notes: Notice how the interpretations of covariance and correlation coefficient differ. 11.3 Correlation Test If \\(X\\) and \\(Y\\) follow a normal distribution (or if we have large samples of \\(X\\) and \\(Y\\)), we can test \\(H_0: \\rho_{XY} = 0\\) using the pivotal quantity \\[T = \\frac{r_{XY}}{\\sqrt{1-r_{XY}^2}/\\sqrt{n-2}} \\overset{H_0}{\\sim} t(n-2)\\] i.e., \\(T\\) follows a \\(t\\)-distribution with \\(n-2\\) degrees of freedom under the null hypothesis that \\(H_0: \\rho_{XY} = 0\\). Here, again recall that \\(\\rho_{XY}\\) is the population correlation coefficient while \\(r_{XY}\\) is the sample correlation coefficient. Now, with this pivotal quantity, we can conduct the hypothesis tests for \\(H_0: \\rho_{XY} = 0\\) and construct confidence intervals the same way as in Chapter 7 and 8. Example 11.9 Suppose in Example 11.7, \\(X\\) and \\(Y\\) both follows a normal distribution, then if we want to test a two-sided test \\(H_0: \\rho_{XY} = 0\\) vs. \\(H_1: \\rho_{XY} \\ne 0\\) at \\(5\\%\\) level of significance, we compute the observed test statistic \\[t = \\frac{r_{XY}}{\\sqrt{1-r_{XY}^2}/\\sqrt{n-2}} = \\frac{0.27938}{\\sqrt{1-0.27938^2}/\\sqrt{10-2}} = 0.8229764\\] The critical value is \\(c=t_{1-\\alpha/2, n-2} = t_{0.975, 8} = 2.306\\). Since \\(|t| &lt; c\\), we do not reject the null hypothesis that \\(\\rho_{XY} = 0\\) at 5% level of significance and conclude that there is not sufficient evidence in the data to support that \\(\\rho_{XY}\\) is different from 0. Exercise 11.2 Suppose the data in Example 11.8 are normally distributed, onduct a two-sided test with \\(H_0: \\rho_{XY} = 0\\) at \\(10\\%\\) significance level. Notes: When \\(X\\) and \\(Y\\) are normally distributed, testing the null hypothesis \\(H_0: \\rho_{XY} = 0\\) is equivalent to testing \\(X\\) and \\(Y\\) are independent. 11.4 Summary It is very important to differentiate between the population parameter (which can be calculated/determined from the population probabilistic distribution) and the sample statistics (which is calculated from the data we collect). I summarize the differences between the population parameter and sample statistics here. Quantity Population Sample Mean \\(\\mathbb{E}(X)\\) \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) Variance \\(\\mathrm{var}(X) = \\mathbb{E}(X^2) - \\left[\\mathbb{E}(X)\\right]^2\\) \\(s_X^2 = \\frac{1}{n-1}\\left[(\\sum_{i=1}^n x_i^2) - n \\bar{x}^2\\right]\\) Covariance \\(\\mathrm{cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\\) \\(s_{XY} = \\frac{1}{n-1}\\left[(\\sum_{i=1}^n x_iy_i) - n\\bar{x}\\bar{y}\\right]\\) Correlation coefficient \\(\\rho_{XY} = \\frac{\\mathrm{cov}(X, Y)}{\\sqrt{\\mathrm{var}(X)\\mathrm{var}(Y)}}\\) \\(r_{XY} = \\frac{s_{XY}}{s_Xs_Y}\\) "],["linear-regression.html", "12 Linear Regression 12.1 Simple Linear Regression 12.2 Linear Regression Inference 12.3 Goodness of Fit of the Model 12.4 Predicted Value 12.5 Multiple Linear Regression", " 12 Linear Regression In the last chapter, we have learned that correlation coefficients illustrates the linear relationship between two continuous variables. Suppose now we want to use one variable to predict another, how do we do that? In this chapter, we will learn the one of the most important statistical models: linear regression. 12.1 Simple Linear Regression In Chapter 11, we have seen that if there is a linear association between two continuous variables, we can draw a trend line in between the data points. But how do we do find such a line? The answer is to conduct a linear regression. 12.1.1 The Model Suppose we have two random variables \\(X\\) and \\(Y\\). Further suppose we are interested in \\(Y\\) and we want to use \\(X\\) to explain \\(Y\\). More specifically, we model \\(Y\\) by a straight line with respect to \\(X\\). \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] Here \\(Y\\) is the response variable, which is model as a linear function of \\(X\\) \\(X\\) is the explanatory variable \\(\\beta_0\\) is the intercept of the line \\(\\beta_1\\) is the slope of the line \\(\\epsilon\\) is a random error term and represents the unknown variation. In this model, \\(\\beta_0\\) and \\(\\beta_1\\) are model/population parameters that are unknown and need to be estimated. This model is called a simple linear regression model. Notes: You can think of the linear regression model as to decompose \\(Y\\) into two components: One component that can be explained by \\(X\\). This component is captured by \\(\\beta_1 X\\). One component that cannot be explained by \\(X\\). This component is captured by \\(\\beta_0 + \\epsilon\\). The value of this component can be due to any other explanatory variable else that we did not use in our model. We suppose that this component has a mean \\(\\beta_0\\) and the rest is just random errors \\(\\epsilon\\) that corresponds to individual differences in responses \\(Y\\) among the population. Example 12.1 Continue with the credit score context of Example 11.4, we are interested in credit score and we want to use income to explain credit score. Therefore, my response variable \\(Y\\) is credit score and my explanatory variable \\(X\\) is income. With the simple linear regression model, I posit that credit score is a linear function of income. Moreover, credit score may not be determined entirely by income, but also transaction history, payment history, etc. that we do not have in our data. These constitutes errors in our linear regression model. Notes: According to the model, \\(X\\) is also a linear function of \\(Y\\). However, \\(Y\\) is our variable of interest (response variable), in the sense that we want to predict \\(Y\\) if we are given \\(X\\). Therefore, we focus our attention on \\(Y\\), and \\(Y\\) is on the left hand side of the model equation. In Example 12.1, we want be able to predict credit score given some income (i.e., if we know someone’s income, we want to predict their credit score). 12.1.2 Fitting the Model to the Sample Data As in any other statistical problems we have seen, we want to know about the population but we cannot afford to know about everything. We therefore need to collect a sample of size \\(n\\) that consists of observations \\((X_i, Y_i)\\). Now, if the model \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) is true, then for each observation \\((X_i, Y_i)\\) in our data, we have \\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\] where \\(Y_i\\) is the value of \\(Y\\) for individual \\(i\\). \\(X_i\\) is the value of \\(X\\) for individual \\(i\\). \\(\\epsilon_i\\) is the unknown error of individual \\(i\\). Example 12.2 In Example 12.1, \\(Y_1\\) is the credit score of the first person in our sample, \\(X_1\\) is the income of that person, and so on. Now, we want to find the model/population parameters \\(\\beta_0\\) and \\(\\beta_1\\). What should be our best guesses (estimates) for the line between \\(Y\\) and \\(X\\) based on what we know from our data? It should be the line that is closest to the data points we collected! 12.1.3 The Least Squares Regression Line The closest line to the data points is the line such that the total distance from the line to all the data points is the smallest. This is called the least squares regression line. Let’s find such a line step by step. First, suppose \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the estimates of \\(\\beta_0\\) and \\(\\beta_1\\). The predicted value of \\(y\\) given \\(X = x\\) is \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] where \\(x\\) represents the different values of \\(X\\) \\(\\hat{y}\\) represents the \\(y\\)-coordinate of the point on the regression line whose \\(x\\)-coordinate equals to \\(x\\). Second, because our line may not be able to represent all the relationship between \\(X\\) and \\(Y\\), not all of the data points will lie on the line. Therefore, there will be some difference between (i) the value \\(y_i\\) that we observe from individual \\(i\\) in our data and (ii) the value \\(\\hat{y}_i\\), which is the \\(y\\)-coordinate corresponding to \\(x\\)-coordinate \\(x_i\\) on the line: \\[e_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)\\] These values \\(e_i\\) are called residuals/residual terms. They are the part of data that we didn’t capture using our simple linear regression model. Notes: Recall that we use lowercase notation \\(x_i, y_i\\) for specific value that we obtained from individual \\(i\\) in our data. We use uppercase notation \\(X_i, Y_i\\) when we want to talk about the random variable version. Figure 12.1: Regression line and residuals Third, because we want to explain our data as best as we can (we want to find a line that is closest to the data points as possible), Hence, we need to minimize the sum of squared residuals \\(\\sum_{i=1}^n e_i^2\\). Fourth, with some mathematics, we can find appropriate formulas for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) so that \\(\\sum_{i=1}^n e_i^2\\) are minimized, and hence we have the line of best fit given by \\[\\hat{\\beta}_1 = \\frac{s_{XY}}{s_X^2} = r_{XY}\\times\\frac{s_Y}{s_X} \\hspace{5mm} \\text{and} \\hspace{5mm} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\] 12.1.4 Interpretation After finding the \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can interpret our result as follows: \\(\\hat{\\beta}_0\\) is the estimated average response when \\(X = 0\\). This value may not be of interest depending on whether \\(X = 0\\) has any meaning or not. \\(\\hat{\\beta}_1\\) is the estimated change in the average response for one unit increase in \\(X\\). For any value \\(x\\) of \\(X\\) lying within the range of the observed values of the explanatory variable \\(X\\), we can predict the value of \\(Y\\) by referring to the corresponding point on the line \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\] It is NOT recommended to predict outside the range of the observed values of \\(X\\). This practice is called extrapolation. The line of best fit comes about from the observed values and so it does the best job when looking within that range only. Once we move outside that range, it may result in very misleading estimates. Example 12.3 Let’s continue with Example 11.7 about credit score and income. Solve for the line of best fit Interpret your intercept and slope Use your model to predict credit score of a person who earns $\\(60,000\\) a year. Do you think this is a good estimate? Solution: Previously, we found that \\[r_{XY} = 0.27928, \\hspace{5mm} \\bar{x} = 1.12, \\hspace{5mm} \\bar{y} = 708.9, \\hspace{5mm} s_X = 0.215, \\hspace{5mm} s_Y = 52.57\\] Hence we have \\[\\hat{\\beta}_1 = r_{XY} \\times \\frac{s_Y}{s_X} = 0.27928 \\times \\frac{52.57}{0.215} = 68.29\\] and \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = 708.9 - (68.29 \\times 1.12) = 632.42\\] So the line of best fit is given by \\[\\hat{y} = 632.42 + 68.29 \\times x\\] \\(\\hat{\\beta}_0 = 632.42\\) implies that for a person with $\\(0\\) income, the average credit score is \\(632.42\\). \\(\\hat{\\beta}_1 = 68.29\\) implies that on average the credit score increases by \\(68.32\\) for every 1 unit increase in annual income. Our prediction is \\[\\hat{y} = 632.42 + (68.29 \\times 0.6) = 673.394\\] This estimate is likely not very accurate for two reasons The data set used is already quite small leading to a model that is likely not very accurate. The \\(X\\)-value being used is far smaller than the minimum value observed in the data set, i.e., we are trying to extrapolate. This is likely reducing the accuracy of the prediction even more. Example 12.4 Consider the problem of Example 11.8. Solve for the line of best fit Interpret your intercept and slope Solution: Previously, we found that \\[r_{XY} = 0.75961, \\hspace{5mm} \\bar{x} = 118, \\hspace{5mm} \\bar{y} = 3.422, \\hspace{5mm} s_X = 57.28874, \\hspace{5mm} s_Y = 0.70317\\] Hence we have \\[\\hat{\\beta}_1 = r_{XY} \\times \\frac{s_Y}{s_X} = 0.75961 \\times \\frac{0.70317}{57.28874} = 0.0093\\] and \\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = 3.433 - (0.0093 \\times 118) = 2.322\\] So the line of best fit is given by \\[\\hat{y} = 2.322 + 0.0093 \\times x\\] \\(\\hat{\\beta}_0 = 632.42\\) implies that on average the game will be completed in \\(2.322\\) hours when the total number of penalty yards is 0. \\(\\hat{\\beta}_1 = 0.0093\\) implies that on average the time to complete the game increases by \\(0.0093\\) hour for every 1 unit increase in the total number of penalty yards. Notes: The keys to this chapter are knowing how to apply the formula to solve example/exercise problems; understanding the difference between population parameters and sample statistics so as to choose the correct formula; understanding the difference between the model (together with its parameters \\(\\beta_0, \\beta_1\\)) and the line of best fit (together with the estimators \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)). 12.2 Linear Regression Inference So far, we have postulated a linear model (a linear relationship) between our response variable \\(Y\\) and and our explanatory variable \\(X\\): \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] where \\(\\beta_0\\) and \\(\\beta_1\\) are model (population) parameters. and we have tried to calculate a line of best fit based on the sample data \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). How can we go from something calculated from the sample as \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to the true parameters \\(\\beta_0\\) and \\(\\beta_1\\)? One way to do so is to assume a distribution of the random error \\(\\epsilon\\). 12.2.1 Assumptions In order to do statistical inference on the line of best fit, we make the so-called LINE assumptions: L-inearity: Given the value of \\(X\\), the expected value of \\(Y\\) is a linear function of \\(X\\): \\(\\mathbb{E}(Y|X) = \\beta_0 + \\beta_1 X\\). That is, on average, \\(Y\\) is a linear function of \\(X\\), or, on average, \\(Y\\) and \\(X\\) form a straight line/has a linear trend. I-ndependence: The random error for each individual \\(i\\), i.e., \\(\\epsilon_i\\) are independently distributed. We assume that the random error of one individual is independent of the random error of another individual in our population. N-ormality: The random error for each individual \\(\\epsilon_i\\) follows a normal distribution. E-qual variance: The random error for each individual \\(\\epsilon_i\\) has mean zero and homogeneous variance \\(\\sigma^2\\). homogeneity: \\(\\sigma^2\\) is the same for all individuals in the population. Notes: The I-N-E assumptions can be summarized as: For each individual \\(i\\) in the population, the random error \\(\\epsilon_i\\) are independently and identically distributed (iid) as a normal distribution of mean 0 and \\(\\sigma^2\\). In short, the I-N-E assumptions assume: \\(\\epsilon_i \\overset{iid}{\\sim} N(0, \\sigma^2)\\). 12.2.2 Inference When the above assumptions are met, we have the pivotal quantity: \\[T = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\mathrm{sd}(\\hat{\\beta}_1)}\\sim t(n-2)\\] with \\[\\mathrm{sd}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}}{\\sqrt{SS_{XX}}} \\hspace{5mm} \\text{and} \\hspace{5mm} \\hat{\\sigma} = \\sqrt{\\frac{SSE}{n-2}} = \\sqrt{\\frac{(y_i-\\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{SS_{YY} - \\hat{\\beta}_1SP_{XY}}{n-2}}\\] where \\[\\begin{align*} SS_{XX} &amp; = \\sum_{i=1}^n (x_i - \\bar{x})^2 = (n-1)s_X^2 \\\\ SS_{YY} &amp; = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1)s_Y^2 \\\\ SP_{XY} &amp; = \\sum_{i=1}^n (x_i - \\bar{x})(y_i-\\bar{y}) = (n-1)s_{xy} \\end{align*}\\] Here, \\(SSE\\) is short for sum of squares of residuals, \\(SS_{XX}\\) is the sum of squares of \\(X\\) values, \\(SS_{YY}\\) is the sum of squares of \\(Y\\) values, and \\(SP_{XY}\\) the sum of product of \\(X\\) and \\(Y\\) values. Now, with the pivotal quantity, we can calculate confidence intervals and conduct hypothesis tests for \\(\\beta_1\\). Example 12.5 Let us again consider Example 11.4: A new graduate is thinking ahead and wanting to better understand what goes into a good credit score. They manage to collect a random sample of annual income (in 100’s of thousands) and credit scores values for 10 people. Income (\\(X\\)) Credit score (\\(Y\\)) \\(1.3\\) \\(756\\) \\(1.1\\) \\(728\\) \\(0.8\\) \\(635\\) \\(1.2\\) \\(599\\) \\(1.4\\) \\(760\\) \\(0.9\\) \\(722\\) \\(0.9\\) \\(743\\) \\(1.4\\) \\(726\\) \\(1.2\\) \\(694\\) \\(1.0\\) \\(726\\) Suppose that the LINE assumptions hold, Test at a \\(10\\)% level of significance whether \\(X\\) is needed in the model? (i.e., whether the coefficient of “income” \\(\\beta_1\\) is significantly different from 0) Create a \\(90\\)% confidence interval for \\(\\beta_1\\) and use your results to determine whether “income” is needed in the model explaining credit score. Solution: Let us begin our calculation from beginning: \\[\\begin{align*} \\bar{x} &amp; = \\frac{1.3+1.1+0.8 + ... + 1.0}{10} = 1.12 \\\\ \\bar{y} &amp; = \\frac{756 + 728 + 635 + ... + 726}{10} = 708.9 \\\\ SS_{XX} &amp; = (1.3-1.12)^2 + (1.1-1.12)^2 + ... + (1.0-1.12)^2 = 0.416 \\\\ SS_{YY} &amp; = (756 - 708.9)^2 + (728-708.9)^2 + ... + (726-708.9)^2 = 24874.9 \\\\ SP_{XY} &amp; = (1.3-1.12)(756-708.9) + (1.1-1.12)(728-708.9) + ... + (1.0-1.12)(726-708.9) = 28.42 \\\\ \\hat{\\beta}_1 &amp; = \\frac{s_{XY}}{s_X^2} = \\frac{SP_{XY}}{SS_{XX}} = \\frac{28.42}{0.416} = 68.32 \\\\ \\hat{\\sigma} &amp; = \\sqrt{\\frac{SS_{YY} - \\hat{\\beta}_1 SP_{XY}}{n-2}} = \\sqrt{\\frac{24874.9 - 68.32 \\times 28.42}{10-2}} = 53.54 \\end{align*}\\] Hypothesis test: Step 1: Hypotheses: \\(\\beta_1 = 0\\) vs \\(\\beta_1 \\ne 0\\). Step 2: Calculate the observed test statistic: \\[t = \\frac{\\hat{\\beta}_1 - \\beta_1}{\\hat{\\sigma}/\\sqrt{SS_{XX}}} = \\frac{68.32 - 0}{53.54/\\sqrt{0.416}} = 0.8230\\] Step 3: Calculate the critical value: \\[c = t_{1-\\alpha/2, n-2} = t_{0.95, 8} = 1.8595\\] Step 4: Because \\(|t| &lt; c\\), we do not reject the null hypothesis at \\(10\\)% significance level and conclude that income does not need to be included in the model explaining credit score, i.e., the data does not provide sufficient evidence to suggest that income is associated with credit score. A \\(90\\)% confidence interval for \\(\\beta_1\\) is \\[\\Big(\\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-2}\\times \\mathrm{sd}(\\hat{\\beta}_1)\\Big) = \\Big(68.32 \\pm 1.8595 \\times \\frac{53.54}{\\sqrt{0.416}} \\Big) = (-86.0376, 222.6776)\\] Therefore we are 90% confident that the true but unknown slope \\(\\beta_1\\) lies between -86.0376 and 222.6776. This interval contains zero, therefore it suggests that zero is a possible value for \\(\\beta_1\\) and so we do NOT reject the hypothesis in part a. Exercise 12.1 Continue with Example 11.5: Many factors affect the length of a professional football game, for example the number of running plays versus the number of passing plays. A study was conducted to determine the relationship between the total number of penalty yards (\\(X\\)) and the time required to complete a game (\\(Y\\), in hours). The table below provides the data. Total (\\(X\\)) Time to complete game (\\(Y\\)) \\(196\\) \\(4.2\\) \\(164\\) \\(4.1\\) \\(167\\) \\(3.5\\) \\(35\\) \\(3.2\\) \\(111\\) \\(3.2\\) \\(78\\) \\(3.6\\) \\(150\\) \\(4.0\\) \\(121\\) \\(3.1\\) \\(40\\) \\(1.9\\) Suppose that the LINE assumptions hold, Test at a \\(5\\)% level of significance whether \\(X\\) is needed in the model? (i.e., whether the coefficient of “total number of penalty yards” \\(\\beta_1\\) is significantly different from 0) Create a \\(95\\)% confidence interval for \\(\\beta_1\\) and use your results to determine whether “total number of penalty yards” is needed in the model explaining time to complete the game. Notes: Here, we do not know \\(\\beta_1\\), we use \\(\\hat{\\beta}_1\\), which is calculated from the data we collected, to make conclusions about \\(\\beta_1\\). So the hypotheses in the above examples are to test whether the true parameter \\(\\beta_1\\) is different from 0 or not, based on the \\(\\hat{\\beta}_1\\) value we obtained from the data. 12.3 Goodness of Fit of the Model 12.3.1 Residuals Let’s move back a bit and rethink our model is \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\). We assumed that it should be true for \\(X\\) and \\(Y\\) in general. That is, the model should be correct for all observations in the population. So, if \\((x_i, y_i)\\) are observed values of \\(Y\\) and \\(X\\) for individual (observation) \\(i\\) in the data set we collected, \\(y_i\\) and \\(x_i\\) should also satisfy \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Notice here \\(\\beta_0\\) and \\(\\beta_1\\) are the same values as what we have in our general model (\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)). Now, we know the numbers \\((x_i, y_i)\\) because they exist in our data set. However, we do not know \\(\\beta_0\\) and \\(\\beta_1\\) because they are model (population) parameters, i.e., they should be the same for all observed observations in our data set and all the unobserved observations in our population. As a result, the error \\(\\epsilon_i\\) is unknown, too. We do not know what is the true error (distance) from the model we suppose \\(\\beta_0 + \\beta_1 x_i\\) to the reality value \\(y_i\\). Now, if we estimate \\(\\beta_0\\) and \\(\\beta_1\\) by \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) respectively, we can estimate \\(\\epsilon_i\\) by the residual \\(e_i\\)’s \\[e_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)\\] To know a line fit the data well, then our residuals should be small, more precisely: as small as possible! But we should only care about the value of the residuals, not their signs, i.e., we only care about how much we are incorrect, but we don’t care about whether we are overestimating or underestimating, because both are as bad. Therefore, we want the sum of squares of residuals to be small. The sum of squares of residuals is, as we learned from Section 12.2.2: \\[SSE = \\sum_{i=1}^n e_i^2\\] 12.3.2 Coefficient of Determination (\\(R^2\\)) The coefficient of determination (\\(R^2\\)) is given by the formula \\[R^2 = 1 - \\frac{SSE}{SS_{YY}}\\] Recall that \\(SS_{YY} = \\sum_{i=1}^n (y_i - y)^2\\), which is the total variation of \\(Y\\) in our data. \\(SSE\\), the sum of squares of residuals \\(e_i\\)’s (which estimates the sum of random errors \\(\\epsilon_i\\)), is the component in the variation of \\(Y\\) that is not explained by the linear model. Therefore, the coefficient of determination (\\(R^2\\)) is the fraction of the variation in \\(Y\\) that is explained by the linear model. For simple linear regression, i.e., for linear model \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\), it can be proved that \\[R^2 = r_{XY}^2\\] Example 12.6 Interpret the \\(R^2\\) value from Example 12.5: credit score and income has \\(r_{XY} = 0.28\\) in the data given. Solution: The correlation coefficient between credit score and income \\(r_{XY}\\) is equal to \\(0.28\\). Therefore the coefficient of determination for the linear regression of \\(Y\\) on \\(X\\) is \\(r^2 = (0.28)^2 = 0.0784\\). This implies that the simple linear regression model using income (\\(X\\)) as explanatory variable explains about \\(7.84\\%\\) of the variation in credit score (\\(Y\\)). In short, the model explains \\(\\sim 7.84\\%\\) of the variation in \\(Y\\). Notes: In the above example, \\(7.84\\%\\) is a very low number. This suggests that income, on its own, is not a good factor to explain credit score. 12.3.3 Influential Points An influential point is an observation that has a large influence on the statistical calculation of the model fitting. In simple linear regression case, both the correlation coefficient \\(r_{XY}\\) and parameter estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are affected by influential points. Under linear regression, such a point is identified as one, which, if we remove it from the data, our line of best fit will change markedly. Notes: Typically outliers in either \\(X\\) or \\(Y\\) direction (or both) are influential points. If such point exist, the analyst should make an effort to see if this is due to an error that occurred while collecting the data, or if there is some other interesting factors that affect this specific observations. Usually, when we want to make conclusions about the majority of the data, we may choose to remove influential points from our analysis. Example 12.7 Suppose in Example 12.5, there is actually another observation in the data set: Income (\\(X\\)) Credit score (\\(Y\\)) \\(1.3\\) \\(756\\) \\(1.1\\) \\(728\\) \\(0.8\\) \\(635\\) \\(1.2\\) \\(599\\) \\(1.4\\) \\(760\\) \\(0.9\\) \\(722\\) \\(0.9\\) \\(743\\) \\(1.4\\) \\(726\\) \\(1.2\\) \\(694\\) \\(1.0\\) \\(726\\) \\(0.6\\) \\(480\\) The outlier in red has quite different values in \\(X\\) and \\(Y\\) compared to the rest of the data points. Now, if we use the full data set above to calculate the line of best fit, we have \\[\\hat{\\beta}_0 = 466.6, \\hspace{5mm} \\hat{\\beta}_1 = 206.4, \\hspace{5mm} r_{XY} = 0.6237\\] When we remove the outlier, we have \\[\\hat{\\beta}_0 = 632.38, \\hspace{5mm} \\hat{\\beta}_1 = 68.32, \\hspace{5mm} r_{XY} = 0.2794\\] Observe how the numbers change when we include or exclude the outlier. Notes: To detect influential points, one can calculate the Cook’s distance and see if the value is outstandingly large. In R, you can use the function cooks.distance() and identify points whose Cook’s distances are larger than \\(4/(n-p-1)\\) as influential points. Here, \\(p\\) is the number of independent variables in your linear regression. For simple linear regression, \\(p = 1\\). 12.3.4 Checking Assumptions When we try to do inference from the model, i.e., to make conclusions about the model parameter \\(\\beta_1\\), we need to make assumptions about the model. The list of assumptions in Section 12.2.1 are all about the random errors \\(\\epsilon\\). L-inearity: \\(\\mathbb{E}(Y|X) = \\beta_0 + \\beta_1 X = \\beta_0 + \\beta_1 X\\). If our model \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) is correct, this assumption is equivalent to the assumption that \\(\\mathbb{E}(\\epsilon|X) = 0\\). I-ndependence: \\(\\epsilon_i\\) are independent of \\(\\epsilon_j\\), for all \\(i\\) and \\(j\\) in the population. N-ormality and E-qual variance: \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) for individual \\(i\\) in the population Again, we do not know the true errors \\(\\epsilon_i\\) because we only have sample data instead of population data. We need to estimate the errors by the residuals \\(e_i\\). Therefore, we can try to check if our model satisfies the assumptions by checking the residuals. There are essentially two plots that we can use: The residuals vs. fitted plot: This plots the residual \\(e_i\\) against the fitted value \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). This is to check the L-inearity, I-independence, and E-qual variance assumptions. The model will seem to satisfy the LIE assumptions if the points of the plot should look like a band of consistent size, scattering around the line 0, and there should be no pattern, as in Figure 12.2 below. Figure 12.2: The residuals vs fitted plot of a model that satisfy the LIE assumption. The QQplot: As we learned from Chapter 10, we can use a QQ plot to check the assumption that certain data follows a normal distribution. Therefore, we can check the N-ormality assumption of the linear regression model by plotting a QQplot for the residuals. The model will seem to satisfy the N-assumption if the points on the QQ plot follows a straight diagonal line, as in Figure 12.3 below. Figure 12.3: The QQ plot of a model that satisfy the N-assumption. Notes: Usually, the normality assumption (for both ANOVA or linear regression) can be compromised if we have a large data set (hundreds or more observations). That is when the Central Limit Theorem kicks in. When the residuals vs fitted plot shows a pattern, we may need other techniques such as transforming the data. In this book, we will not cover this. If you are interested, try reading the Box-Cox transformation. 12.4 Predicted Value As now we have the fitted model, i.e., the line of best fit \\(y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\), we can predict \\(y\\) if we know a value of \\(x\\). A prediction for a value \\(x = x_0\\) is \\[\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0\\] Example 12.8 Continue with Example 12.7 and the case where we use the full data. I can predict the credit score for someone who earns \\(\\$100,000\\) a year using the fitted model: \\[\\hat{y} = 466.6 + 206.4 \\times 1 = 673. \\] Notes: There are two types of uncertainty we have when we try to make a prediction from a fitted model The random error \\(\\epsilon\\) that comes from other factors we did not account for in our model. The uncertainty that comes from the calculation of the model itself. Recall that the model is calculated from randomly selected samples, and the observations we have in our data set happens to be there by chance! To construct confidence intervals for a predicted value, you will need to take another class in statistics. However, we can do this automatically in R with function predict() setting interval = \"condience\". As mentioned in Section 12.1.4, do NOT predict (far) outside the range of the observed values of \\(X\\). This practice is called extrapolation and it can give misleading predictions. This is because, who knows what happens outside what we have seen? Maybe a nonlinear model? 12.5 Multiple Linear Regression We mentioned in Example 12.6, income only explains \\(\\sim 7.84\\%\\) of the variation in credit score, which is very low. In general, we can actually do a better job in explaining \\(Y\\) by adding not only one, but multiple explanatory variables: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\] In this model We have \\(p\\) explanatory variables \\(X_1, X_2, ..., X_p\\). Usually we do not want these explanatory variables to be correlated (associated) with one another, we’d rather them be independent. Casually speaking, this is because we want our model to be meaningful and do not want to include many similar variables into one model. There are more technical reasons for this, too, but probably another book will explain this better. Model fitting: Since the model now has more explanatory variables, we need another formula for the coefficient estimates and that will not be covered in this book. However, we can calculate the estimates (and their standard deviations) automatically using statistical softwares like R. The estimated coefficient \\(\\hat{\\beta}_j\\) for the \\(j\\)th explanatory variable can be interpreted in this way: keeping all other factors the same, one unit increase in the \\(j\\)th explanatory variable is associated with an average of \\(\\hat{\\beta}_j\\) increase in the response variable. For categorical variables: Statistical softwares R will treat categorical variables as follows. They will take one category as the baseline (=0), and then create one variable (yes/no or 1/0) for each of the additional category. For example, in the student rent example, I can do a linear regression of falculty on income by first choosing Arts as my baseline, and then create another variable “isEngineering” which will give me 1 for students who go to Engineering major and 0 otherwise. And then I make another variable called “IsEnvironment”, and so forth Interpretation: Now, suppose our estimated coefficient for “isEngineering” is \\(\\hat{\\beta}_{\\text{isEngineering}}\\). We interpret this number as: a student in Engineering major will on average pays \\(\\hat{\\beta}_{\\text{isEngineering}}\\) more on rent compared to a student in Arts major, keeping other variables fixed. Model Inference: To do statistical inference, i.e., using the data to make conclusions about the true population parameters \\(\\beta_1, \\beta_2, ..., \\beta_p\\), we make the same LINE assumptions about the random errors \\(\\epsilon\\). Therefore, the model checking process is the same. We still use the similar Wald-type tests as introduced in Section 12.2.2 with the estimated values and their standard deviation output from software. However, the degree of freedom of the \\(t\\)-distribution is now \\(n - p - 1\\). The coefficient of determinant, \\(R^2\\) value, does not simply equal to the square of the correlation coefficient \\(r^2\\) anymore. We can still get \\(R^2\\) value from the R output and interpret it the same way: \\(R^2 \\times 100\\%\\) of the variation of \\(Y\\) is explained by the model. As we add more variables into the model, \\(R^2\\) will surely increase. However, we need to be very careful, or else we will do “overfitting”. That is, when we add too many variables, we are trying too hard to explain all the specific details of a sample, while losing our power to make a generalized conclusion about the population. Model selection is beyond the scope of this book. "],["app-rintro.html", "A Introduction to R A.1 Why R? A.2 Installing R and RStudio A.3 The RStudio Interface A.4 Helper Codes A.5 Basic Calculations A.6 Objects A.7 Vectors A.8 Data Sets", " A Introduction to R A.1 Why R? For conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. And most importantly, R is free. In this book, we will learn how to use R to conduct basic statistical analyses. A.2 Installing R and RStudio To install R, go to the following link: https://www.r-project.org/. You can choose the CRAN closest to your location. To work with R more easily, download RStudio, an interface software for R, from the following link: https://www.rstudio.com/products/rstudio/download/. The free version is good enough for our use. After downloading, install R and RStudio on your computer following the instruction of the installation exe file. A.3 The RStudio Interface When you open RStudio, the below window will pop up. Figure A.1: RStduio windows Notice that there are four panels in the RStudio window. Source code panel: This is where we write scripts that contains codes to be saved for later use. You can create a new R script by clicking the white button below File and choose R Script. Console panel: This is where we execute the codes (in our script) interactively and where we see the printed output of the code. We can run our code by selecting the code in our script and click run, or typing or pasting the code in the console, then hit Enter. The output of our code will appear in the console. Environment panel: The Environment tab shows the active datasets or variables that are currently saved in R’s working memory. The History tab keeps track of every single line of code that has been entered and run through the console. The Files, Plots, Packages, Help and Viewer panel: The Plots tab shows the plots (or graphs) that we create using our code. The Files tab keep track of the files in our working directory. The Packages tab will show available packages in our local R library. The Help tab will show documentation about the packages or functions if we ask. A.4 Helper Codes A.4.1 Packages In R, packages contain functions that will become handy when you conduct data analyses. You can view the available CRAN packages in your library or search for a specific package in the Packages tab. To install packages in R, you can run the code install.packages(&quot;packagename&quot;) replacing packagename by the name of the package. Once a package is installed we need to load them in order to use their functions using the code library(packagename) Note that a package is only installed once but needs to be loaded every time we want to use its functions. A.4.2 Getting Help install.packages() and library() are functions in R. To understand the functionality of any built-in function or any function from a loaded package, run the following command in the console ?functionname with functionname replaced by the name of the function. For example, if we run ?install.packages in the console, the Help tab will open with information about the function install.packages(). You can also search for the name of the function you want to understand more about in the Search bar of the Help tab. A.4.3 Leaving Comments It is good practice to always have some comments to provide brief explanations about what your code is intended to do. Your code script will be more organized in this way. To indicate that a line is a comment, (not code), and should not be run in a script, use the hashtag/sharp/pound key #. For example # This is a comment and will not run! A.5 Basic Calculations As R will help us to analyze the data, it is able to handle many calculations. Try typing the code below in the console and hit Enter, you will see the calculated result printed in the console. Addition 20+22 ## [1] 42 Subtraction 20-22 ## [1] -2 Multiplication 20*22 ## [1] 440 Division 20/22 ## [1] 0.9090909 Power 5^4 ## [1] 625 or you can also write 5**4 ## [1] 625 Combining the operations (5^4)+(20/22)-(20-22) ## [1] 627.9091 Taking the square root sqrt(5) ## [1] 2.236068 Natural log log(5) ## [1] 1.609438 or log base 10 log(5, 10) ## [1] 0.69897 Exponent exp(5) ## [1] 148.4132 Round to 2 decimal places round(log(5), 2) ## [1] 1.61 Exercise A.1 Calculate the following expression using R \\[8^4 \\times 12- \\lfloor 247 \\times \\log(10)/1.25 \\rfloor\\] Hints: \\(\\lfloor x \\rfloor\\) returns the largest integer less than \\(x\\). Try looking up functions floor() using instructions from Section A.4.2. A.6 Objects You can save the values of any of the above calculations to objects in R using the &lt;- or = operator. For example, if you run the following code three &lt;- 3 Then, in the Environment tab, an object called three will appear with value 3. You can use this object for later calculation. For example, three + 10 ## [1] 13 will give 13 in the console. Object is an important concept in R or in any programming language. It can contain different types of information, such as a value (i.e., atomic vector), a vector, a matrix, a data frame, a list, or a function, etc. We will get more familiar to this as we go. You can almost freely name an object using any string of characters so that: The string can contain word characters, numbers, no space, and no special characters except for dot . and underscore _. The name should not start with a number or a special character. Give informative names so that you can recall what this object is when you read the code later. Avoid using the same name with R built-in values (e.g., pi) or functions (e.g., round). R differentiates between capital and lowercase letters in object names. So objects Three and three can refer to different objects in R. A.7 Vectors A.7.1 Vector Creation Vectors contain one or more values. You can create vectors using c(): function to create a vector by listing its values rep(): function to repeat values of a vector seq(): create equally-spaced sequences of number. Try looking these functions up following instruction in Section A.4.2. Example A.1 Create the vector \\(\\mathbf{x} = (-1,0,1)\\): x &lt;- c(-1,0,1) Now object x stores the vector \\((-1,0,1)\\). We can print the value of x: x ## [1] -1 0 1 Example A.2 Create a sequence of numbers from 0 to 10 jumping in units of two (i.e., 0,2,4,6,8,10) then store it in an object called y: y &lt;- seq(0, 10, by=2) y ## [1] 0 2 4 6 8 10 Example A.3 Create a sequence of numbers from 0 to 10 jumping in units of two that repeats twice: y &lt;- rep(seq(0,10,by=2),2) y ## [1] 0 2 4 6 8 10 0 2 4 6 8 10 Example A.4 You can also store qualitative values (as characters) into vectors in R color1 &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;) color1 ## [1] &quot;red&quot; &quot;blue&quot; &quot;green&quot; color2 &lt;- rep(color1, each = 2) color2 ## [1] &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; &quot;green&quot; &quot;green&quot; A.7.2 Vector Operations You can perform mathematical operations on the vector. For example, x &lt;- c(-1,0,1) y &lt;- c(2,4,6) z &lt;- x + y z ## [1] 1 4 7 will give the element-wise sum of the two vectors x and y. length(x) ## [1] 3 will give the number of values (length) of vector x. sum(x) ## [1] 0 will give the sum of the elements of vector x. A.7.3 Extracting Elements from Vectors Example A.5 Create a vector x that contains numbers 1 to 11 x &lt;- seq(1,11,by=1) Create another vector y that contains numbers 2 to 30 y &lt;- seq(2,30,by=1) Join the two vectors together z &lt;- c(x,y) z ## [1] 1 2 3 4 5 6 7 8 9 10 11 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## [35] 25 26 27 28 29 30 What if we now want to extract certain values from this new vector z? z[22] ## [1] 12 returns the 22nd element of vector z. z[4:8] ## [1] 4 5 6 7 8 returns the fourth to eighth elements in the vector z. z[c(4,8)] ## [1] 4 8 returns the the fourth and the eighth elements in to vector z. Negative indices can be used to remove certain elements z[-2] ## [1] 1 3 4 5 6 7 8 9 10 11 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [35] 26 27 28 29 30 returns a the same vector as z but with the second element removed. The third to eleventh elements of z can be skipped using z[-(3:11)] ## [1] 1 2 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Do not mix positive and negative indices. Consider z[c(-2,3)] which will throw an error. A.8 Data Sets A.8.1 Data Frame A data frame consists of columns that are vectors. You can group the vectors together to create a data set using the data.frame() function. Example A.6 x &lt;- c(-1,0,1) y &lt;- c(2,4,6) datset &lt;- data.frame(x,y) datset ## x y ## 1 -1 2 ## 2 0 4 ## 3 1 6 A.8.2 Load Data Files Suppose that your data is saved in a file called mydata.txt. You can import the dataset into R environment. mydata &lt;- read.table(&quot;mydata.txt&quot;, header=TRUE) The argument header=TRUE tells R to use the first line in the text file as the names of the columns. If the column headings are not included in the file, the argument can be omitted. Now, the mydata.txt data set is loaded into R environment and is saved as a data-frame object called mydata. We can then work with this data set using R. You can view the dataset using the command View(mydata) You can also print the first 6 lines of the dataset in the console using the command head(mydata) ## Gender Age Weight Height ## 1 M 50 68 155 ## 2 F 23 60 101 ## 3 M 65 72 220 ## 4 F 35 65 133 ## 5 M 15 71 166 The names of the columns can be printed in the console by the command names(mydata) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Height&quot; or colnames(mydata) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Height&quot; R can also import other data formats, such as excel or csv. Try looking up the functions read.csv() and read.table() using instruction from Section A.4.2. To load a data set using RStudio options: Figure A.2: Importing Datasets in RStudio A.8.3 Working Directory Your data file can be anywhere in the computer. To load the file correctly you need to specify the directory where the file is saved. For example, if it is saved in drive C: mydata &lt;- read.table(&quot;C:/mydata.txt&quot;, header=TRUE) To use this command mydata &lt;- read.table(&quot;mydata.txt&quot;, header=TRUE) i.e., to load the data without specifying the full directory, you need to set the working directory to be same as the directory where the file is saved. You can set the working directory using one of these ways In RStudio along the top bar choose Session &gt; Set Working Directory &gt; Choose Directory Use the function getwd() to see what the current working directory (wd) is and use setwd() to set a working directory. For example setwd(&quot;Your directory&quot;) Now, if you go to the Files tab, you can see all the files in your directory. One tip is to (i) save your .R or .Rmarkdown file and the data set in the same folder and then (ii) open a new RStudio window by opening .R or .Rmarkdown file from that folder. This way the default working directory will be the directory your files are stored. It’s also easier to manage all your files in one folder. A.8.4 Extracting Elements from a Data Frame You can use the column names of a data frame as defined objects in the Environment by attaching the data frame to R attach(mydata) For example, the data framw mydata has column Gender. We can work with this column directly as a saved vector in R. Gender ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; Once you have completed work with the data set, you can detach them from use detach(mydata) If you do not want to attach the data set, you can call the column (variable) using the $ operator mydata$Gender ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;M&quot; Exercise A.2 How many people are in the mydata dataset? What is the sum of their heights? Notes: To learn more about R, visit https://libraryguides.mcgill.ca/R. "],["app-rmarkdown.html", "B Introduction to Rmarkdown B.1 Install Rmarkdown B.2 Components of A Rmarkdown File", " B Introduction to Rmarkdown B.1 Install Rmarkdown Rmarkdown is a file format that helps you write reports which combine texts and codes and code outputs in an easy way. In fact, this book was written in Rmarkdown as well! In order to work with Rmarkdown, you will need to install the rmarkdown package install.packages(&#39;rmarkdown&#39;) To write a new .Rmarkdown file: Click File &gt; New File &gt; R Markdown or click the New File button under the File button in the top left of the RStudio window, choose R Markdown. You can choose the output format that you want, but usually we will choose PDF or HTML format. To generate PDF output, you will need to install LaTeX. You can install an independent LaTeX distribution, for example, personally I use MiKTeX. However, for anyone who never uses LaTeX before, you can just install tinytex, which is a convenient, lightweight, and beginner-friendly LaTeX distribution install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() # install TinyTeX B.2 Components of A Rmarkdown File When you open a new .Rmarkdown file, you will see that the top of the file contains some information about the file, for example, title, date, output format, etc. This component is written in between two --- signs. This is called the metadata of the file. You can change the metadata by typing the name, date, author of your file. Then, you can start writing the texts. The text is written in using the markdown language. Basically you can just write the text you want. Other caveats about headings, tables, colors, etc. can be found here. Maths can be typed using LaTeX syntax inside two dollar signs, for example $3^2$ will result in \\(3^2\\). More information about LaTeX math syntax can be found here. Finally, you can add R code chunks into the Rmarkdown document by clicking the green +C button under the code file tab and choose R. Figure B.1: Adding a code chunk An R code chunk starts with ```{r} and end with three back ticks. You can set some options to the code chunks. Two of the most important options are eval=TRUE: This means that Rmarkdown will evaluate your code when “knitting” the .Rmarkdown file into PDF (or HTML, or Words). If eval=FALSE, then Rmarkdown will not evaluate your code when knitting. You can use this option by typing ```{r, eval=FALSE}. This is useful when you just want to show the code but don’t want to run it. echo=TRUE: This means that Rmarkdown will display your code when “knitting” the file. echo=FALSE means that Rmarkdown will not display your code when knitting the file. Figure B.2: Components of a code chunk Finally, you can “knit” your Rmarkdown file into a PDF file by hitting the Knit button on the top bar under the file tabs. Figure B.3: Knitting an Rmarkdown file Exercise B.1 Create a PDF file that includes the solution (code and output) to Exercises A.1 and A.2. Notes: To learn more about Rmarkdown, visit https://bookdown.org/yihui/rmarkdown/. "],["app-dataset.html", "C Data Set", " C Data Set We use an artificially generated pseudo data set fakeRent.csv as a running example throughout the book. The data set contains information about students’ rent expenses. The variables collected in the data set corresponds to columns in the data set, namely rent: the rent price of students, which is continuous. study: indicates whether the level of study of the student is undergraduate or graduate. This variable is categorical. gender: the gender of the student, which can be either \"female\", \"male\", or \"others\". This variable is categorical. faculty: the faculty of the student. It can be either \"Arts\", \"Engineering\", \"Environment\", \"Health\", \"Math\", or \"Science\". This variable is categorical. distance: the distance from the rented place to school. It can be either \"less than 15 minutes\", \"15-30 minutes\", \"30-45 minutes\", or \"more than 40 minutes\". This variable is categorical. washroom: whether the rented place has a private washroom. 1 indicates there is a private washroom and 0 otherwise. This variable is binary. area: the area of the rented place in \\(m^2\\). This variable is continuous. Read the data set and save it into R working environment as an object called rent using the following code rent &lt;- read.csv(&quot;fakeRent.csv&quot;) This rent object will be used throughout the book. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
